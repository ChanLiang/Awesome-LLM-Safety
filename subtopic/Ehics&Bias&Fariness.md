# Ethics

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating witha the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                                                      Institute                                                                                       |     Publication     |                                                                                    Paper                                                                                    |                                           Keywords                                            |
|:-----:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------:|
| 23.06 |                                                                      University of Illinois at Urbana-Champaign                                                                      |        arxiv        |                               [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)                                |                      **Robustness**&**Ethics**&**Privacy**&**Toxicity**                       |
| 23.11 |                                                                                Allen Institute for AI                                                                                |        arxiv        |                                   [Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs](https://arxiv.org/abs/2311.04892)                                    |                                   **Bias**&**Stereotypes**                                    |
| 23.11 |                                                                                   Adobe Inc. India                                                                                   |        arxiv        |                     [All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation](https://arxiv.org/abs/2311.05451)                     |                                    **Fairness**&**Biases**                                    |
| 23.11 |                                                                         National Taiwan University, Meta AI                                                                          |        arXiv        |                          [Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems](https://arxiv.org/abs/2311.06513)                          |               **Task-oriented Dialogue Systems**&**Societal Bias**&**Fairness**               |
| 23.11 |                                                                          UNC Chapel Hill, IBM Research MIT                                                                           |        arxiv        |                             [Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion](https://arxiv.org/abs/2311.07682)                              |                **Model Fusion**&**Bias Reduction**&**Selective Memorization**                 |
| 23.11 |                                                                                      ETH Z√ºrich                                                                                      |        arxiv        |                   [Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures](https://arxiv.org/abs/2311.08605)                   |                          **Political Debates**&**Bias Attribution**                           |
| 23.11 |                                                                     University of Pisa, University of Copenhagen                                                                     |        arxiv        |                                     [Social Bias Probing: Fairness Benchmarking for Language Models](https://arxiv.org/abs/2311.09090)                                      |             **Social Biases**&**Fairness Benchmarking**&**Identity Stereotypes**              |
| 23.11 |                                                                                 Tsinghua University                                                                                  |        arxiv        |     [ALIGNMENT IS NOT SUFFICIENT TO PREVENT LARGE LANGUAGE MODELS FROM GENERATING HARMFUL INFORMATION: A PSYCHOANALYTIC PERSPECTIVE](https://arxiv.org/abs/2311.08487)      |                      **Alignment**&**Psychoanalysis Theory**&**Ethics**                       |
| 23.11 |                                                                    University of Toronto, University of Michigan                                                                     |        arxiv        |                     [Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks](https://arxiv.org/abs/2311.09730)                     |                                **Gender Bias**&**Racial Bias**                                |
| 23.11 |                                                    University of Michigan, University of Hawaii at Hilo, Northeastern University                                                     |        arxiv        |                                      [MOKA: Moral Knowledge Augmentation for Moral Event Extraction](https://arxiv.org/abs/2311.09733)                                      |          **Moral Event Extraction**&**MOKA Framewor**k&**External Moral Knowledge**           |
| 23.11 |              Mila - Quebec AI Institute, Universit√© du Qu√©bec √† Montr√©al, Data & Society Research Institute, Mantium, IBM Research, University of California Riverside               |        arxiv        |                                  [Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset](https://arxiv.org/abs/2311.09443)                                  |                **Subtle Misogyny**&B**iasly Dataset**&**Dataset Development**                 |
| 23.11 |                                                   Illinois Institute of Technology, University of Illinois Chicago, Cisco Research                                                   |        arxiv        |                             [Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models](https://arxiv.org/abs/2311.09428)                             |              **Fairness**&**Abusive Language Detection**&**Adversarial Attacks**              |
| 23.11 |                                                  Cornell University, KTH Royal Institute of Technology, University of Pennsylvania                                                   |        arxiv        |                                              [Auditing and Mitigating Cultural Bias in LLMs](https://arxiv.org/abs/2311.14096)                                              |                          **Cultural Bias**&**Mitigation Strategies**                          |
| 23.11 |                                                                        University College London, Holistic AI                                                                        |        arxiv        |                            [Towards Auditing Large Language Models: Improving Text-based Stereotype Detection](https://arxiv.org/abs/2311.14126)                            |                                   **Stereotype Detection**                                    |
| 23.11 |                                                                           Georgia Institute of Technology                                                                            |        arxiv        |                                 [Evaluating Large Language Models through Gender and Racial Stereotypes](https://arxiv.org/abs/2311.14788)                                  |                **Gender Bias**&**Racial Stereotypes**&**Evaluation Framework**                |
| 23.11 |                                             Polytechnique Montr√©al, √âTS Montr√©al, CISPA-Helmholtz Center for Information Security, Mila                                              |        arxiv        |                                           [Survey on AI Ethics: A Socio-technical Perspective](https://arxiv.org/abs/2311.17228)                                            |                     **AI Ethics&****Trustworthiness**&**Responsibility**                      |
| 23.11 |                                                                                         Meta                                                                                         |      EMNLP2023      |                                   [ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://arxiv.org/abs/2311.18140)                                    |                         **Bias Evaluation**&**Fairness**&**Toxicity**                         |
| 23.11 |                                                                      Comcast Applied AI, University of Waterloo                                                                      |        arxiv        |                       [What Do Llamas Really Think? Revealing Preference Biases in Language Model Representations](https://arxiv.org/abs/2311.18812)                        |                **Implicit Bias**&**Sociodemographic Bias**&B**ias Mitigation**                |
| 23.11 |                                                             Shanghai Artificial Intelligence Laboratory&Fudan University                                                             |      NAACL2024      |                                         [FLAMES: Benchmarking Value Alignment of LLMs in Chinese](https://arxiv.org/abs/2311.06899)                                         |                            **Value Alignment**&**LLMs**&**Safety**                            |
| 23.11 |                                                     University of Michigan&University of Hawaii at Hilo&Northeastern University                                                      |      NAACL2024      |                                      [MOKA: Moral Knowledge Augmentation for Moral Event Extraction](https://arxiv.org/abs/2311.09733)                                      |              **Moral Knowledge Augmentation**&**Moral Event Extraction**&**NLP**              |
| 23.12 |                                                          University of Auckland, University of Waikato, University of Macau                                                          |        arXiv        |                      [Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies](https://arxiv.org/abs/2312.01509)                       |                             **Governmental Regulations**&**Bias**                             |
| 23.12 |                            Yildiz Technical University, Istanbul Technical University, Maersk Mc-kinney Moeller Institute University of Southern Denmark                             |        arXiv        |                     [Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection](https://arxiv.org/abs/2312.01787)                      |                         **Offensive Language**&**Data-augmentation**                          |
| 23.12 |                                                                                  TCS Research India                                                                                  | EMNLP2023(Workshop) |                     [Towards Mitigating Perceived Unfairness in Contracts from a Non-Legal Stakeholder‚Äôs Perspective](https://arxiv.org/abs/2312.01398)                     |                 **Fairness**&**Non-legal Stakeholders**&**Data Augmentation**                 |
| 23.12 |                                                                                   Korea University                                                                                   |      EMNLP2023      |                            [Improving Bias Mitigation through Bias Experts in Natural Language Understanding](https://arxiv.org/abs/2312.03577)                             |                      **Bias Mitigation**&**Multi-Class Classification**                       |
| 23.12 |                                                          Queensland University of Technology, University of New South Wales                                                          | EMNLP2023(Workshop) |           [Measuring Misogyny in Natural Language Generation: Preliminary Results from a Case Study on two Reddit Communities](https://arxiv.org/abs/2312.03330)            |                      **Misogyny**&**Toxicity Classifier**s&**Benchmark**                      |
| 23.12 |                                   Carnegie Mellon University, Pittsburgh, Universidade NOVA de Lisboa, Allen Institute for Artificial Intelligence                                   |      EMNLP2023      |                          [Understanding the Effect of Model Compression on Social Bias in Large Language Models](https://arxiv.org/abs/2312.05662)                          |               **Model Compression**&**Social Bias**&&**Knowledge Distillation**               |
| 23.12 |                                 Eindhoven University of Technology, University of Liverpool, Griffith University, The Pennsylvania State University                                  |        arXiv        |                             [GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models](https://arxiv.org/abs/2312.06315)                             |           **Bias Evaluation**&**Bias Attack Instructions**&**Intersectional Bias**            |
| 23.12 |                                                                                   IBM Research AI                                                                                    |      AAAI2024       |                        [SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models](https://arxiv.org/abs/2312.07492)                        |                        **Social Bias**&**Question-answering Dataset**                         |
| 23.12 |                                                                       New York University, CUNY Queens College                                                                       |        arxiv        |                         [Red AI? Inconsistent Responses from GPT Models on Political Issues in the US and China](https://arxiv.org/abs/2312.09917)                          |                                     **Bias**&**Politics**                                     |
| 23.12 |                                                                  University of California Los Angeles, Amazon Alexa                                                                  |        arXiv        |     [Are you talking to [‚Äòxem‚Äô] or [‚Äòx‚Äô ‚Äòem‚Äô]? On Tokenization and Addressing Misgendering in LLMs with Pronoun Tokenization Parity](https://arxiv.org/abs/2312.11779)      |                                        **Gender Bias**                                        |
| 23.12 |                                                       University of Oxford, University Canada West, Amazon Web Services (AWS)                                                        |        arxiv        |                                               [Large Language Model (LLM) Bias Index‚ÄîLLMBI](https://arxiv.org/abs/2312.14769)                                               |                          **Bias Quantification**&**Bias Mitigation**                          |
| 23.12 | Institute of Information Engineering Chinese Academy of Sciences, School of Cyber Security University of Chinese Academy of Sciences, JD AI Research Beijing, Independent Researcher |        arXiv        |                                             [A Group Fairness Lens for Large Language Models](https://arxiv.org/abs/2312.15478)                                             |                              **Group Fairness**&**Social Bias**                               |
| 24.01 |                                                                             Hong Kong Baptist University                                                                             |        arxiv        |                         [GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse](https://arxiv.org/abs/2401.01523)                          |      **Large Multimodal Models (LMMs)**&**Meme-Based Social Abuse**&**Safety Insights**       |
| 24.01 |                                                                       Beihang University, Westcliff University                                                                       |        arxiv        | [Unveiling Bias in Fairness Evaluations of Large Language Models: A Critical Literature Review of Music and Movie Recommendation Systems](https://arxiv.org/abs/2401.04057) | **Music and Movie Recommendations**&**Recommender Systems**&**Fairness Evaluation Framework** |
| 24.01 |                                                            Pacific Northwest National Laboratory, University of Michigan                                                             |        arxiv        |                    [Whose wife is it anyway? Assessing bias against same-gender relationships in machine translation](https://arxiv.org/abs/2401.04972)                     |                         **Gender Bias**&**Same-Gender Relationships**                         |
| 24.01 |                             Universit√© Jean Monnet Saint-Etienne, CNRS Institut d'Optique Graduate School, T√©l√©com Paris Institut Polytechnique de Paris                             |        arxiv        |                            [An investigation ofstructures responsible for gender bias in BERT and DistilBERT](https://arxiv.org/abs/2401.06495)                             |                                  **Fairness**&**Imbalance**                                   |
| 24.01 |                                                                                University of Toronto                                                                                 |        arXiv        |  [Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models](https://arxiv.org/abs/2401.10745)  |                                 **Ethical AI**&**Governance**                                 |
| 24.02 |                                                University of Manchester, Idiap Research Institute, National Biomarker Centre CRUK-MI                                                 |        arxiv        |                      [Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement](https://arxiv.org/abs/2402.00745)                      |                     **Ethical Reasoning**&**Neuro-Symbolic Integration**                      |
| 24.02 |                                                                       Indian Institute of Technology Kharagpur                                                                       |        arxiv        |    [How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries](https://arxiv.org/abs/2402.15302)     |                 **Instruction-centric Responses**&**Ethical Vulnerabilities**                 |
| 24.02 |                                                                                         HBKU                                                                                         |  LREC-COLING 2024   |                      [Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles](https://arxiv.org/abs/2402.17478)                       |                     **Propaganda Span Detection**&**Zero-shot Learning**                      |
| 24.02 |                                                            University of Pisa&University of Edinburgh&Bocconi University                                                             |        arxiv        |                                        [FAIRBELIEF ‚Äì Assessing Harmful Beliefs in Language Models](https://arxiv.org/abs/2402.17389)                                        |                              **Beliefs Assessment**&**Fairness**                              |
| 24.03 |                                      √âcole polytechnique f√©d√©rale de Lausanne, Carnegie Mellon University, University of Maryland College Park                                       |        arxiv        |                              ["Flex Tape Can‚Äôt Fix That": Bias and Misinformation in Edited Language Models](https://arxiv.org/abs/2403.00180)                              |                   **Model Editing**&**Demographic Bias**&**Misinformation**                   |
| 24.03 |                                                          University of Maryland, University of Antwerp, New York University                                                          |        arxiv        |                                        [Evaluating LLMs for Gender Disparities in Notable Persons](https://arxiv.org/abs/2403.09148)                                        |                           **Bias**&**Fairness**&**Hallucinations**                            |
| 24.03 |                                                                                 Tsinghua University                                                                                  |        arxiv        |                                                [Evaluation Ethics of LLMs in Legal Domain](https://arxiv.org/abs/2403.11152)                                                |                            **Legal Domain**&**Ethics Evaluation**                             |
| 24.03 |                                                                    University of Science and Technology of China,                                                                    |        arxiv        |                                      [Locating and Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2403.14409)                                       |                              **Causal Intervention**&**Debias**                               |
| 24.03 |                                                                       University of Illinois Urbana-Champaign                                                                        |        arxiv        |                           [A Moral Imperative: The Need for Continual Superalignment of Large Language Models](https://arxiv.org/abs/2403.14683)                            |                               **Superalignment**&**AI Ethics**                                |
| 24.03 |                                                                               Fondazione Bruno Kessler                                                                               | NAACL2024(findings) |                                      [NLP for Counterspeech against Hate: A Survey and How-To Guide](https://arxiv.org/abs/2403.20103)                                      |                           **Counterspeech**&**Online Hate**&**NLP**                           |
| 24.04 |                                                                                   Google Research                                                                                    | NAACL2024(findings) |                      [Modeling the Sacred: Considerations when Using Religious Texts in Natural Language Processing](https://arxiv.org/abs/2404.14740)                      |                **Religious Texts**&**Natural Language Processing**&**Ethics**                 |
| 24.05 |                                                                     Northwestern University, Rutgers University                                                                      |        arxiv        |                                           [Large Language Model Agent for Fake News Detection](https://arxiv.org/abs/2405.01593)                                            |                         **Fake News Detection**&**Agentic Approach**                          |
| 24.05 |                                                                         Vector Institute, Queen‚Äôs University                                                                         |        arxiv        |                              [BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models](https://arxiv.org/abs/2405.04756)                               |                    **Adversarial Knowledge Graph**&**Bias**&**AI Safety**                     |
| 24.05 |                                                                                 Zhejiang University                                                                                  |        arxiv        |                             [Large Language Model Bias Mitigation from the Perspective of Knowledge Editing](https://arxiv.org/abs/2405.09341)                              |                           **Bias Mitigation**&**Knowledge Editing**                           |
| 24.05 |                                                                                University of Waterloo                                                                                |        arxiv        |                                         [UnMarker: A Universal Attack on Defensive Watermarking](https://arxiv.org/abs/2405.08363)                                          |                         **Deepfake Watermarking**&**Adversarial ML**                          |
| 24.06 |                                                                      Sa√Ød Business School, University of Oxford                                                                      |        arxiv        |                             [How Ethical Should AI Be? How AI Alignment Shapes the Risk Preferences of LLMs](https://arxiv.org/abs/2406.01168)                              |          **AI Alignment**&**Risk Preferences**&**AI in Finance**&**Underinvestment**          |
| 24.06 |                                                                                University of Catania                                                                                 |        arxiv        |                           [Do Language Models Understand Morality? Towards a Robust Detection of Moral Content](https://arxiv.org/abs/2406.04143)                           |                      **Value Detection**&**Natural Language Inference**                       |
| 24.06 |                                                                  Korea Advanced Institute of Science and Technology                                                                  |  ACL 2024 Findings  |                       [Ask LLMs Directly, "What shapes your bias?": Measuring Social Bias in Large Language Models](https://arxiv.org/abs/2406.04064)                       |           **Social Bias**&**Bias Measurement**&**QA Format**&**Social Perception**            |
| 24.06 |                                                                                  Rutgers University                                                                                  |        arxiv        |                                                  [MoralBench: Moral Evaluation of LLMs](https://arxiv.org/abs/2406.04428)                                                   |                              **Moral Evaluation**&**MoralBench**                              |
| 24.06 |                                                                                     Cornell Tech                                                                                     |        arxiv        |                           [Annotation Alignment: Comparing LLM and Human Annotations of Conversational Safety](https://arxiv.org/abs/2406.06369)                            |                      **Annotation Alignment**&**Conversational Safety**                       |
| 24.06 |                                                                     University of North Carolina at Chapel Hill                                                                      |        arxiv        |                                   [Exploring Safety-Utility Trade-Offs in Personalized Language Models](https://arxiv.org/abs/2406.11107)                                   |    **Safety-Utility Trade-Offs**&**Personalized Language Models**&**Personalization Bias**    |
| 24.06 |                                                                                  Auburn University                                                                                   |        arxiv        |                             [Investigating Annotator Bias in Large Language Models for Hate Speech Detection](https://arxiv.org/abs/2406.11109)                             |                         **Annotator Bias**&**Hate Speech Detection**                          |
| 24.06 |                                                                                 Syracuse University                                                                                  |        arxiv        |                          [Global Data Constraints: Ethical and Effectiveness Challenges in Large Language Model](https://arxiv.org/abs/2406.11214)                          |        **Global Data Constraints**&**Ethical Challenges**&**Effectiveness Challenges**        |
| 24.06 |                                                                                University of Maryland                                                                                |      ACL 2024       |                 [Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?](https://arxiv.org/abs/2406.10486)                  |                            **Discrimination**&**Hiring Decisions**                            |
| 24.06 |                                                                                University of Maryland                                                                                |        arxiv        |                                 [Large Language Models are Biased Because They Are Large Language Models](https://arxiv.org/abs/2406.13138)                                 |                        **LLM Bias**&**Bias Mitigation**&**Bias in AI**                        |
| 24.06 |                        Shanghai Jiao Tong University, Carnegie Mellon University, Fudan University, Shanghai AI Laboratory, Generative AI Research Lab (GAIR)                        |        arxiv        |                                         [BEHONEST: Benchmarking Honesty of Large Language Models](https://arxiv.org/abs/2406.13261)                                         |                       **LLM Honesty**&**Benchmarking**&**Consistency**                        |
| 24.06 |                                                           Vector Institute, Scotia Bank, Ernst & Young, Queen‚Äôs University                                                           |        arxiv        |                                     [Mitigating Social Biases in Language Models through Unlearning](https://arxiv.org/abs/2406.13551)                                      |                        **Social Biases**&**Unlearning**&**Debiasing**                         |
| 24.06 |                                                                        ELLIS Alicante, University of Alicante                                                                        |        arxiv        |                              [Leveraging Large Language Models to Measure Gender Bias in Gendered Languages](https://arxiv.org/abs/2406.13677)                              |                  **Gender Bias**&**Bias Quantification**&**Spanish Corpora**                  |
| 24.06 |                                          South China University of Technology, Pazhou Laboratory, University of Maryland, Baltimore County                                           |        arxiv        |                          [GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2406.13925)                          |             **Gender Bias Mitigation**&**Alignment Dataset**&**Bias Categories**              |
| 24.06 |                            CAS Key Laboratory of AI Safety, CAS Key Lab of Network Data Science and Technology, University of Chinese Academy of Sciences                            |        arxiv        |                     [Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](https://arxiv.org/abs/2406.14023)                      |                **Psychometric Evaluation**&**Bias Attacks**&**Ethical Risks**                 |
| 24.06 |                                                                              University College London                                                                               |        arxiv        |                            [JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models](https://arxiv.org/abs/2406.15484)                            |                       **Gender Bias**&**Hiring Bias**&**Benchmarking**                        |
| 24.06 |                                                                          The University of Texas at Austin                                                                           |        arxiv        |                                 [Navigating LLM Ethics: Advancements, Challenges, and Future Directions](https://arxiv.org/abs/2406.18841)                                  |                    **LLM Ethics**&**Accountable LLM**&**Responsible LLM**                     |
| 24.07 |                                                                               George Mason University                                                                                |        arxiv        |               [Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis](https://arxiv.org/abs/2407.02030)                |                           **Social Biases**&**Contact Hypothesis**                            |
| 24.07 |                                                                 Bangladesh University of Engineering and Technology                                                                  |        arxiv        |                    [Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias](https://arxiv.org/abs/2407.03536)                     |                      **Social Bias**&**Gender Bias**&**Religious Bias**                       |
| 24.07 |                                                                                University of Calabria                                                                                |        arxiv        |         [Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation](https://arxiv.org/abs/2407.08441)          |                       **Bias**&**Jailbreak**&**Adversarial Robustness**                       |
| 24.07 |                                                                                 University of Oxford                                                                                 |        arxiv        |                          [What an Elegant Bridge: Multilingual LLMs are Biased Similarly in Different Languages](https://arxiv.org/abs/2407.09704)                          |                     **Multilingual LLMs**&**Bias**&**Grammatical Gender**                     |
| 24.07 |                                                                                 Zhejiang University                                                                                  |        arxiv        |                                    [BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs](https://arxiv.org/abs/2407.10241)                                    |                              **Bias Detection**&**Social Bias**                               |
| 24.07 |                                                                                      CVS Health                                                                                      |        arxiv        |                        [An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases](https://arxiv.org/abs/2407.10853)                        |                               **Bias Assessment**&**Fairness**                                |
| 24.07 |                                                                               University of Amsterdam                                                                                |        arxiv        |                             [How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine Studies](https://arxiv.org/abs/2407.11733)                             |                             **Stereotyping**&**Safety Training**&                             |
| 24.07 |                                                                             American University in Cairo                                                                             |        arxiv        |                           [BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization](https://arxiv.org/abs/2407.13928)                            |                    **Bias Mitigation**&**Direct Preference Optimization**                     |
| 24.07 |                                                                                       SoftlyAI                                                                                       |        arxiv        |                                                    [Harmful Suicide Content Detection](https://arxiv.org/abs/2407.13942)                                                    |                       **Suicide Content Detection**&**Harmful Content**                       |
| 24.07 |                                                                               University of Cambridge                                                                                |        arxiv        |                   [LLMs left, right, and center: Assessing GPT‚Äôs capabilities to label political bias from web domains](https://arxiv.org/abs/2407.14344)                   |                        **GPT-4**&**Political Bias**&**Data Labeling**                         |
| 24.07 |                                                                                Georgetown University                                                                                 |        arxiv        |                                     [US-China perspectives on extreme AI risks and global governance](https://arxiv.org/abs/2407.16903)                                     |           **Extreme AI Risks**&**Global Governance**&**International Cooperation**            |
| 24.07 |                                                                              Carnegie Mellon University                                                                              |        arxiv        |                 [Examining the Influence of Political Bias on Large Language Model Performance in Stance Classification](https://arxiv.org/abs/2407.17688)                  |                         **Political Bias**&**Stance Classification**                          |
| 24.07 |                                                                                  Xidian University                                                                                   |        arxiv        |                            [The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models](https://arxiv.org/abs/2407.17915)                            |                             **Function Calling**&**Jailbreaking**                             |
| 24.07 |                                                                                 Universit√§t Hamburg                                                                                  |      AIES 2024      |                  [Decoding Multilingual Moral Preferences: Unveiling LLM‚Äôs Biases Through the Moral Machine Experiment](https://arxiv.org/abs/2407.15184)                   |                   **Moral Preferences**&**Multilingual Analysis**&**Bias**                    |
| 24.07 |                                                                           Florida International University                                                                           |        arxiv        |                                            [Fairness Definitions in Language Models Explained](https://arxiv.org/abs/2407.18454)                                            |                               **Fairness**&**Bias Mitigation**                                |
| 24.07 |                                                                           Barcelona Supercomputing Center                                                                            |        arxiv        |                               [The Power of Prompts: Evaluating and Mitigating Gender Bias in MT with LLMs](https://arxiv.org/abs/2407.18786)                               |                            **Gender Bias**&**Prompt Engineering**                             |
| 24.07 |                                                                               University of Washington                                                                               |      AIES 2024      |                         [Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval](https://arxiv.org/abs/2407.20371)                          |                                 **Gender bias**&**Race bias**                                 |
| 24.08 |                                                                              Seoul National University                                                                               |        arxiv        |                      [Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment](https://arxiv.org/abs/2408.00137)                       |                     **Negative Bias**&**Attention Score**&**Fine-tuning**                     |
| 24.08 |                                                                                  Rutgers University                                                                                  |        arxiv        |                                        [A Taxonomy of Stereotype Content in Large Language Models](https://arxiv.org/abs/2408.00162)                                        |                        **Bias**&**Stereotypes**&**Social Psychology**                         |
| 24.08 |                                                                           Florida International University                                                                           |      CIKM '24       |                                            [Fairness in Large Language Models in Three Hours](https://arxiv.org/abs/2408.00992)                                             |                               **Fairness**&**Bias Mitigation**                                |
| 24.08 |                                                                                      Intel Labs                                                                                      |        arxiv        |                     [Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models](https://arxiv.org/abs/2408.03907)                      |             **Gender Bias**&**Adversarial Prompt Generation**&**LLM Evaluation**              |
| 24.08 |                                                                                 Shandong University                                                                                  |        arxiv        |                                               [Social Debiasing for Fair Multi-modal LLMs](https://arxiv.org/abs/2408.06569)                                                |            **Social Debiasing**&**Multi-modal LLMs**&**Anti-Stereotype Debiasing**            |
| 24.08 |                                                                                 King Saud University                                                                                 |        arxiv        |                      [Covert Bias: The Severity of Social Views' Unalignment Towards Implicit and Explicit Opinion](https://arxiv.org/abs/2408.08212)                       |                             **Covert Bias**&**Implicit Opinion**                              |
| 24.08 |                                                                      Shanghai University of Engineering Science                                                                      |        arxiv        |           [Promoting Equality in Large Language Models: Identifying and Mitigating the Implicit Bias based on Bayesian Theory](https://arxiv.org/abs/2408.10608)            |                    **Implicit Bias**&**Bayesian Theory**&**Bias Removal**                     |
| 24.08 |                                                                                 Zhejiang University                                                                                  |        arxiv        |                                   [Editable Fairness: Fine-Grained Bias Mitigation in Language Models](https://arxiv.org/abs/2408.11843)                                    |                          **Bias Mitigation**&**Knowledge Retention**                          |
| 24.08 |                                                                    University of Science and Technology of China                                                                     |        arxiv        |                  [GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models](https://arxiv.org/abs/2408.12494)                  |              **Gender Bias**&**Large Language Models**&**Algorithmic Fairness**               |
| 24.08 |                                                                                 Stanford University                                                                                  |        arxiv        |                                         [Uncovering Biases with Reflective Large Language Models](https://arxiv.org/abs/2408.13464)                                         |                             **Bias Detection**&**Reflective AI**                              |
| 24.08 |                                                                            University of Western Ontario                                                                             |        arxiv        |                   [Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision by Large Language Models](https://arxiv.org/abs/2408.15895)                   |                          **Bias**&**Political Cues**&**Annotation**                           |
| 24.09 |                                                                         The Chinese University of Hong Kong                                                                          |        arxiv        |                        [Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness](https://arxiv.org/abs/2409.00551)                         |                         **Correctness**&**Non-Toxicity**&**Fairness**                         |
| 24.09 |                                                                                 University of Trento                                                                                 |        arxiv        |                                          [More is More: Addition Bias in Large Language Models](https://arxiv.org/abs/2409.02569)                                           |                             &**Addition Bias**&**Cognitive Bias**                             |
| 24.09 |                                                                       Nanjing University, Southeast University                                                                       |        arxiv        |                                       [AGR: Age Group Fairness Reward for Bias Mitigation in LLMs](https://arxiv.org/abs/2409.04340)                                        |     **Age Bias**&**LLM Alignment**&**Reinforcement Learning with Human Feedback (RLHF)**      |
| 24.09 |                                                               MIT Center for Constructive Communication, MIT Media Lab                                                               |        arxiv        |                                 [On the Relationship between Truth and Political Bias in Language Models](https://arxiv.org/abs/2409.05283)                                 |                              **Truthfulness**&**Political Bias**                              |
| 24.09 |                                                                      AppCubic, Georgia Institute of Technology                                                                       |        arxiv        |                           [Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks](https://arxiv.org/abs/2409.08087)                           |                 **Misinformation**&**Jailbreak Attacks**&**Prompt Injection**                 |
| 24.09 |                                                                            Technical University of Munich                                                                            |        arxiv        |                                      [Understanding Knowledge Drift in LLMs through Misinformation](https://arxiv.org/abs/2409.07085)                                       |                    **Knowledge Drift**&**Misinformation**&**Uncertainty**                     |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars
