# Defense

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                                          Institute                                                                          |                          Publication                           |                                                                           Paper                                                                           |                                                      Keywords                                                      |
|:-----:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------:|
| 21.07 |                                                                       Google Research                                                                       |                            ACL2022                             |                          [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)                          |                              **Privacy Protected**&**Deduplication**&**Memorization**                              |
| 23.05 |                                                                  UC Davis, USC, UW-Madison                                                                  |                           NAACL2024                            |                            [From Shortcuts to Triggers: Backdoor Defense with Denoised PoE](https://arxiv.org/abs/2305.14910)                             |                      **Backdoor Attacks**&**Defense Methods**&**Natural Language Processing**                      |
| 23.08 |                                                                  Georgia Tech, Intel Labs                                                                   |                             arxiv                              |                        [LLM Self Defense: By Self Examination LLMs Know They Are Being Tricked](https://arxiv.org/abs/2308.07308)                         |                       **Adversarial Attacks**&**Self Defense**&**Harmful Content Detection**                       |
| 23.08 |                                                                   University of Michigan                                                                    |                             arxiv                              |                                  [DETECTING LANGUAGE MODEL ATTACKS WITH PERPLEXITY](https://arxiv.org/abs/2308.14132v3)                                   |                            **Adversarial Suffixes**&**Perplexity**&**Attack Detection**                            |
| 23.09 |                                                                   University of Maryland                                                                    |                             arxiv                              |                                  [Certifying LLM Safety against Adversarial Prompting](https://arxiv.org/abs/2309.02705)                                  |                                     **Safety Filter**&**Adversarial Prompts**                                      |
| 23.09 |                                                                   University of Maryland                                                                    |                             arxiv                              |                       [BASELINE DEFENSES FOR ADVERSARIAL ATTACKS AGAINST ALIGNED LANGUAGE MODELS](https://arxiv.org/abs/2309.00614)                       |                          **Perplexity**&**Input Preprocessing**&**Adversarial Training**                           |
| 23.09 |                                                              The Pennsylvania State University                                                              |                             arxiv                              |                         [DEFENDING AGAINST ALIGNMENT-BREAKING ATTACKS VIA ROBUSTLY ALIGNED LLM](https://arxiv.org/abs/2309.14348)                         |                  **Alignment-Breaking Attacks**&**Adversarial Prompts**&**Jailbreaking Prompts**                   |
| 23.10 |                                                                 University of Pennsylvania                                                                  |                             arxiv                              |                        [SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2310.03684)                        |                               **Jailbreak**&**Adversarial Attack**&**Perturbation**                                |
| 23.10 |                                                                  Michigan State University                                                                  |                             arXiv                              |                         [Jailbreaker in Jail: Moving Target Defense for Large Language Models](https://arxiv.org/abs/2310.02417)                          |                   **Dialogue System**&**Trustworthy Machine Learning**&**Moving Target Defense**                   |
| 23.10 |                                                                      Peking University                                                                      |                             arxiv                              |                  [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations](https://arxiv.org/abs/2310.06387)                  |                   **In-Context Learning**&**Adversarial Attacks**&**In-Context Demonstrations**                    |
| 23.10 |                                                        The Chinese University of Hong Kong&Microsoft                                                        |                           NAACL2024                            |                                    [SELF-GUARD: Empower the LLM to Safeguard Itself](https://arxiv.org/abs/2310.15851)                                    |                       **Large Language Models**&**Jailbreak Attacks**&**Safety Mechanisms**                        |
| 23.11 |                                                               University of California Irvine                                                               |                             arxiv                              |                     [Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield](https://arxiv.org/abs/2311.00172)                     |                                **Adversarial Prompt Shield**&**Safety Classifier**                                 |
| 23.11 |                                                              Child Health Evaluative Sciences                                                               |                             arxiv                              |                         [Pyclipse, a library for deidentification of free-text clinical notes](https://arxiv.org/abs/2311.02748)                          |                                    **Clinical Text Data**&**Deidentification**                                     |
| 23.11 |                                                                     Tsinghua University                                                                     |                             arxiv                              |               [Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization](https://arxiv.org/abs/2311.09096)                |                            **Jailbreaking Attacks**&**Goal Prioritization**&**Safety**                             |
| 23.11 |                   University of Southern California, Harvard University, University of California Davis, University of Wisconsin-Madison                    |                             arxiv                              |            [Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations](https://arxiv.org/abs/2311.09763)            |                      **Backdoor Attacks**&**Defensive Demonstrations**&**Test-Time Defense**                       |
| 23.11 |                                                             University of Maryland College Park                                                             |                             arxiv                              |           [Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information](https://arxiv.org/abs/2311.11509)            |                 **Adversarial Prompt Detection**&**Perplexity Measures**&**Token-level Analysis**                  |
| 23.12 |                                                  Rensselaer Polytechnic Institute, Northeastern University                                                  |                             arxiv                              |                     [Combating Adversarial Attacks through a Conscience-Based Alignment Framework](https://arxiv.org/abs/2312.00029)                      |                         **Adversarial Attacks**&**Conscience-Based Alignment**&**Safety**                          |
| 23.12 |                                                     Azure Research, Microsoft Security Response Center                                                      |                             arXiv                              |                           [Maatphor: Automated Variant Analysis for Prompt Injection Attacks](https://arxiv.org/abs/2312.11513)                           |                            **Prompt Injection Attacks**&**Automated Variant Analysis**                             |
| 23.12 |                         University of Massachusetts Amherst, Columbia University, Google, Stanford University, New York University                          |                             arxiv                              |                           [Learning and Forgetting Unsafe Examples in Large Language Models](https://arxiv.org/abs/2312.12736)                            |                          **Safety Issues**&**ForgetFilter Algorithm**&**Unsafe Content**                           |
| 23.12 |                                                 UC Berkeley, King Abdulaziz City for Science and Technology                                                 |                             arXiv                              |                              [Jatmo: Prompt Injection Defense by Task-Specific Finetuning](https://arxiv.org/abs/2312.17673)                              |                                           Prompt Injection&LLM Security                                            |
| 24.01 |                                                                  Arizona State University                                                                   |                             arxiv                              | [The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness](https://arxiv.org/abs/2401.00287) |                              **Safety**&**Over-Defensiveness**&**Defense Strategies**                              |
| 24.01 |                                                    Logistic and Supply Chain MultiTech R&D Centre (LSCM)                                                    |                             arxiv                              |          [Detection and Defense Against Prominent Attacks on Preconditioned LLM-Integrated Virtual Assistants](https://arxiv.org/abs/2401.00994)          |                                       **Preconditioning**&**Cyber Security**                                       |
| 24.01 |            The Hong Kong University of Science and Technology, University of Illinois at Urbana-Champaign, The Hong Kong Polytechnic University             |                             arxiv                              |                          [MLLM-Protector: Ensuring MLLM‚Äôs Safety without Hurting Performance](https://arxiv.org/abs/2401.02906)                           |                   **Multimodal Large Language Models (MLLMs)**&**Safety**&**Malicious Attacks**                    |
| 24.01 |                                                                 Carnegie Mellon University                                                                  |                             arxiv                              |                                    [TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121)                                     |                                **Data Privacy**&**Ethical Concerns**&**Unlearning**                                |
| 24.01 |                                                         Wuhan University, The University of Sydney                                                          |                             arxiv                              |                  [Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender](https://arxiv.org/abs/2401.06561)                   |                              **Intention Analysis**&**Jailbreak Defense**&**Safety**                               |
| 24.01 |                                                            The Hong Kong Polytechnic University                                                             |                             arxiv                              |         [Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications](https://arxiv.org/abs/2401.07612)         |                                    **AI Security**&**Prompt Injection Attacks**                                    |
| 24.01 |                                              University of Illinois at Urbana-Champaign, University of Chicago                                              |                             arxiv                              |              [Robust Prompt Optimization for Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2401.17263)              |                          **AI Alignment**&**Jailbreaking**&**Robust Prompt Optimization**                          |
| 24.02 |                                                                  Arizona State University                                                                   |                             arxiv                              |                      [Adversarial Text Purification: A Large Language Model Approach for Defense](https://arxiv.org/abs/2402.06655)                       |                           **Textual Adversarial Defenses**&**Adversarial Purification**                            |
| 24.02 |                                                             Peking University, Wuhan University                                                             |                             arxiv                              |                             [Fight Back Against Jailbreaking via Prompt Adversarial Tuning](https://arxiv.org/abs/2402.06255)                             |                   **Jailbreaking Attacks**&**Prompt Adversarial Tuning**&**Defense Mechanisms**                    |
| 24.02 |                                     University of Washington, The Pennsylvania State University, Allen Institute for AI                                     |                             arxiv                              |                      [SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding](https://arxiv.org/abs/2402.08983)                      |                                  **Jailbreak Attacks**&**Safety-Aware Decoding**                                   |
| 24.02 |                                                         Shanghai Artificial Intelligence Laboratory                                                         |                             arxiv                              |                        [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283)                        |                                **LLM Conversation Safety**&**Attacks**&**Defenses**                                |
| 24.02 |                                     University of Notre Dame, INRIA&King Abdullah University of Science and Technology                                      |                             arxiv                              |                              [Defending Jailbreak Prompts via In-Context Adversarial Game](https://arxiv.org/abs/2402.13148)                              |                                   **Adversarial Training**&**Jailbreak Defense**                                   |
| 24.02 |             University of New South Wales Australia, Delft University of Technology The Netherlands&Nanyang Technological University Singapore              |                             arxiv                              |                        [LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study](https://arxiv.org/abs/2402.13457)                         |                                    **Jailbreak Attacks**&**Defense Techniques**                                    |
| 24.02 |                                             The Hong Kong University of Science and Technology, Duke University                                             |                             arxiv                              |                   [GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis](https://arxiv.org/abs/2402.13494)                   |                         **Safety-Critical Gradient Analysis**&**Unsafe Prompt Detection**                          |
| 24.02 |                                                                 The University of Melbourne                                                                 |                             arxiv                              |                   [Round Trip Translation Defence against Large Language Model Jailbreaking Attacks](https://arxiv.org/abs/2402.13517)                    |                              **Social-Engineered Attacks**&**Round Trip Translation**                              |
| 24.02 |                                                              Nanyang Technological University                                                               |                             arxiv                              |                 [LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper](https://arxiv.org/abs/2402.15727)                 |                                     **Jailbreaking Attacks**&**Self-Defense**                                      |
| 24.02 |                                                                       Ajou University                                                                       |                             arxiv                              |               [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement](https://arxiv.org/abs/2402.15180)               |                                     **Jailbreak Attacks**&**Self-Refinement**                                      |
| 24.02 |                                                                            UCLA                                                                             |                             arxiv                              |                            [Defending LLMs against Jailbreaking Attacks via Backtranslation](https://arxiv.org/abs/2402.16459)                            |                                    **Backtranslation**&**Jailbreaking Attacks**                                    |
| 24.02 |                                                           University of California Santa Barbara                                                            |                             arxiv                              |                   [Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing](https://arxiv.org/abs/2402.16192)                    |                                    **Semantic Smoothing**&**Jailbreak Attacks**                                    |
| 24.02 |                                                               University of Wisconsin-Madison                                                               |                             arxiv                              |                       [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment](https://arxiv.org/abs/2402.14968)                        |                         **Fine-tuning Attack**&**Backdoor Alignment**&**Safety Examples**                          |
| 24.02 |                                                                    University of Exeter                                                                     |                             arxiv                              |                    [Is the System Message Really Important to Jailbreaks in Large Language Models?](https://arxiv.org/abs/2402.14857)                     |                                         **Jailbreak**&**System Messages**                                          |
| 24.03 |                                                      The Chinese University of Hong Kong, IBM Research                                                      |                             arxiv                              |       [Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes](https://arxiv.org/abs/2403.00867)        |                              **Jailbreak Attacks**&**Refusal Loss**&**Gradient Cuff**                              |
| 24.03 |                           Oregon State University, Pennsylvania State University, CISPA Helmholtz Center for Information Security                           |                             arxiv                              |                            [AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks](https://arxiv.org/abs/2403.04783)                             |                                    **Defense Mechanisms**&**Jailbreak Attacks**                                    |
| 24.03 |                  Peking University, University of Wisconsin‚ÄìMadison, International Digital Economy Academy, University of California Davis                  |                             arxiv                              |  [AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting](https://arxiv.org/abs/2403.09513)   |                          **Multimodal Large Language Models Safety**&**Defense Strategy**                          |
| 24.03 |         Southern University of Science and Technology, Hong Kong University of Science and Technology, Huawei Noah‚Äôs Ark Lab, Peng Cheng Laboratory         |                             arxiv                              |                  [Eyes Closed Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation](https://arxiv.org/abs/2403.09572)                   |                                           **Multimodal LLMs**&**Safety**                                           |
| 24.03 |                                    UIUC, Virginia Tech, Salesforce Research, University of California Berkeley, UChicago                                    |                             arxiv                              |                  [RIGORLLM: RESILIENT GUARDRAILS FOR LARGE LANGUAGE MODELS AGAINST UNDESIRED CONTENT](https://arxiv.org/abs/2403.13031)                   |                              **Biases**&**Harmful Content**&**Resilient Guardrails**                               |
| 24.03 |                                                                          Microsoft                                                                          |                             arxiv                              |                         [Defending Against Indirect Prompt Injection Attacks With Spotlighting](https://arxiv.org/abs/2403.14720)                         |                                   **Indirect Prompt Injection**&**Spotlighting**                                   |
| 24.03 |                     XiaMen University, Yanshan University, IDEA Research, Inner Mongolia University, Microsoft, Microsoft Research Asia                     |                           NAACL2024                            |               [Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models](https://arxiv.org/abs/2403.11838)                |                           **Language Models**&**Safety Guidelines**&**Model Alignment**                            |
| 24.03 |                                                                          IIT Delhi                                                                          |                           NAACL2024                            |       [Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF](https://arxiv.org/abs/2403.10088)        | **Counterspeech Generation**&**Multi-Task Instruction Tuning**&**Reinforcement Learning from AI Feedback (RLAIF)** |
| 24.03 |                                                                   Stony Brook University                                                                    |                      NAACL2024(findings)                       |                              [Task-Agnostic Detector for Insertion-Based Backdoor Attacks](https://arxiv.org/abs/2403.17155)                              |                             **Backdoor Detection**&**Logit Features**&**NLP Security**                             |
| 24.03 |                                                                    Chung-Ang University                                                                     |                      NAACL2024(findings)                       |      [Don‚Äôt be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks](https://arxiv.org/abs/2403.15467)       |                  **Offensive Language Detection**&**Adversarial Attacks**&**Pooling Strategies**                   |
| 24.04 |                                                   South China University of Technology&Pazhou Laboratory                                                    |                             arxiv                              |                [Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge](https://arxiv.org/abs/2404.05880)                 |                                          **Jailbreaking**&**Unlearning**                                           |
| 24.04 |                                                        Zhejiang University, Johns Hopkins University                                                        |                             arxiv                              |                         [SAFEGEN: Mitigating Unsafe Content Generation in Text-to-Image Models](https://arxiv.org/abs/2404.06666)                         |                         **Text-to-Image Models**&**Unsafe Content**&**Content Mitigation**                         |
| 24.04 |                                            Hong Kong University of Science and Technology, University of Oxford                                             |                             arxiv                              |                             [Latent Guard: A Safety Framework for Text-to-Image Generation](https://arxiv.org/abs/2404.08031)                             |                           **Text-to-Image Models**&**Safety Framework**&**Latent Guard**                           |
| 24.04 | Nanjing University, Microsoft Research Asia, Tsinghua University, Queen Mary University of London, Pennsylvania State University,  NEC Laboratories America |                             arxiv                              |                                   [Protecting Your LLMs with Information Bottleneck](https://arxiv.org/abs/2404.13968)                                    |                                 **Information Bottleneck**&**Adversarial Defense**                                 |
| 24.04 |                                      Centre for Software Excellence Huawei, University of Manitoba, Queen‚Äôs University                                      |                             arxiv                              |                  [A Framework for Real-time Safeguarding the Text Generation of Large Language Models](https://arxiv.org/abs/2404.19048)                  |                               **Text Generation Safety**&**Real-time Safeguarding**                                |
| 24.04 |                                                              Princeton University&UC Davis&USC                                                              |                           NAACL2024                            |                 [Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors](https://arxiv.org/abs/2404.02356)                  |                       **Nested Product of Experts**&**Data Poisoning**&**Backdoor Defense**                        |
| 24.05 |                                                                     Tsinghua University                                                                     |                             arxiv                              |             [Learnable Linguistic Watermarks for Tracing Model Extraction Attacks on Large Language Models](https://arxiv.org/abs/2405.01509)             |                                   **Model Extraction Attacks**&**Watermarking**                                    |
| 24.05 |                                                                     Tsinghua University                                                                     |                             arxiv                              |                             [Adaptive and Robust Watermark against Model Extraction Attack](https://arxiv.org/abs/2405.02365)                             |                                   **Model Extraction Attacks**&**Watermarking**                                    |
| 24.05 |                                                                  Johns Hopkins University                                                                   |                           ICML 2024                            |                       [PARDEN: Can You Repeat That? Defending against Jailbreaks via Repetition](https://arxiv.org/abs/2405.07932)                        |                                             **Jailbreaks**&**Defense**                                             |
| 24.05 |                                                                   University of Edinburgh                                                                   |                             arxiv                              |                          [Spectral Editing of Activations for Large Language Model Alignment](https://arxiv.org/abs/2405.09719)                           |                       **Bias Mitigation**&**Truthfulness Enhancement**&**Spectral Editing**                        |
| 24.05 |                                                                East China Normal University                                                                 |                             arxiv                              |              [A Safety Realignment Framework via Subspace-Oriented Model Fusion for Large Language Models](https://arxiv.org/abs/2405.09055)              |                                      **Model Fusion**&**Safeguard Strategy**                                       |
| 24.05 |                                                     The Hong Kong University of Science and Technology                                                      |                             arxiv                              |                                 [Backdoor Removal for Generative Large Language Models](https://arxiv.org/abs/2405.07667)                                 |                           **Backdoor Attacks**&**Generative Models**&**Safety Training**                           |
| 24.05 |                                                                   Stony Brook University                                                                    |                             arxiv                              |                     [Robustifying Safety-Aligned Large Language Models through Clean Data Curation](https://arxiv.org/abs/2405.19358)                     |                     **Jailbreaking Attacks**&**Clean Data Curation**&**LLM Safety Alignment**                      |
| 24.05 |                                                                 University of Pennsylvania                                                                  |                             arxiv                              |                      [One-Shot Safety Alignment for Large Language Models via Optimal Dualization](https://arxiv.org/abs/2405.19544)                      |                         **Safety Alignment**&**Constrained RLHF**&**Optimal Dualization**                          |
| 24.05 |                                                                            Naver                                                                            |                             arxiv                              |                           [SLM as Guardian: Pioneering AI Safety with Small Language Models](https://arxiv.org/abs/2405.19795)                            |                        **AI Safety**&**Small Language Models**&**Harmful Query Detection**                         |
| 24.05 |                                                             The Chinese University of Hong Kong                                                             |                             arxiv                              |             [Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks](https://arxiv.org/abs/2405.20099)              |                         **Jailbreak Attacks**&**Defensive Prompt Patch**&**LLM Security**                          |
| 24.05 |                                                       Tokyo University of Agriculture and Technology                                                        |                             arxiv                              |                     [Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent](https://arxiv.org/abs/2405.20770)                      |                            **Adversarial Robustness**&**LLM Agent**&**Textual Attacks**                            |
| 24.06 |                                                             University of California, Riverside                                                             |                             arxiv                              |                           [Cross-Modal Safety Alignment: Is Textual Unlearning All You Need?](https://arxiv.org/abs/2406.02575)                           |               **Cross-Modality Safety Alignment**&**Textual Unlearning**&**Vision-Language Models**                |
| 24.06 |                                                                   University of Liverpool                                                                   | IEEE Transactions on Pattern Analysis and Machine Intelligence |                                     [Safeguarding Large Language Models: A Survey](https://arxiv.org/abs/2406.02622)                                      |                                    **Survey**&**Safeguards**&**Trustworthy AI**                                    |
| 24.06 |                                                                   Oregon State University                                                                   |                             arxiv                              |               [Defending Large Language Models Against Attacks With Residual Stream Activation Analysis](https://arxiv.org/abs/2406.03230)                |                               **Adversarial Machine Learning**&**Machine Learning**                                |
| 24.06 |                                                        Huazhong University of Science and Technology                                                        |                             arxiv                              |                   [AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens](https://arxiv.org/abs/2406.03805)                   |                                   **Jailbreak Attacks**&**Dependency Analysis**                                    |
| 24.06 |                                                     The Hong Kong University of Science and Technology                                                      |                             arxiv                              |                   [SELFDEFEND: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner](https://arxiv.org/abs/2406.05498)                   |                                      **Jailbreaking Defense**&**SELFDEFEND**                                       |
| 24.06 |                                                                    Princeton University                                                                     |                             arxiv                              |                           [Safety Alignment Should Be Made More Than Just a Few Tokens Deep](https://arxiv.org/abs/2406.05946)                            |                                    **Safety Alignment**&**Adversarial Attacks**                                    |
| 24.06 |                                                           The University of Alabama at Birmingham                                                           |                             arxiv                              |                        [Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models](https://arxiv.org/abs/2406.05948)                        |                                     **Backdoor Attacks**&**Chain-of-Scrutiny**                                     |
| 24.06 |                                               The Hong Kong University of Science and Technology (Guangzhou)                                                |                             arxiv                              |                           [Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs](https://arxiv.org/abs/2406.06622)                            |                                    **Jailbreak Attacks**&**Adversarial Tuning**                                    |
| 24.06 |                                                                         Komorebi AI                                                                         |                             arxiv                              |                               [Merging Improves Self-Critique Against Jailbreak Attacks](https://arxiv.org/abs/2406.07188)                                |                                      **Jailbreak Attacks**&**Self-Critique**                                       |
| 24.06 |                                                       Tsinghua Shenzhen International Graduate School                                                       |                             arxiv                              |              [MLLMGUARD: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models](https://arxiv.org/abs/2406.07594)              |                               **Safety Evaluation**&**MLLMs**&**Multi-dimensional**                                |
| 24.06 |                                                                      Purdue University                                                                      |                             arxiv                              |                  [RL-JACK: Reinforcement Learning-powered Black-box Jailbreaking Attack against LLMs](https://arxiv.org/abs/2406.08725)                   |                                **Jailbreaking Attacks**&**Reinforcement Learning**                                 |
| 24.06 |                                                   Mohamed bin Zayed University of Artificial Intelligence                                                   |                             arxiv                              |                         [MirrorCheck: Efficient Adversarial Defense for Vision-Language Models](https://arxiv.org/abs/2406.09250)                         |                         **Adversarial Defense**&**Vision-Language Models**&**MirrorCheck**                         |
| 24.06 |                                                                     Teesside University                                                                     |                             arxiv                              |                [Threat Modelling and Risk Analysis for Large Language Model (LLM)-Powered Applications](https://arxiv.org/abs/2406.11007)                 |                        **Threat Modelling**&**Risk Analysis**&**LLM-Powered Applications**                         |
| 24.06 |                                                                     NVIDIA Corporation                                                                      |                             arxiv                              |                             [garak: A Framework for Security Probing Large Language Models](https://arxiv.org/abs/2406.11036)                             |                                           **garak**&**Security Probing**                                           |
| 24.06 |                                                        University of Science and Technology of China                                                        |                             arxiv                              |              [Self and Cross-Model Distillation for LLMs: Effective Methods for Refusal Pattern Alignment](https://arxiv.org/abs/2406.11285)              |                  **Self Distillation**&**Cross-Model Distillation**&**Refusal Pattern Alignment**                  |
| 24.06 |                                                                  University of Washington                                                                   |                             arxiv                              |                  [CLEANGEN: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models](https://arxiv.org/abs/2406.12257)                  |                               **CLEANGEN**&**Backdoor Attacks**&**Generation Tasks**                               |
| 24.06 |                                                                     Columbia University                                                                     |                             arxiv                              |                            [Defending Against Social Engineering Attacks in the Age of LLMs](https://arxiv.org/abs/2406.12263)                            |                                      **Social Engineering**&**CSE Detection**                                      |
| 24.06 |                                                          Indian Institute of Technology Kharagpur                                                           |                             arxiv                              |                 [SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models](https://arxiv.org/abs/2406.12274)                  |                          **SafeInfer**&**Context Adaptive Decoding**&**Safety Alignment**                          |
| 24.06 |                                                                 Chinese Academy of Sciences                                                                 |                             arxiv                              | [Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization](https://arxiv.org/abs/2406.16743) |                                   **Safety Alignment**&**Contrastive Decoding**                                    |
| 24.07 |                                                                    University of Toronto                                                                    |                             arxiv                              |                      [A False Sense of Safety: Unsafe Information Leakage in ‚ÄòSafe‚Äô AI Responses](https://arxiv.org/abs/2407.02551)                       |                                   **Jailbreak Attacks**&**Information Leakage**                                    |
| 24.07 |                                                                     Tsinghua University                                                                     |                             arxiv                              |       [Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks](https://arxiv.org/abs/2407.02855)        |                                        **Jailbreak Attacks**&**Unlearning**                                        |
| 24.07 |                                                              National University of Singapore                                                               |                             arxiv                              |                           [Self-Evaluation as a Defense Against Adversarial Attacks on LLMs](https://arxiv.org/abs/2407.03234)                            |                                    **Adversarial Attacks**&**Self-Evaluation**                                     |
| 24.07 |                                                                     Tianjin University                                                                      |                             arxiv                              |                              [DART: Deep Adversarial Automated Red Teaming for LLM Safety](https://arxiv.org/abs/2407.03876)                              |                         **Automated Red Teaming**&**Adversarial Training**&**LLM Safety**                          |
| 24.07 |                                                                  Seoul National University                                                                  |                       ACL 2024 Workshop                        |                       [Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders](https://arxiv.org/abs/2407.06851)                        |                       **Sentence Encoders**&**Safety-Critical Knowledge**&**Unsafe Prompts**                       |
| 24.07 |                                                                       Duke University                                                                       |                             arxiv                              |                              [Refusing Safe Prompts for Multi-modal Large Language Models](https://arxiv.org/abs/2407.09050)                              |                        **Multimodal LLMs**&**Safe Prompt Refusal**&**Adversarial Attacks**                         |
| 24.07 |                                                             The Chinese University of Hong Kong                                                             |                             arxiv                              |               [Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training](https://arxiv.org/abs/2407.09121)                |                    **Safety Training**&**Refusal Position Bias**&**Decoupled Refusal Training**                    |
| 24.07 |                                                                      GovTech Singapore                                                                      |                             arxiv                              |             [LionGuard: Building a Contextualized Moderation Classifier to Tackle Localized Unsafe Content](https://arxiv.org/abs/2407.10995)             |                             **Moderation Classifier**&**Localized Content**&**Safety**                             |
| 24.07 |                                                               Georgia Institute of Technology                                                               |                             arxiv                              |           [Targeted Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs](https://arxiv.org/abs/2407.15549)            |                                   **Latent Adversarial Training**&**Robustness**                                   |
| 24.07 |                                                               Georgia Institute of Technology                                                               |                             arxiv                              |          [A Voter-Based Stochastic Rejection-Method Framework for Asymptotically Safe Language Model Outputs](https://arxiv.org/abs/2407.16994)           |                                     **Stochastic Rejection-Method**&**Safety**                                     |
| 24.07 |                                                                Shanghai Jiao Tong University                                                                |                             arxiv                              |                                       [SAFETY-J: Evaluating Safety with Critique](https://arxiv.org/abs/2407.17075)                                       |                     **Safety Evaluation**&**Critique-based Judgment**&**Bilingual Evaluator**                      |
| 24.07 |                                                                          Dynamo AI                                                                          |                       ICML 2024 Workshop                       |                             [PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing](https://arxiv.org/abs/2407.16318)                             |                              **Inference-Time Guardrails**&**Safety**&**Helpfulness**                              |
| 24.07 |                                                                   ShanghaiTech University                                                                   |                             arxiv                              |                      [Defending Jailbreak Attack in VLMs via Cross-modality Information Detector](https://arxiv.org/abs/2407.21659)                       |               **Jailbreak Attacks**&**Vision Language Models (VLMs)**&**Cross-modality Information**               |
| 24.08 |                                                           University of Illinois Urbana-Champaign                                                           |                             arxiv                              |                                   [Tamper-Resistant Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2408.00761)                                    |                             **Tamper Resistance**&**Security**&**Adversarial Attacks**                             |

## üíªPresentations & Talk


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars
