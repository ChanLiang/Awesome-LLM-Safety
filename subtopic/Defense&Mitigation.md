# Defense

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                                          Institute                                                                          |                          Publication                           |                                                                                  Paper                                                                                  |                                                      Keywords                                                      |
|:-----:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------:|
| 21.07 |                                                                       Google Research                                                                       |                            ACL2022                             |                                 [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)                                 |                              **Privacy Protected**&**Deduplication**&**Memorization**                              |
| 23.05 |                                                                  UC Davis, USC, UW-Madison                                                                  |                           NAACL2024                            |                                   [From Shortcuts to Triggers: Backdoor Defense with Denoised PoE](https://arxiv.org/abs/2305.14910)                                    |                      **Backdoor Attacks**&**Defense Methods**&**Natural Language Processing**                      |
| 23.08 |                                                                  Georgia Tech, Intel Labs                                                                   |                             arxiv                              |                               [LLM Self Defense: By Self Examination LLMs Know They Are Being Tricked](https://arxiv.org/abs/2308.07308)                                |                       **Adversarial Attacks**&**Self Defense**&**Harmful Content Detection**                       |
| 23.08 |                                                                   University of Michigan                                                                    |                             arxiv                              |                                         [DETECTING LANGUAGE MODEL ATTACKS WITH PERPLEXITY](https://arxiv.org/abs/2308.14132v3)                                          |                            **Adversarial Suffixes**&**Perplexity**&**Attack Detection**                            |
| 23.09 |                                                                   University of Maryland                                                                    |                             arxiv                              |                                         [Certifying LLM Safety against Adversarial Prompting](https://arxiv.org/abs/2309.02705)                                         |                                     **Safety Filter**&**Adversarial Prompts**                                      |
| 23.09 |                                                                   University of Maryland                                                                    |                             arxiv                              |                              [BASELINE DEFENSES FOR ADVERSARIAL ATTACKS AGAINST ALIGNED LANGUAGE MODELS](https://arxiv.org/abs/2309.00614)                              |                          **Perplexity**&**Input Preprocessing**&**Adversarial Training**                           |
| 23.09 |                                                              The Pennsylvania State University                                                              |                             arxiv                              |                                [DEFENDING AGAINST ALIGNMENT-BREAKING ATTACKS VIA ROBUSTLY ALIGNED LLM](https://arxiv.org/abs/2309.14348)                                |                  **Alignment-Breaking Attacks**&**Adversarial Prompts**&**Jailbreaking Prompts**                   |
| 23.10 |                                                                 University of Pennsylvania                                                                  |                             arxiv                              |                               [SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2310.03684)                               |                               **Jailbreak**&**Adversarial Attack**&**Perturbation**                                |
| 23.10 |                                                                  Michigan State University                                                                  |                             arXiv                              |                                [Jailbreaker in Jail: Moving Target Defense for Large Language Models](https://arxiv.org/abs/2310.02417)                                 |                   **Dialogue System**&**Trustworthy Machine Learning**&**Moving Target Defense**                   |
| 23.10 |                                                                      Peking University                                                                      |                             arxiv                              |                         [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations](https://arxiv.org/abs/2310.06387)                         |                   **In-Context Learning**&**Adversarial Attacks**&**In-Context Demonstrations**                    |
| 23.10 |                                                        The Chinese University of Hong Kong&Microsoft                                                        |                           NAACL2024                            |                                           [SELF-GUARD: Empower the LLM to Safeguard Itself](https://arxiv.org/abs/2310.15851)                                           |                       **Large Language Models**&**Jailbreak Attacks**&**Safety Mechanisms**                        |
| 23.11 |                                                               University of California Irvine                                                               |                             arxiv                              |                            [Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield](https://arxiv.org/abs/2311.00172)                            |                                **Adversarial Prompt Shield**&**Safety Classifier**                                 |
| 23.11 |                                                              Child Health Evaluative Sciences                                                               |                             arxiv                              |                                [Pyclipse, a library for deidentification of free-text clinical notes](https://arxiv.org/abs/2311.02748)                                 |                                    **Clinical Text Data**&**Deidentification**                                     |
| 23.11 |                                                                     Tsinghua University                                                                     |                             arxiv                              |                      [Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization](https://arxiv.org/abs/2311.09096)                       |                            **Jailbreaking Attacks**&**Goal Prioritization**&**Safety**                             |
| 23.11 |                   University of Southern California, Harvard University, University of California Davis, University of Wisconsin-Madison                    |                             arxiv                              |                   [Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations](https://arxiv.org/abs/2311.09763)                   |                      **Backdoor Attacks**&**Defensive Demonstrations**&**Test-Time Defense**                       |
| 23.11 |                                                             University of Maryland College Park                                                             |                             arxiv                              |                  [Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information](https://arxiv.org/abs/2311.11509)                   |                 **Adversarial Prompt Detection**&**Perplexity Measures**&**Token-level Analysis**                  |
| 23.12 |                                                  Rensselaer Polytechnic Institute, Northeastern University                                                  |                             arxiv                              |                            [Combating Adversarial Attacks through a Conscience-Based Alignment Framework](https://arxiv.org/abs/2312.00029)                             |                         **Adversarial Attacks**&**Conscience-Based Alignment**&**Safety**                          |
| 23.12 |                                                     Azure Research, Microsoft Security Response Center                                                      |                             arXiv                              |                                  [Maatphor: Automated Variant Analysis for Prompt Injection Attacks](https://arxiv.org/abs/2312.11513)                                  |                            **Prompt Injection Attacks**&**Automated Variant Analysis**                             |
| 23.12 |                         University of Massachusetts Amherst, Columbia University, Google, Stanford University, New York University                          |                             arxiv                              |                                  [Learning and Forgetting Unsafe Examples in Large Language Models](https://arxiv.org/abs/2312.12736)                                   |                          **Safety Issues**&**ForgetFilter Algorithm**&**Unsafe Content**                           |
| 23.12 |                                                 UC Berkeley, King Abdulaziz City for Science and Technology                                                 |                             arXiv                              |                                     [Jatmo: Prompt Injection Defense by Task-Specific Finetuning](https://arxiv.org/abs/2312.17673)                                     |                                           Prompt Injection&LLM Security                                            |
| 24.01 |                                                                  Arizona State University                                                                   |                             arxiv                              |        [The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness](https://arxiv.org/abs/2401.00287)        |                              **Safety**&**Over-Defensiveness**&**Defense Strategies**                              |
| 24.01 |                                                    Logistic and Supply Chain MultiTech R&D Centre (LSCM)                                                    |                             arxiv                              |                 [Detection and Defense Against Prominent Attacks on Preconditioned LLM-Integrated Virtual Assistants](https://arxiv.org/abs/2401.00994)                 |                                       **Preconditioning**&**Cyber Security**                                       |
| 24.01 |            The Hong Kong University of Science and Technology, University of Illinois at Urbana-Champaign, The Hong Kong Polytechnic University             |                             arxiv                              |                                 [MLLM-Protector: Ensuring MLLM‚Äôs Safety without Hurting Performance](https://arxiv.org/abs/2401.02906)                                  |                   **Multimodal Large Language Models (MLLMs)**&**Safety**&**Malicious Attacks**                    |
| 24.01 |                                                                 Carnegie Mellon University                                                                  |                             arxiv                              |                                           [TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121)                                            |                                **Data Privacy**&**Ethical Concerns**&**Unlearning**                                |
| 24.01 |                                                         Wuhan University, The University of Sydney                                                          |                             arxiv                              |                         [Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender](https://arxiv.org/abs/2401.06561)                          |                              **Intention Analysis**&**Jailbreak Defense**&**Safety**                               |
| 24.01 |                                                            The Hong Kong Polytechnic University                                                             |                             arxiv                              |                [Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications](https://arxiv.org/abs/2401.07612)                |                                    **AI Security**&**Prompt Injection Attacks**                                    |
| 24.01 |                                              University of Illinois at Urbana-Champaign, University of Chicago                                              |                             arxiv                              |                     [Robust Prompt Optimization for Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2401.17263)                     |                          **AI Alignment**&**Jailbreaking**&**Robust Prompt Optimization**                          |
| 24.02 |                                                                  Arizona State University                                                                   |                             arxiv                              |                             [Adversarial Text Purification: A Large Language Model Approach for Defense](https://arxiv.org/abs/2402.06655)                              |                           **Textual Adversarial Defenses**&**Adversarial Purification**                            |
| 24.02 |                                                             Peking University, Wuhan University                                                             |                             arxiv                              |                                    [Fight Back Against Jailbreaking via Prompt Adversarial Tuning](https://arxiv.org/abs/2402.06255)                                    |                   **Jailbreaking Attacks**&**Prompt Adversarial Tuning**&**Defense Mechanisms**                    |
| 24.02 |                                     University of Washington, The Pennsylvania State University, Allen Institute for AI                                     |                             arxiv                              |                             [SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding](https://arxiv.org/abs/2402.08983)                             |                                  **Jailbreak Attacks**&**Safety-Aware Decoding**                                   |
| 24.02 |                                                         Shanghai Artificial Intelligence Laboratory                                                         |                             arxiv                              |                               [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283)                               |                                **LLM Conversation Safety**&**Attacks**&**Defenses**                                |
| 24.02 |                                     University of Notre Dame, INRIA&King Abdullah University of Science and Technology                                      |                             arxiv                              |                                     [Defending Jailbreak Prompts via In-Context Adversarial Game](https://arxiv.org/abs/2402.13148)                                     |                                   **Adversarial Training**&**Jailbreak Defense**                                   |
| 24.02 |             University of New South Wales Australia, Delft University of Technology The Netherlands&Nanyang Technological University Singapore              |                             arxiv                              |                               [LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study](https://arxiv.org/abs/2402.13457)                                |                                    **Jailbreak Attacks**&**Defense Techniques**                                    |
| 24.02 |                                             The Hong Kong University of Science and Technology, Duke University                                             |                             arxiv                              |                          [GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis](https://arxiv.org/abs/2402.13494)                          |                         **Safety-Critical Gradient Analysis**&**Unsafe Prompt Detection**                          |
| 24.02 |                                                                 The University of Melbourne                                                                 |                             arxiv                              |                          [Round Trip Translation Defence against Large Language Model Jailbreaking Attacks](https://arxiv.org/abs/2402.13517)                           |                              **Social-Engineered Attacks**&**Round Trip Translation**                              |
| 24.02 |                                                              Nanyang Technological University                                                               |                             arxiv                              |                        [LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper](https://arxiv.org/abs/2402.15727)                        |                                     **Jailbreaking Attacks**&**Self-Defense**                                      |
| 24.02 |                                                                       Ajou University                                                                       |                             arxiv                              |                      [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement](https://arxiv.org/abs/2402.15180)                      |                                     **Jailbreak Attacks**&**Self-Refinement**                                      |
| 24.02 |                                                                            UCLA                                                                             |                             arxiv                              |                                   [Defending LLMs against Jailbreaking Attacks via Backtranslation](https://arxiv.org/abs/2402.16459)                                   |                                    **Backtranslation**&**Jailbreaking Attacks**                                    |
| 24.02 |                                                           University of California Santa Barbara                                                            |                             arxiv                              |                          [Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing](https://arxiv.org/abs/2402.16192)                           |                                    **Semantic Smoothing**&**Jailbreak Attacks**                                    |
| 24.02 |                                                               University of Wisconsin-Madison                                                               |                             arxiv                              |                              [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment](https://arxiv.org/abs/2402.14968)                               |                         **Fine-tuning Attack**&**Backdoor Alignment**&**Safety Examples**                          |
| 24.02 |                                                                    University of Exeter                                                                     |                             arxiv                              |                           [Is the System Message Really Important to Jailbreaks in Large Language Models?](https://arxiv.org/abs/2402.14857)                            |                                         **Jailbreak**&**System Messages**                                          |
| 24.03 |                                                      The Chinese University of Hong Kong, IBM Research                                                      |                             arxiv                              |              [Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes](https://arxiv.org/abs/2403.00867)               |                              **Jailbreak Attacks**&**Refusal Loss**&**Gradient Cuff**                              |
| 24.03 |                           Oregon State University, Pennsylvania State University, CISPA Helmholtz Center for Information Security                           |                             arxiv                              |                                   [AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks](https://arxiv.org/abs/2403.04783)                                    |                                    **Defense Mechanisms**&**Jailbreak Attacks**                                    |
| 24.03 |                  Peking University, University of Wisconsin‚ÄìMadison, International Digital Economy Academy, University of California Davis                  |                             arxiv                              |         [AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting](https://arxiv.org/abs/2403.09513)          |                          **Multimodal Large Language Models Safety**&**Defense Strategy**                          |
| 24.03 |         Southern University of Science and Technology, Hong Kong University of Science and Technology, Huawei Noah‚Äôs Ark Lab, Peng Cheng Laboratory         |                             arxiv                              |                         [Eyes Closed Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation](https://arxiv.org/abs/2403.09572)                          |                                           **Multimodal LLMs**&**Safety**                                           |
| 24.03 |                                    UIUC, Virginia Tech, Salesforce Research, University of California Berkeley, UChicago                                    |                             arxiv                              |                         [RIGORLLM: RESILIENT GUARDRAILS FOR LARGE LANGUAGE MODELS AGAINST UNDESIRED CONTENT](https://arxiv.org/abs/2403.13031)                          |                              **Biases**&**Harmful Content**&**Resilient Guardrails**                               |
| 24.03 |                                                                          Microsoft                                                                          |                             arxiv                              |                                [Defending Against Indirect Prompt Injection Attacks With Spotlighting](https://arxiv.org/abs/2403.14720)                                |                                   **Indirect Prompt Injection**&**Spotlighting**                                   |
| 24.03 |                     XiaMen University, Yanshan University, IDEA Research, Inner Mongolia University, Microsoft, Microsoft Research Asia                     |                           NAACL2024                            |                      [Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models](https://arxiv.org/abs/2403.11838)                       |                           **Language Models**&**Safety Guidelines**&**Model Alignment**                            |
| 24.03 |                                                                          IIT Delhi                                                                          |                           NAACL2024                            |              [Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF](https://arxiv.org/abs/2403.10088)               | **Counterspeech Generation**&**Multi-Task Instruction Tuning**&**Reinforcement Learning from AI Feedback (RLAIF)** |
| 24.03 |                                                                   Stony Brook University                                                                    |                      NAACL2024(findings)                       |                                     [Task-Agnostic Detector for Insertion-Based Backdoor Attacks](https://arxiv.org/abs/2403.17155)                                     |                             **Backdoor Detection**&**Logit Features**&**NLP Security**                             |
| 24.03 |                                                                    Chung-Ang University                                                                     |                      NAACL2024(findings)                       |             [Don‚Äôt be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks](https://arxiv.org/abs/2403.15467)              |                  **Offensive Language Detection**&**Adversarial Attacks**&**Pooling Strategies**                   |
| 24.04 |                                                   South China University of Technology&Pazhou Laboratory                                                    |                             arxiv                              |                       [Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge](https://arxiv.org/abs/2404.05880)                        |                                          **Jailbreaking**&**Unlearning**                                           |
| 24.04 |                                                        Zhejiang University, Johns Hopkins University                                                        |                             arxiv                              |                                [SAFEGEN: Mitigating Unsafe Content Generation in Text-to-Image Models](https://arxiv.org/abs/2404.06666)                                |                         **Text-to-Image Models**&**Unsafe Content**&**Content Mitigation**                         |
| 24.04 |                                            Hong Kong University of Science and Technology, University of Oxford                                             |                             arxiv                              |                                    [Latent Guard: A Safety Framework for Text-to-Image Generation](https://arxiv.org/abs/2404.08031)                                    |                           **Text-to-Image Models**&**Safety Framework**&**Latent Guard**                           |
| 24.04 | Nanjing University, Microsoft Research Asia, Tsinghua University, Queen Mary University of London, Pennsylvania State University,  NEC Laboratories America |                             arxiv                              |                                          [Protecting Your LLMs with Information Bottleneck](https://arxiv.org/abs/2404.13968)                                           |                                 **Information Bottleneck**&**Adversarial Defense**                                 |
| 24.04 |                                      Centre for Software Excellence Huawei, University of Manitoba, Queen‚Äôs University                                      |                             arxiv                              |                         [A Framework for Real-time Safeguarding the Text Generation of Large Language Models](https://arxiv.org/abs/2404.19048)                         |                               **Text Generation Safety**&**Real-time Safeguarding**                                |
| 24.04 |                                                              Princeton University&UC Davis&USC                                                              |                           NAACL2024                            |                        [Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors](https://arxiv.org/abs/2404.02356)                         |                       **Nested Product of Experts**&**Data Poisoning**&**Backdoor Defense**                        |
| 24.05 |                                                                     Tsinghua University                                                                     |                             arxiv                              |                    [Learnable Linguistic Watermarks for Tracing Model Extraction Attacks on Large Language Models](https://arxiv.org/abs/2405.01509)                    |                                   **Model Extraction Attacks**&**Watermarking**                                    |
| 24.05 |                                                                     Tsinghua University                                                                     |                             arxiv                              |                                    [Adaptive and Robust Watermark against Model Extraction Attack](https://arxiv.org/abs/2405.02365)                                    |                                   **Model Extraction Attacks**&**Watermarking**                                    |
| 24.05 |                                                                  Johns Hopkins University                                                                   |                           ICML 2024                            |                              [PARDEN: Can You Repeat That? Defending against Jailbreaks via Repetition](https://arxiv.org/abs/2405.07932)                               |                                             **Jailbreaks**&**Defense**                                             |
| 24.05 |                                                                   University of Edinburgh                                                                   |                             arxiv                              |                                 [Spectral Editing of Activations for Large Language Model Alignment](https://arxiv.org/abs/2405.09719)                                  |                       **Bias Mitigation**&**Truthfulness Enhancement**&**Spectral Editing**                        |
| 24.05 |                                                                East China Normal University                                                                 |                             arxiv                              |                     [A Safety Realignment Framework via Subspace-Oriented Model Fusion for Large Language Models](https://arxiv.org/abs/2405.09055)                     |                                      **Model Fusion**&**Safeguard Strategy**                                       |
| 24.05 |                                                     The Hong Kong University of Science and Technology                                                      |                             arxiv                              |                                        [Backdoor Removal for Generative Large Language Models](https://arxiv.org/abs/2405.07667)                                        |                           **Backdoor Attacks**&**Generative Models**&**Safety Training**                           |
| 24.05 |                                                                   Stony Brook University                                                                    |                             arxiv                              |                            [Robustifying Safety-Aligned Large Language Models through Clean Data Curation](https://arxiv.org/abs/2405.19358)                            |                     **Jailbreaking Attacks**&**Clean Data Curation**&**LLM Safety Alignment**                      |
| 24.05 |                                                                 University of Pennsylvania                                                                  |                             arxiv                              |                             [One-Shot Safety Alignment for Large Language Models via Optimal Dualization](https://arxiv.org/abs/2405.19544)                             |                         **Safety Alignment**&**Constrained RLHF**&**Optimal Dualization**                          |
| 24.05 |                                                                            Naver                                                                            |                             arxiv                              |                                  [SLM as Guardian: Pioneering AI Safety with Small Language Models](https://arxiv.org/abs/2405.19795)                                   |                        **AI Safety**&**Small Language Models**&**Harmful Query Detection**                         |
| 24.05 |                                                             The Chinese University of Hong Kong                                                             |                             arxiv                              |                    [Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks](https://arxiv.org/abs/2405.20099)                     |                         **Jailbreak Attacks**&**Defensive Prompt Patch**&**LLM Security**                          |
| 24.05 |                                                       Tokyo University of Agriculture and Technology                                                        |                             arxiv                              |                            [Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent](https://arxiv.org/abs/2405.20770)                             |                            **Adversarial Robustness**&**LLM Agent**&**Textual Attacks**                            |
| 24.06 |                                                             University of California, Riverside                                                             |                             arxiv                              |                                  [Cross-Modal Safety Alignment: Is Textual Unlearning All You Need?](https://arxiv.org/abs/2406.02575)                                  |               **Cross-Modality Safety Alignment**&**Textual Unlearning**&**Vision-Language Models**                |
| 24.06 |                                                                   University of Liverpool                                                                   | IEEE Transactions on Pattern Analysis and Machine Intelligence |                                            [Safeguarding Large Language Models: A Survey](https://arxiv.org/abs/2406.02622)                                             |                                    **Survey**&**Safeguards**&**Trustworthy AI**                                    |
| 24.06 |                                                                   Oregon State University                                                                   |                             arxiv                              |                      [Defending Large Language Models Against Attacks With Residual Stream Activation Analysis](https://arxiv.org/abs/2406.03230)                       |                               **Adversarial Machine Learning**&**Machine Learning**                                |
| 24.06 |                                                        Huazhong University of Science and Technology                                                        |                             arxiv                              |                          [AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens](https://arxiv.org/abs/2406.03805)                          |                                   **Jailbreak Attacks**&**Dependency Analysis**                                    |
| 24.06 |                                                     The Hong Kong University of Science and Technology                                                      |                             arxiv                              |                          [SELFDEFEND: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner](https://arxiv.org/abs/2406.05498)                          |                                      **Jailbreaking Defense**&**SELFDEFEND**                                       |
| 24.06 |                                                                    Princeton University                                                                     |                             arxiv                              |                                  [Safety Alignment Should Be Made More Than Just a Few Tokens Deep](https://arxiv.org/abs/2406.05946)                                   |                                    **Safety Alignment**&**Adversarial Attacks**                                    |
| 24.06 |                                                           The University of Alabama at Birmingham                                                           |                             arxiv                              |                               [Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models](https://arxiv.org/abs/2406.05948)                               |                                     **Backdoor Attacks**&**Chain-of-Scrutiny**                                     |
| 24.06 |                                               The Hong Kong University of Science and Technology (Guangzhou)                                                |                             arxiv                              |                                  [Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs](https://arxiv.org/abs/2406.06622)                                   |                                    **Jailbreak Attacks**&**Adversarial Tuning**                                    |
| 24.06 |                                                                         Komorebi AI                                                                         |                             arxiv                              |                                      [Merging Improves Self-Critique Against Jailbreak Attacks](https://arxiv.org/abs/2406.07188)                                       |                                      **Jailbreak Attacks**&**Self-Critique**                                       |
| 24.06 |                                                       Tsinghua Shenzhen International Graduate School                                                       |                             arxiv                              |                     [MLLMGUARD: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models](https://arxiv.org/abs/2406.07594)                     |                               **Safety Evaluation**&**MLLMs**&**Multi-dimensional**                                |
| 24.06 |                                                                      Purdue University                                                                      |                             arxiv                              |                         [RL-JACK: Reinforcement Learning-powered Black-box Jailbreaking Attack against LLMs](https://arxiv.org/abs/2406.08725)                          |                                **Jailbreaking Attacks**&**Reinforcement Learning**                                 |
| 24.06 |                                                   Mohamed bin Zayed University of Artificial Intelligence                                                   |                             arxiv                              |                                [MirrorCheck: Efficient Adversarial Defense for Vision-Language Models](https://arxiv.org/abs/2406.09250)                                |                         **Adversarial Defense**&**Vision-Language Models**&**MirrorCheck**                         |
| 24.06 |                                                                     Teesside University                                                                     |                             arxiv                              |                       [Threat Modelling and Risk Analysis for Large Language Model (LLM)-Powered Applications](https://arxiv.org/abs/2406.11007)                        |                        **Threat Modelling**&**Risk Analysis**&**LLM-Powered Applications**                         |
| 24.06 |                                                                     NVIDIA Corporation                                                                      |                             arxiv                              |                                    [garak: A Framework for Security Probing Large Language Models](https://arxiv.org/abs/2406.11036)                                    |                                           **garak**&**Security Probing**                                           |
| 24.06 |                                                        University of Science and Technology of China                                                        |                             arxiv                              |                     [Self and Cross-Model Distillation for LLMs: Effective Methods for Refusal Pattern Alignment](https://arxiv.org/abs/2406.11285)                     |                  **Self Distillation**&**Cross-Model Distillation**&**Refusal Pattern Alignment**                  |
| 24.06 |                                                                  University of Washington                                                                   |                             arxiv                              |                         [CLEANGEN: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models](https://arxiv.org/abs/2406.12257)                         |                               **CLEANGEN**&**Backdoor Attacks**&**Generation Tasks**                               |
| 24.06 |                                                                     Columbia University                                                                     |                             arxiv                              |                                   [Defending Against Social Engineering Attacks in the Age of LLMs](https://arxiv.org/abs/2406.12263)                                   |                                      **Social Engineering**&**CSE Detection**                                      |
| 24.06 |                                                          Indian Institute of Technology Kharagpur                                                           |                             arxiv                              |                        [SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models](https://arxiv.org/abs/2406.12274)                         |                          **SafeInfer**&**Context Adaptive Decoding**&**Safety Alignment**                          |
| 24.06 |                                                                 Chinese Academy of Sciences                                                                 |                             arxiv                              |        [Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization](https://arxiv.org/abs/2406.16743)        |                                   **Safety Alignment**&**Contrastive Decoding**                                    |
| 24.07 |                                                                    University of Toronto                                                                    |                             arxiv                              |                             [A False Sense of Safety: Unsafe Information Leakage in ‚ÄòSafe‚Äô AI Responses](https://arxiv.org/abs/2407.02551)                              |                                   **Jailbreak Attacks**&**Information Leakage**                                    |
| 24.07 |                                                                     Tsinghua University                                                                     |                             arxiv                              |              [Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks](https://arxiv.org/abs/2407.02855)               |                                        **Jailbreak Attacks**&**Unlearning**                                        |
| 24.07 |                                                              National University of Singapore                                                               |                             arxiv                              |                                  [Self-Evaluation as a Defense Against Adversarial Attacks on LLMs](https://arxiv.org/abs/2407.03234)                                   |                                    **Adversarial Attacks**&**Self-Evaluation**                                     |
| 24.07 |                                                                     Tianjin University                                                                      |                             arxiv                              |                                     [DART: Deep Adversarial Automated Red Teaming for LLM Safety](https://arxiv.org/abs/2407.03876)                                     |                         **Automated Red Teaming**&**Adversarial Training**&**LLM Safety**                          |
| 24.07 |                                                                  Seoul National University                                                                  |                       ACL 2024 Workshop                        |                              [Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders](https://arxiv.org/abs/2407.06851)                               |                       **Sentence Encoders**&**Safety-Critical Knowledge**&**Unsafe Prompts**                       |
| 24.07 |                                                                       Duke University                                                                       |                             arxiv                              |                                     [Refusing Safe Prompts for Multi-modal Large Language Models](https://arxiv.org/abs/2407.09050)                                     |                        **Multimodal LLMs**&**Safe Prompt Refusal**&**Adversarial Attacks**                         |
| 24.07 |                                                             The Chinese University of Hong Kong                                                             |                             arxiv                              |                      [Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training](https://arxiv.org/abs/2407.09121)                       |                    **Safety Training**&**Refusal Position Bias**&**Decoupled Refusal Training**                    |
| 24.07 |                                                                      GovTech Singapore                                                                      |                             arxiv                              |                    [LionGuard: Building a Contextualized Moderation Classifier to Tackle Localized Unsafe Content](https://arxiv.org/abs/2407.10995)                    |                             **Moderation Classifier**&**Localized Content**&**Safety**                             |
| 24.07 |                                                               Georgia Institute of Technology                                                               |                             arxiv                              |                  [Targeted Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs](https://arxiv.org/abs/2407.15549)                   |                                   **Latent Adversarial Training**&**Robustness**                                   |
| 24.07 |                                                               Georgia Institute of Technology                                                               |                             arxiv                              |                 [A Voter-Based Stochastic Rejection-Method Framework for Asymptotically Safe Language Model Outputs](https://arxiv.org/abs/2407.16994)                  |                                     **Stochastic Rejection-Method**&**Safety**                                     |
| 24.07 |                                                                Shanghai Jiao Tong University                                                                |                             arxiv                              |                                              [SAFETY-J: Evaluating Safety with Critique](https://arxiv.org/abs/2407.17075)                                              |                     **Safety Evaluation**&**Critique-based Judgment**&**Bilingual Evaluator**                      |
| 24.07 |                                                                          Dynamo AI                                                                          |                       ICML 2024 Workshop                       |                                    [PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing](https://arxiv.org/abs/2407.16318)                                    |                              **Inference-Time Guardrails**&**Safety**&**Helpfulness**                              |
| 24.07 |                                                                   ShanghaiTech University                                                                   |                             arxiv                              |                             [Defending Jailbreak Attack in VLMs via Cross-modality Information Detector](https://arxiv.org/abs/2407.21659)                              |               **Jailbreak Attacks**&**Vision Language Models (VLMs)**&**Cross-modality Information**               |
| 24.08 |                                                           University of Illinois Urbana-Champaign                                                           |                             arxiv                              |                                          [Tamper-Resistant Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2408.00761)                                           |                             **Tamper Resistance**&**Security**&**Adversarial Attacks**                             |
| 24.08 |                                                     Beijing University of Posts and Telecommunications                                                      |                             arxiv                              |                            [SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models](https://arxiv.org/abs/2408.02632)                            |                                 **Self-Evolving Framework**&**Adversarial Safety**                                 |
| 24.08 |                                                             University of Texas at San Antonio                                                              |                       KDD 2024 AI4Cyber                        |                       [Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?](https://arxiv.org/abs/2408.02651)                        |                                       &**Adversarial Attacks**&**Alignment**                                       |
| 24.08 |                                                              University of Texas at Arlington                                                               |                             arxiv                              |                  [Utilizing Large Language Models to Optimize the Detection and Explainability of Phishing Websites](https://arxiv.org/abs/2408.05667)                  |                                     **Phishing Detection**&**Explainability**                                      |
| 24.08 |                                                                     New York University                                                                     |                             arxiv                              |                          [Towards Robust and Cost-Efficient Knowledge Unlearning for Large Language Models](https://arxiv.org/abs/2408.06621)                           |                                    **Knowledge Unlearning**&**Cost-Efficiency**                                    |
| 24.08 |                                                     Beijing University of Posts and Telecommunications                                                      |                             arxiv                              |                [Alignment-Enhanced Decoding: Defending via Token-Level Adaptive Refining of Probability Distributions](https://arxiv.org/abs/2408.07663)                |                               **Alignment-Enhanced Decoding**&**Jailbreak Defense**                                |
| 24.08 |                                                        University of Science and Technology of China                                                        |                             arxiv                              |                   [Prefix Guidance: A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks](https://arxiv.org/abs/2408.08924)                   |                                     **Jailbreak Defense**&**Prefix Guidance**                                      |
| 24.08 |                                                                   University of Liverpool                                                                   |                             arxiv                              |                      [Adaptive Guardrails for Large Language Models via Trust Modeling and In-Context Learning](https://arxiv.org/abs/2408.08959)                       |                         **Adaptive Guardrails**&**Trust Modeling**&**In-Context Learning**                         |
| 24.08 |                                                              National University of Singapore                                                               |                             arxiv                              | [BaThe: Defense against the Jailbreak Attack in Multimodal Large Language Models by Treating Harmful Instruction as Backdoor Trigger](https://arxiv.org/abs/2408.09093) |                  **Jailbreak Defense**&**Multimodal Large Language Models**&**Backdoor Trigger**                   |
| 24.08 |                                                               Georgia Institute of Technology                                                               |                             arxiv                              |                  [Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning](https://arxiv.org/abs/2408.09600)                  |                     **Safety Alignment**&**Harmful Fine-tuning**&**Post-fine-tuning Defense**                      |
| 24.08 |                                                                       LG Electronics                                                                        |                             arxiv                              |                                   [ATHENA: Safe Autonomous Agents with Verbal Contrastive Learning](https://arxiv.org/abs/2408.11021)                                   |                    **Autonomous Agents**&**Verbal Contrastive Learning**&**Safety Evaluation**                     |
| 24.08 |                                                                   Duke Kunshan University                                                                   |                             arxiv                              |                  [EEG-Defender: Defending against Jailbreak through Early Exit Generation of Large Language Models](https://arxiv.org/abs/2408.11308)                   |                                  **Jailbreak Defense**&**Early Exit Generation**                                   |
| 24.08 |                                                       Hong Kong University of Science and Technology                                                        |                             arxiv                              |                        [AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems](https://arxiv.org/abs/2408.14972)                        |                          **Multi-Agent Systems**&**Predictive Modeling**&**AgentMonitor**                          |
| 24.09 |                                                            University of Maryland, College Park                                                             |                             CoLM24                             |                  [Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models](https://arxiv.org/abs/2409.00598)                  |                         **False Refusals**&**Pseudo-Harmful Prompts**&**Safety Alignment**                         |
| 24.09 |                                                               Georgia Institute of Technology                                                               |                             arxiv                              |                [BOOSTER: Tackling Harmful Fine-Tuning for Large Language Models via Attenuating Harmful Perturbation](https://arxiv.org/abs/2409.01586)                 |                       **Harmful Fine-Tuning**&**LLM Alignment**&**Perturbation Attenuation**                       |
| 24.09 |                                                          University of Chinese Academy of Sciences                                                          |                             arxiv                              |                              [Recent Advances in Attack and Defense Approaches of Large Language Models](https://arxiv.org/abs/2409.03274)                              |                                    **Attack Approaches**&**Defense Mechanisms**                                    |
| 24.09 |                    Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, Institute of Artificial Intelligence                     |                             arxiv                              |                                [HSF: Defending against Jailbreak Attacks with Hidden State Filtering](https://arxiv.org/abs/2409.03788)                                 |                       **Hidden State Filtering**&**Jailbreak Attacks**&**Defense Mechanism**                       |
| 24.09 |                                                           Mahindra International School, SuperAGI                                                           |                             arxiv                              |                                [Safeguarding AI Agents: Developing and Analyzing Safety Architectures](https://arxiv.org/abs/2409.03793)                                |                             **AI Agents**&**Safety Architectures**&**Risk Mitigation**                             |
| 24.09 |                                                                Southern Illinois University                                                                 |                             arxiv                              |                   [Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks](https://arxiv.org/abs/2409.07353)                   |                    **Vision-Language Models**&**Jailbreak Attacks**&**Adversarial Fine-Tuning**                    |
| 24.09 |                                                                       Noah's Ark Lab                                                                        |                           COLM 2024                            |                [CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration](https://arxiv.org/abs/2409.11365)                 |          **Multimodal Large Language Models (MLLMs)**&**Safety Awareness**&**Constitutional Calibration**          |
| 24.09 |                                                                     Rutgers University                                                                      |                             arxiv                              |                                   [Data-centric NLP Backdoor Defense from the Lens of Memorization](https://arxiv.org/abs/2409.14200)                                   |                         **NLP Backdoor Defense**&**Memorization**&**Data-centric Defense**                         |
| 24.09 |                                                                   Northwestern University                                                                   |                             arxiv                              |                      [PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs](https://arxiv.org/abs/2409.14729)                       |                                 **Prompt Injection**&**Fuzzing**&**LLM Security**                                  |
| 24.09 |                                                                   University of Minnesota                                                                   |                             arxiv                              |           [On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains](https://arxiv.org/abs/2409.17275)            |           **Retrieval-Augmented Generation**&**Poisoning Attacks**&**Knowledge-Intensive Applications**            |
| 24.09 |                                                                     IBM Research Europe                                                                     |                             arxiv                              |                      [MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks](https://arxiv.org/abs/2409.17699)                      |                        **Jailbreak Attacks**&**Mixture of Experts**&**Tabular Classifiers**                        |
| 24.09 |                                              Institute of Information Engineering, Chinese Academy of Sciences                                              |                           EMNLP 2024                           |      [Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction](https://arxiv.org/abs/2409.16783)      |                        **Red Teaming**&**Multi-turn Interaction**&**Automated Red Teaming**                        |
| 24.09 |                                                                         Chegg Inc.                                                                          |                             arxiv                              |                                         [Overriding Safety Protections of Open-Source Models](https://arxiv.org/abs/2409.19476)                                         |                             **Harmfulness**&**Knowledge Drift**&**Model Uncertainty**                              |
| 24.09 |                                                                    University of Toronto                                                                    |                             arxiv                              |                                  [ROBUST LLM SAFEGUARDING VIA REFUSAL FEATURE ADVERSARIAL TRAINING](https://arxiv.org/abs/2409.20089)                                   |                         **Adversarial Training**&**LLM Safeguarding**&**Refusal Feature**                          |
| 24.10 |                                                                   University of A Coru√±a                                                                    |                             arxiv                              |                                 [Decoding Hate: Exploring Language Models' Reactions to Hate Speech](https://arxiv.org/abs/2410.00775)                                  |                                 **Hate Speech**&**LLMs**&**Mitigation Strategies**                                 |
| 24.10 |                                                                     Tel Aviv University                                                                     |                             arxiv                              |                                 [MITIGATING COPY BIAS IN IN-CONTEXT LEARNING THROUGH NEURON PRUNING](https://arxiv.org/abs/2410.01288)                                  |                              **Copy Bias**&**In-Context Learning**&**Neuron Pruning**                              |
| 24.10 |                                                                 Chinese Academy of Sciences                                                                 |                             arxiv                              |          [JAILBREAK ANTIDOTE: RUNTIME SAFETY-UTILITY BALANCE VIA SPARSE REPRESENTATION ADJUSTMENT IN LARGE LANGUAGE MODELS](https://arxiv.org/abs/2410.02298)           |                             **Jailbreaking**&**LLM Safety**&**Sparse Representation**                              |
| 24.10 |                                                               CAS Key Laboratory of AI Safety                                                               |                             arxiv                              |                          [HIDDENGUARD: FINE-GRAINED SAFE GENERATION WITH SPECIALIZED REPRESENTATION ROUTER](https://arxiv.org/abs/2410.02684)                           |                      **Safe Generation**&**Representation Router**&**Token-level Moderation**                      |
| 24.10 |                                                                          HydroX AI                                                                          |                             arxiv                              |                               [Precision Knowledge Editing: Enhancing Safety in Large Language Models](https://arxiv.org/abs/2410.03772)                                |                                          **Knowledge Editing**&**Safety**                                          |
| 24.10 |                                                                      IBM Research, MIT                                                                      |                             arxiv                              |                                        [Large Language Models Can Be Strong Self-Detoxifiers](https://arxiv.org/abs/2410.03818)                                         |                                   **Toxicity Reduction**&**Self-Detoxification**                                   |
| 24.10 |                                                                   UC Berkeley, Meta, FAIR                                                                   |                             arxiv                              |                                         [Aligning LLMs to Be Robust Against Prompt Injection](https://arxiv.org/abs/2410.05451)                                         |                                         **Prompt Injection**&**Security**                                          |
| 24.10 |                                                                      Purdue University                                                                      |                             arxiv                              |                          [ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time](https://arxiv.org/abs/2410.06625)                           |                         **Vision Language Models**&**Safety Alignment**&**Inference Time**                         |
| 24.10 |                                            Aerospace Information Research Institute, Chinese Academy of Sciences                                            |                             arxiv                              |                                [Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level](https://arxiv.org/abs/2410.06809)                                |                                **Decoding-Level Defense**&**Jailbreak Prevention**                                 |
| 24.10 |                                                       Rensselaer Polytechnic Institute, IBM Research                                                        |                             arxiv                              |                              [SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection](https://arxiv.org/abs/2410.07471)                               |                      **Safety-enhanced Aligned LLM**&**Fine-tuning**&**Bilevel Optimization**                      |
| 24.10 |                                                                  KAIST AI, Naver Cloud AI                                                                   |                             arxiv                              |                          [HOW DOES VISION-LANGUAGE ADAPTATION IMPACT THE SAFETY OF VISION LANGUAGE MODELS?](https://arxiv.org/abs/2410.07571)                           |                                **Vision-Language Adaptation**&**Safety**&**LVLMs**                                 |
| 24.10 |                                                 Johns Hopkins University, Microsoft Responsible AI Research                                                 |                             arxiv                              |                       [Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements](https://arxiv.org/abs/2410.08968)                       |                    **Safety alignment**&**Inference-time adaptation**&**LLMs controllability**                     |
| 24.10 |                                                       Princeton University, Zoom Video Communications                                                       |                             arxiv                              |                          [Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy](https://arxiv.org/abs/2410.09102)                           |                    **Instruction hierarchy**&**LLM safety**&**Instructional segment embedding**                    |
| 24.10 |                                                Zhejiang University, Westlake University, Beihang University                                                 |                             arxiv                              |                                               [LOCKING DOWN THE FINETUNED LLMS SAFETY](https://arxiv.org/abs/2410.10343)                                                |                            **Safety alignment**&**Fine-tuned LLMs**&**Meta-SafetyLock**                            |
| 24.10 |                                                                 Carnegie Mellon University                                                                  |                             arxiv                              |                                             [Data Defenses Against Large Language Models](https://arxiv.org/abs/2410.13138)                                             |                     **Data defenses**&**Adversarial prompt injections**&**Privacy protection**                     |
| 24.10 |                                                               University of Wisconsin-Madison                                                               |            NeurIPS 2024 Safe Generative AI Workshop            |                                          [Safety-Aware Fine-Tuning of Large Language Models](https://arxiv.org/abs/2410.10014)                                          |                   **Safety-aware fine-tuning**&**Harmful data detection**&**Data contamination**                   |
| 24.10 |                                                                     Zhejiang University                                                                     |                             arxiv                              |                 [CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment](https://arxiv.org/abs/2410.13903)                 |                                       **Model Stealing**&**Edge Deployment**                                       |
| 24.10 |                                                              Nanyang Technological University                                                               |                             arxiv                              |                           [Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation](https://arxiv.org/abs/2410.14425)                           |                **Backdoor Defense**&**Knowledge Distillation**&**Parameter-Efficient Fine-Tuning**                 |
| 24.10 |                                                                   Stony Brook University                                                                    |                             arxiv                              |                         [RobustKV: Defending Large Language Models Against Jailbreak Attacks via KV Eviction](https://arxiv.org/abs/2410.19937)                         |                          **Jailbreak Attacks**&**LLM Defense**&**KV Cache Optimization**                           |
| 24.10 |                                                                         UW-Madison                                                                          |                             arxiv                              |                       [FATH: Authentication-based Test-time Defense against Indirect Prompt Injection Attacks](https://arxiv.org/abs/2410.21492)                        |                      **Prompt Injection Defense**&**LLM Security**&**Authentication System**                       |
| 24.10 |                                                            Vanderbilt University Medical Center                                                             |                             arxiv                              |                                   [Embedding-based Classifiers Can Detect Prompt Injection Attacks](https://arxiv.org/abs/2410.22284)                                   |             **Prompt Injection Detection**&**Embedding-based Classification**&**Adversarial Attacks**              |
| 24.10 |                                                               University of Wisconsin-Madison                                                               |                             arxiv                              |                      [InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models](https://arxiv.org/abs/2410.22770)                      |                    **Prompt Injection Defense**&**Over-defense Detection**&**Guardrail Models**                    |
| 24.11 |                                                                 National Taiwan University                                                                  |                             arxiv                              |                                    [Attention Tracker: Detecting Prompt Injection Attacks in LLMs](https://arxiv.org/abs/2411.00348)                                    |                                    **Prompt Injection**&**Attention Mechanism**                                    |
| 24.11 |                                                              National University of Singapore                                                               |                             arxiv                              |                               [Defense Against Prompt Injection Attack by Leveraging Attack Techniques](https://arxiv.org/abs/2411.00459)                               |                                 **Prompt Injection Defense**&**Attack Techniques**                                 |
| 24.11 |                                                               Georgia Institute of Technology                                                               |                             arxiv                              |               [UNIGUARD: Towards Universal Safety Guardrails for Jailbreak Attacks on Multimodal Large Language Models](https://arxiv.org/abs/2411.01703)               |                 **Jailbreak Attacks**&**Multimodal Safety Guardrails**&**Adversarial Robustness**                  |
| 24.11 |                                                                 Mila ‚Äì Quebec AI Institute                                                                  |                             arXiv                              |                    [Combining Domain and Alignment Vectors to Achieve Better Knowledge-Safety Trade-offs in LLMs](https://arxiv.org/abs/2411.06824)                     |                            **Domain Expertise**&**Safety Alignment**&**Model Merging**                             |
| 24.11 |                                                                          Anthropic                                                                          |                             arXiv                              |                                    [Rapid Response: Mitigating LLM Jailbreaks with a Few Examples](https://arxiv.org/abs/2411.07494)                                    |                                      **Jailbreak Defense**&**Rapid Response**                                      |
| 24.11 |                                                              National University of Singapore                                                               |                             arXiv                              |                                 [The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense](https://arxiv.org/abs/2411.08410)                                  |                        **Vision-Language Models**&**Jailbreak Defense**&**Model Alignment**                        |
| 24.11 |                                                     Beijing University of Posts and Telecommunications                                                      |                             arXiv                              |                            [Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey](https://arxiv.org/abs/2411.09259)                            |                   **Jailbreak Attacks**&**Multimodal Generative Models**&**Security Challenges**                   |
| 24.11 |                                                               Singapore Management University                                                               |                             arxiv                              |                   [CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization](https://arxiv.org/abs/2411.12768)                    |                                **Backdoor Defense**&**Consistency Regularization**                                 |
| 24.11 |                                                                         UC Berkeley                                                                         |                          COLING 2025                           |                                   [Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings](https://arxiv.org/abs/2411.14398)                                    |                           **Safety Guardrails**&**Fine-tuned BERT**&**Prompt Filtering**                           |
| 24.11 |                                     The Hong Kong University of Science and Technology (Guangzhou), Tsinghua University                                     |                             arxiv                              |                            [PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2411.17453)                            |                                      **Backdoor Detection**&**PEFT**&**LoRA**                                      |
| 24.12 |                                                                    MIT,Speechmatics,MATS                                                                    |                  NeurIPS 2024 SoLaR workshops                  |           [Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods and a New Transcript-Classifier Approach](https://arxiv.org/abs/2412.02159)            |                          **Jailbreak Defense**&**LLM Security**&**Transcript Classifier**                          |
| 24.12 |                                                                Shanghai Jiao Tong University                                                                |                          COLING 2025                           |                    [Gracefully Filtering Backdoor Samples for Generative Large Language Models without Retraining](https://arxiv.org/abs/2412.02454)                    |                            **Backdoor Attacks**&**Generative LLMs**&**Frequency Space**                            |
| 24.12 |                                                                     NVIDIA Corporation                                                                      |                             arxiv                              |                             [Improved Large Language Model Jailbreak Detection via Pretrained Embeddings](https://arxiv.org/abs/2412.01547)                             |                         **Jailbreak Detection**&**Pretrained Embeddings**&**LLM Security**                         |
| 24.11 |                                                      Fudan University, Worcester Polytechnic Institute                                                      |                             arxiv                              |                      [Knowledge Database or Poison Base? Detecting RAG Poisoning Attack through LLM Activations](https://arxiv.org/abs/2411.18948)                      |                             **RAG Poisoning Attack**&**LLM Activations**&**Security**                              |
| 24.11 |                              University of Maryland-College Park, Indian Institute of Technology Bombay, Princeton University                               |                             arxiv                              |                    [Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment](https://arxiv.org/abs/2411.18688)                     |                       **Safety Alignment**&**Jailbreak Attack**&**Inference-Time Alignment**                       |
| 24.12 |                                                                        Not specified                                                                        |                             arxiv                              |                                       [Enhancing Adversarial Resistance in LLMs with Recursion](https://arxiv.org/abs/2412.06181)                                       |                       **Adversarial Resistance**&**LLM Security**&**Prompt Simplification**                        |
| 24.12 |                                                  University of Maryland, Capital One, New York University                                                   |                             arxiv                              |                             [Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models](https://arxiv.org/abs/2412.06748)                             |                                      **Refusal Tokens**&**Model Calibration**                                      |
| 24.12 |                                                                    Dalhousie University                                                                     |                             arxiv                              |                                               [Classifier-free Guidance in LLMs Safety](https://arxiv.org/abs/2412.06846)                                               |                       **Unlearning**&**Reinforcement Learning**&**Classifier-free Guidance**                       |
| 24.12 |                                                                Princeton University, Google                                                                 |                             arxiv                              |                                    [Evaluating the Durability of Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2412.07097)                                     |                         **LLM Safeguards**&**Open-Weight Models**&**Adversarial Defense**                          |
| 24.12 |                                                  Michigan State University, University of Hawaii at Manoa                                                   |                             arxiv                              |             [FlexLLM: Exploring LLM Customization for Moving Target Defense on Black-Box LLMs Against Jailbreak Attacks](https://arxiv.org/abs/2412.07672)              |                       **LLM Customization**&**Moving Target Defense**&**Jailbreak Attacks**                        |
| 24.12 |                                                                          Neudesic                                                                           |                             arxiv                              |                                   [Lightweight Safety Classification Using Pruned Language Models](https://arxiv.org/abs/2412.13435)                                    |                        **Model Pruning**&**Content Safety**&**Prompt Injection Detection**                         |
| 24.12 |                                                                     Asan Medical Center                                                                     |                             arxiv                              |                             [Mitigating Adversarial Attacks in LLMs through Defensive Suffix Generation](https://arxiv.org/abs/2412.13705)                              |                    **Adversarial Defense**&**Gradient-Based Optimization**&**Defensive Suffix**                    |
| 24.12 |                                                      The Chinese University of Hong Kong, IBM Research                                                      |                           AAAI 2025                            |                      [Token Highlighter: Inspecting and Mitigating Jailbreak Prompts for Large Language Models](https://arxiv.org/abs/2412.18171)                       |                         **Jailbreak Attacks**&**Affirmation Loss**&**Token-Level Defense**                         |
| 24.12 |                                       MBZUAI, Huazhong University of Science and Technology, University of Notre Dame                                       |                             arxiv                              |               [Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models](https://arxiv.org/abs/2412.17034)                |                    **Jailbreak Attacks**&**Activation Boundary Defense (ABD)**&**LLM Security**                    |
| 24.12 |                                                              The Pennsylvania State University                                                              |                             arxiv                              |                 [The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents](https://arxiv.org/abs/2412.16682)                 |                         **Indirect Prompt Injection**&**Task Alignment**&**LLM Security**                          |
| 24.12 |                                                                           OpenAI                                                                            |                             arxiv                              |                                   [Deliberative Alignment: Reasoning Enables Safer Language Models](https://arxiv.org/abs/2412.16339)                                   |                       **Safety Alignment**&**Reasoning-Based Training**&**Chain-of-Thought**                       |
| 24.12 |                                                         MMLab, The Chinese University of Hong Kong                                                          |                             arxiv                              |                   [RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting](https://arxiv.org/abs/2412.18826)                   |                 **Multimodal Large Language Models**&**Defensive Prompting**&**Safety Mechanisms**                 |
| 24.12 |                                                                 National Taiwan University                                                                  |                             arxiv                              |                                [Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging](https://arxiv.org/abs/2412.19512)                                 |                                       **Safety Alignment**&**Model Merging**                                       |


## üíªPresentations & Talk


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars
