# Truthfulness&Misinformation

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                                                                      Institute                                                                       |     Publication     |                                                                                            Paper                                                                                            |                                          Keywords                                           |
|:-----:|:----------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------:|
| 21.09 |                                                                 University of Oxford                                                                 |       ACL2022       |                                                 [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                                                 |                               **Benchmark**&**Truthfulness**                                |
| 23.07 | Microsoft Research Asia, Hong Kong University of Science and Technology, University of Science and Technology of China, Tsinghua University, Sony AI |   ResearchSquare    |                                    [Defending ChatGPT against Jailbreak Attack via Self-Reminder](https://www.researchsquare.com/article/rs-2873090/v1)                                     |                   **Jailbreak Attack**&**Self-Reminder**&**AI Security**                    |
| 23.10 |                                                                 University of Zurich                                                                 |        arxiv        |                                          [Lost in Translation -- Multilingual Misinformation and its Evolution](https://arxiv.org/abs/2310.18089)                                           |                             **Misinformation**&**Multilingual**                             |
| 23.10 |                                                           New York University&Javier Rando                                                           |        arxiv        |                                               [Personas as a Way to Model Truthfulness in Language Models](https://arxiv.org/abs/2310.18168)                                                |                            **Truthfulness**&**Truthful Persona**                            |
| 23.11 |                                                                  Dialpad Canada Inc                                                                  |        arxiv        |                          [Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs](https://arxiv.org/abs/2311.00681)                           |                                  **Factuality Assessment**                                  |
| 23.11 |                                                             The University of Manchester                                                             |        arxiv        |                                                     [Emotion Detection for Misinformation: A Review](https://arxiv.org/abs/2311.00671)                                                      |                         **Survey**&**Misinformation**&**Emotions**                          |
| 23.11 |                                                                University of Virginia                                                                |        arxiv        |                             [Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics](https://arxiv.org/abs/2311.01386)                             |                                   **Language Illusions**                                    |
| 23.11 |                                                       University of Illinois Urbana-Champaign                                                        |        arxiv        |          [Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism](https://arxiv.org/abs/2311.01041)          |                          **Hallucinations**&**Refusal Mechanism**                           |
| 23.11 |                                                           University of Washington Bothell                                                           |        arxiv        |                                         [Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI](https://arxiv.org/abs/2311.01463)                                         |                    **Healthcare**&**Trustworthiness**&**Hallucinations**                    |
| 23.11 |                                                                  Intuit AI Research                                                                  |      EMNLP2023      |                     [SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency](https://arxiv.org/abs/2311.01740)                      |                       **Hallucination Detection**&**Trustworthiness**                       |
| 23.11 |                                                            Shanghai Jiao Tong University                                                             |        arxiv        |                          [Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation](https://arxiv.org/abs/2311.01766)                           |                  **Misinformation**&**Disinformation**&**Out-of-Context**                   |
| 23.11 |                                                             Hamad Bin Khalifa University                                                             |        arxiv        |                                 [ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text](https://arxiv.org/abs/2311.03179)                                 |                             **Disinformation**&**Arabic Text**                              |
| 23.11 |                                                                   UNC-Chapel Hill                                                                    |        arxiv        |                                  [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)                                  |                       **Hallucination**&**Benchmark**&**Multimodal**                        |
| 23.11 |                                                                  Cornell University                                                                  |        arxiv        |                                            [Adapting Fake News Detection to the Era of Large Language Models](https://arxiv.org/abs/2311.04917)                                             |                **Fake news detection**&**Generated News**&**Misinformation**                |
| 23.11 |                                                            Harbin Institute of Technology                                                            |        arxiv        |                        [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232)                         |                **Hallucination**&**Factual Consistency**&**Trustworthiness**                |
| 23.11 |                                                      Korea University, KAIST AI,LG AI Research                                                       |        arXiv        |                                   [VOLCANO: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision](https://arxiv.org/abs/2311.07362)                                    |                  **Multimodal Models**&**Hallucination**&**Self-Feedback**                  |
| 23.11 |                                              Beijing Jiaotong University, Alibaba Group, Peng Cheng Lab                                              |        arXiv        |                                    [AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation](https://arxiv.org/abs/2311.07397)                                    |                  Multi-modal Large Language Models&Hallucination&Benchmark                  |
| 23.11 |                                            LMU Munich; Munich Center of Machine Learning; Google Research                                            |        arXiv        |                                                 [Hallucination Augmented Recitations for Language Models](https://arxiv.org/abs/2311.07424)                                                 |                        **Hallucination**&**Counterfactual Datasets**                        |
| 23.11 |                                                         Stanford University, UNC Chapel Hill                                                         |        arxiv        |                                                       [Fine-tuning Language Models for Factuality](https://arxiv.org/abs/2311.08401)                                                        |      **Factuality**&**Reference-Free Truthfulness**&**Direct Preference Optimization**      |
| 23.11 |                                                      Corporate Data and Analytics Office (CDAO)                                                      |        arxiv        |                                     [Hallucination-minimized Data-to-answer Framework for Financial Decision-makers](https://arxiv.org/abs/2311.07592)                                      |                **Financial Decision Making**&**Hallucination Minimization**                 |
| 23.11 |                                                               Arizona State University                                                               |        arxiv        |                                             [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914)                                              |                     **Knowledge Graphs**&**Hallucinations**&**Survey**                      |
| 23.11 |                                    Kempelen Institute of Intelligent Technologies; Brno University of Technology                                     |        arxiv        |                                                  [Disinformation Capabilities of Large Language Models](https://arxiv.org/abs/2311.08838)                                                   |          **Disinformation Generation**&**Safety Filters**&**Automated Evaluation**          |
| 23.11 |                                                      UNC-Chapel Hill, University of Washington                                                       |        arxiv        |                        [EVER: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification](https://arxiv.org/abs/2311.09114)                         |               **Hallucination**&**Real-Time Verification**&**Rectification**                |
| 23.11 |                                                      Peking University, WeChat AI, Tencent Inc.                                                      |        arXiv        |                                    [RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge](https://arxiv.org/abs/2311.08147)                                    |            **External Counterfactual Knowledge**&**Benchmarking**&**Robustness**            |
| 23.11 |                                                                    PolyAI Limited                                                                    |        arXiv        |                     [Dial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning](https://arxiv.org/abs/2311.09800)                      |                **Factuality**&**Behavioural Fine-Tuning**&**Hallucination**                 |
| 23.11 |                             The Hong Kong University of Science and Technology, University of Illinois Urbana-Champaign                              |        arxiv        |                                          [R-Tuning: Teaching Large Language Models to Refuse Unknown Questions](https://arxiv.org/abs/2311.09677)                                           |          **Hallucination**&**Refusal-Aware Instruction Tuning**&**Knowledge Gap**           |
| 23.11 |                            University of Southern California, University of Pennsylvania, University of California Davis                             |        arxiv        |                             [Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702)                              |                 **Hallucinations**&**Semantic Associations**&**Benchmark**                  |
| 23.11 |                                              The Ohio State University, University of California Davis                                               |        arxiv        |                     [How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities](https://arxiv.org/abs/2311.09447)                      |          **Trustworthiness**&**Malicious Demonstrations**&**Adversarial Attacks**           |
| 23.11 |                                                               University of Sheffield                                                                |        arXiv        |                  [Lighter yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization](https://arxiv.org/abs/2311.09335)                  |                     **Hallucinations**&&**Language Model Reliability**                      |
| 23.11 |                     Institute of Information Engineering Chinese Academy of Sciences, University of Chinese Academy of Sciences                      |        arxiv        |                      [Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study](https://arxiv.org/abs/2311.12699)                      |                                **Misinformation Detection**                                 |
| 23.11 |                     Shanghai Jiaotong University, Amazon AWS AI, Westlake University, IGSNRR Chinese Academy of Sciences, China                      |        arXiv        |                                         [Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus](https://arxiv.org/abs/2311.13230)                                         |      **Hallucination Detection**&**Uncertainty-Based Methods**&**Factuality Checking**      |
| 23.11 |                             Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences                             |        arXiv        |                            [Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting](https://arxiv.org/abs/2311.13314)                             |                  **Hallucinations**&**Knowledge Graphs**&**Retrofitting**                   |
| 23.11 |                                                              Applied Research Quantiphi                                                              |        arxiv        |                                       [Minimizing Factual Inconsistency and Hallucination in Large Language Models](https://arxiv.org/abs/2311.13878)                                       |                         **Factual Inconsistency**&**Hallucination**                         |
| 23.11 |                                                           Microsoft Research, Georgia Tech                                                           |        arxiv        |                                                       [Calibrated Language Models Must Hallucinate](https://arxiv.org/abs/2311.14648)                                                       |                   **Hallucination&Calibration**&**Statistical Analysis**                    |
| 23.11 |                                                   School of Information Renmin University of China                                                   |        arxiv        |                          [UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://arxiv.org/abs/2311.15296)                          |                         **Hallucination**&**Evaluation Benchmark**                          |
| 23.11 |                                       DAMO Academy Alibaba Group, Nanyang Technological University, Hupan Lab                                        |        arxiv        |                          [Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding](https://arxiv.org/abs/2311.16922)                           |                    **Vision-Language Models**&**Object Hallucinations**                     |
| 23.11 |                                                                Shanghai AI Laboratory                                                                |        arxiv        |                            [Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization](https://arxiv.org/abs/2311.16839)                            | **Multimodal Language Models**&**Hallucination Problem**&**Direct Preference Optimization** |
| 23.12 |                Singapore Management University, Beijing Forestry University, University of Electronic Science and Technology of China                |      MMM 2024       |                         [Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites](https://arxiv.org/abs/2312.01701)                         |          **Vision-language Models**&**Hallucination**&**Fine-grained Evaluation**           |
| 23.12 |                                                               Mila, McGill University                                                                | EMNLP2023(findings) |                           [Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness](https://arxiv.org/abs/2312.01858)                            |                   **Knowledge Bases**&**Dataset**&**Evaluation Protocol**                   |
| 23.12 |                                                                      MIT CSAIL                                                                       |        arxiv        |                       [Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?](https://arxiv.org/abs/2312.03729)                       |                        **Truthfulness**&**Internal Representations**                        |
| 23.12 |           University of Illinois Chicago, Bosch Research North America & Bosch Center for Artificial Intelligence (BCAI), UNC Chapel-Hill            |        arxiv        |                                       [DELUCIONQA: Detecting Hallucinations in Domain-specific Question Answering](https://arxiv.org/abs/2312.05200)                                        |       **Hallucination Detection**&**Domain-specific QA**&**Retrieval-augmented LLMs**       |
| 23.12 |                                                   The University of Hong Kong, Beihang University                                                    |      AAAI2024       |                                         [Improving Factual Error Correction by Learning to Inject Factual Errors](https://arxiv.org/abs/2312.07049)                                         |                                **Factual Error Correction**                                 |
| 23.12 |                                                                Allen Institute for AI                                                                |        arxiv        |                               [BARDA: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability](https://arxiv.org/abs/2312.07527)                               |                   **Dataset**&**Factual Accuracy**&**Reasoning Ability**                    |
| 23.12 |                      Tsinghua University, Shanghai Jiao Tong University, Stanford University, Nanyang Technological University                       |        arxiv        |                       [The Earth is Flat because...: Investigating LLMs‚Äô Belief towards Misinformation via Persuasive Conversation](https://arxiv.org/abs/2312.09085)                       |            **Misinformation**&**Persuasive Conversation**&**Factual Questions**             |
| 23.12 |                                                            University of California Davis                                                            |        arXiv        |                                         [A Revisit of Fake News Dataset with Augmented Fact-checking by ChatGPT](https://arxiv.org/abs/2312.11870)                                          |                               **Fake News**&**Fact-checking**                               |
| 23.12 |                                                                 Amazon Web Services                                                                  |        arxiv        |                                           [On Early Detection of Hallucinations in Factual Question Answering](https://arxiv.org/abs/2312.14183)                                            |                      **Hallucinations**&**Factual Question Answering**                      |
| 23.12 |                                                         University of California Santa Cruz                                                          |        arxiv        | [Don‚Äôt Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models](https://arxiv.org/abs/2312.14346) |                     **Hallucinations**&**Faithfulness**&**Token-level**                     |
| 23.12 |                                              Department of Radiology, The University of Tokyo Hospital                                               |        arxiv        |                                                     [Theory of Hallucinations based on Equivariance](https://arxiv.org/abs/2312.14504)                                                      |                             **Hallucinations**&**Equivariance**                             |
| 23.12 |                                                           Georgia Institute of Technology                                                            |        arXiv        |                                               [REDUCING LLM HALLUCINATIONS USING EPISTEMIC NEURAL NETWORKS](https://arxiv.org/abs/2312.15576)                                               |                **Hallucinations**&**Uncertainty Estimation**&**TruthfulQA**                 |
| 23.12 |                 Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Tencent AI Lab                  |        arXiv        |                                   [Alleviating Hallucinations of Large Language Models through Induced Hallucinations](https://arxiv.org/abs/2312.15710)                                    |             **Hallucinations**&**Induce-then-Contrast Decoding**&**Factuality**             |
| 23.12 |     SKLOIS Institute of Information Engineering Chinese Academy of Sciences, School of Cyber Security University of Chinese Academy of Sciences      |        arXiv        |                                   [LLM Factoscope: Uncovering LLMs‚Äô Factual Discernment through Inner States Analysis](https://arxiv.org/abs/2312.16374)                                    |                           **Factual Detection**&**Inner States**                            |
| 24.01 |                                                 The Chinese University of Hong Kong, Tencent AI Lab                                                  |        arxiv        |                                          [The Earth is Flat? Unveiling Factual Errors in Large Language Models](https://arxiv.org/abs/2401.00761)                                           |                **Factual Errors**&**Knowledge Graph**&**Answer Assessment**                 |
| 24.01 |                                                  NewsBreak, University of Illinois Urbana-Champaign                                                  |        arxiv        |                             [RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models](https://arxiv.org/abs/2401.00396)                             |         **Retrieval-Augmented Generation**&**Hallucination Detection**&**Dataset**          |
| 24.01 |                                  University of California Berkeley, Universit√© de Montr√©al, McGill UniversityÔºå Mila                                  |        arxiv        |                                                   [Uncertainty Resolution in Misinformation Detection](https://arxiv.org/abs/2401.01197)                                                    |                        **Misinformation**&**Uncertainty Resolution**                        |
| 24.01 |                                                         Yale University, Stanford University                                                         |        arxiv        |                                      [Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models](https://arxiv.org/abs/2401.01301)                                      |                                  **Legal Hallucinations**                                   |
| 24.01 |                     Islamic University of Technology, AI Institute University of South Carolina, Stanford University, Amazon AI                      |        arxiv        |                                 [A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models](https://arxiv.org/abs/2401.01313)                                  |                √ü                               **Hallucination Mitigation**                 |
| 24.01 |                                 Renmin University of China, Renmin University of China, DIRO, Universit√© de Montr√©al                                 |        arxiv        |                            [The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models](https://arxiv.org/abs/2401.03205)                             |             **Hallucination**&**Detection and Mitigation**&**Empirical Study**              |
| 24.01 |                     IIT Hyderabad India, Parmonic USA, University of Glasgow UK, LDRP Institute of Technology and Research India                     |        arxiv        |                              [Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset](https://arxiv.org/abs/2401.04481)                              |                **Misinformation Detection**&**LLM-generated Synthetic Data**                |
| 24.01 |                                                              University College London                                                               |        arxiv        |                                              [Hallucination Benchmark in Medical Visual Question Answering](https://arxiv.org/abs/2401.05827)                                               |              **Medical Visual Question Answering**&**Hallucination Benchmark**              |
| 24.01 |                                                                  Soochow University                                                                  |        arxiv        |                                                        [LightHouse: A Survey of AGI Hallucination](https://arxiv.org/abs/2401.06792)                                                        |                                    **AGI Hallucination**                                    |
| 24.01 |                                     University of Washington, Carnegie Mellon University, Allen Institute for AI                                     |        arxiv        |                                          [Fine-grained Hallucination Detection and Editing for Language Models](https://arxiv.org/abs/2401.06855)                                           |                            **Hallucination Detection**&**FAVA**                             |
| 24.01 |                                          Dartmouth College, Universit√© de Montr√©al, McGill UniversityÔºåMila                                           |        arxiv        |                                      [Comparing GPT-4 and Open-Source Language Models in Misinformation Mitigation](https://arxiv.org/abs/2401.06920)                                       |                           **GPT-4**&**Misinformation Detection**                            |
| 24.01 |                                                                  Utrecht University                                                                  |        arxiv        |                                                         [The Pitfalls of Defining Hallucination](https://arxiv.org/abs/2401.07897)                                                          |                                      **Hallucination**                                      |
| 24.01 |                                                                  Samsung AI Center                                                                   |        arxiv        |                                         [Hallucination Detection and Hallucination Mitigation: An Investigation](https://arxiv.org/abs/2401.08358)                                          |                  **Hallucination Detection**&**Hallucination Mitigation**                   |


## üíªPresentations & Talks



## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars