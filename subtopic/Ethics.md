# Ethics

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating witha the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                                                      Institute                                                                                       |     Publication     |                                                                                    Paper                                                                                    |                                           Keywords                                            |
|:-----:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------:|
| 23.06 |                                                                      University of Illinois at Urbana-Champaign                                                                      |        arxiv        |                               [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)                                |                      **Robustness**&**Ethics**&**Privacy**&**Toxicity**                       |
| 23.11 |                                                                                Allen Institute for AI                                                                                |        arxiv        |                                   [Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs](https://arxiv.org/abs/2311.04892)                                    |                                   **Bias**&**Stereotypes**                                    |
| 23.11 |                                                                                   Adobe Inc. India                                                                                   |        arxiv        |                     [All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation](https://arxiv.org/abs/2311.05451)                     |                                    **Fairness**&**Biases**                                    |
| 23.11 |                                                                         National Taiwan University, Meta AI                                                                          |        arXiv        |                          [Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems](https://arxiv.org/abs/2311.06513)                          |               **Task-oriented Dialogue Systems**&**Societal Bias**&**Fairness**               |
| 23.11 |                                                                          UNC Chapel Hill, IBM Research MIT                                                                           |        arxiv        |                             [Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion](https://arxiv.org/abs/2311.07682)                              |                **Model Fusion**&**Bias Reduction**&**Selective Memorization**                 |
| 23.11 |                                                                                      ETH Z√ºrich                                                                                      |        arxiv        |                   [Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures](https://arxiv.org/abs/2311.08605)                   |                          **Political Debates**&**Bias Attribution**                           |
| 23.11 |                                                                     University of Pisa, University of Copenhagen                                                                     |        arxiv        |                                     [Social Bias Probing: Fairness Benchmarking for Language Models](https://arxiv.org/abs/2311.09090)                                      |             **Social Biases**&**Fairness Benchmarking**&**Identity Stereotypes**              |
| 23.11 |                                                                                 Tsinghua University                                                                                  |        arxiv        |     [ALIGNMENT IS NOT SUFFICIENT TO PREVENT LARGE LANGUAGE MODELS FROM GENERATING HARMFUL INFORMATION: A PSYCHOANALYTIC PERSPECTIVE](https://arxiv.org/abs/2311.08487)      |                      **Alignment**&**Psychoanalysis Theory**&**Ethics**                       |
| 23.11 |                                                                    University of Toronto, University of Michigan                                                                     |        arxiv        |                     [Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks](https://arxiv.org/abs/2311.09730)                     |                                **Gender Bias**&**Racial Bias**                                |
| 23.11 |                                                    University of Michigan, University of Hawaii at Hilo, Northeastern University                                                     |        arxiv        |                                      [MOKA: Moral Knowledge Augmentation for Moral Event Extraction](https://arxiv.org/abs/2311.09733)                                      |          **Moral Event Extraction**&**MOKA Framewor**k&**External Moral Knowledge**           |
| 23.11 |              Mila - Quebec AI Institute, Universit√© du Qu√©bec √† Montr√©al, Data & Society Research Institute, Mantium, IBM Research, University of California Riverside               |        arxiv        |                                  [Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset](https://arxiv.org/abs/2311.09443)                                  |                **Subtle Misogyny**&B**iasly Dataset**&**Dataset Development**                 |
| 23.11 |                                                   Illinois Institute of Technology, University of Illinois Chicago, Cisco Research                                                   |        arxiv        |                             [Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models](https://arxiv.org/abs/2311.09428)                             |              **Fairness**&**Abusive Language Detection**&**Adversarial Attacks**              |
| 23.11 |                                                  Cornell University, KTH Royal Institute of Technology, University of Pennsylvania                                                   |        arxiv        |                                              [Auditing and Mitigating Cultural Bias in LLMs](https://arxiv.org/abs/2311.14096)                                              |                          **Cultural Bias**&**Mitigation Strategies**                          |
| 23.11 |                                                                        University College London, Holistic AI                                                                        |        arxiv        |                            [Towards Auditing Large Language Models: Improving Text-based Stereotype Detection](https://arxiv.org/abs/2311.14126)                            |                                   **Stereotype Detection**                                    |
| 23.11 |                                                                           Georgia Institute of Technology                                                                            |        arxiv        |                                 [Evaluating Large Language Models through Gender and Racial Stereotypes](https://arxiv.org/abs/2311.14788)                                  |                **Gender Bias**&**Racial Stereotypes**&**Evaluation Framework**                |
| 23.11 |                                             Polytechnique Montr√©al, √âTS Montr√©al, CISPA-Helmholtz Center for Information Security, Mila                                              |        arxiv        |                                           [Survey on AI Ethics: A Socio-technical Perspective](https://arxiv.org/abs/2311.17228)                                            |                     **AI Ethics&****Trustworthiness**&**Responsibility**                      |
| 23.11 |                                                                                         Meta                                                                                         |      EMNLP2023      |                                   [ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://arxiv.org/abs/2311.18140)                                    |                         **Bias Evaluation**&**Fairness**&**Toxicity**                         |
| 23.11 |                                                                      Comcast Applied AI, University of Waterloo                                                                      |        arxiv        |                       [What Do Llamas Really Think? Revealing Preference Biases in Language Model Representations](https://arxiv.org/abs/2311.18812)                        |                **Implicit Bias**&**Sociodemographic Bias**&B**ias Mitigation**                |
| 23.12 |                                                          University of Auckland, University of Waikato, University of Macau                                                          |        arXiv        |                      [Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies](https://arxiv.org/abs/2312.01509)                       |                             **Governmental Regulations**&**Bias**                             |
| 23.12 |                            Yildiz Technical University, Istanbul Technical University, Maersk Mc-kinney Moeller Institute University of Southern Denmark                             |        arXiv        |                     [Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection](https://arxiv.org/abs/2312.01787)                      |                         **Offensive Language**&**Data-augmentation**                          |
| 23.12 |                                                                                  TCS Research India                                                                                  | EMNLP2023(Workshop) |                     [Towards Mitigating Perceived Unfairness in Contracts from a Non-Legal Stakeholder‚Äôs Perspective](https://arxiv.org/abs/2312.01398)                     |                 **Fairness**&**Non-legal Stakeholders**&**Data Augmentation**                 |
| 23.12 |                                                                                   Korea University                                                                                   |      EMNLP2023      |                            [Improving Bias Mitigation through Bias Experts in Natural Language Understanding](https://arxiv.org/abs/2312.03577)                             |                      **Bias Mitigation**&**Multi-Class Classification**                       |
| 23.12 |                                                          Queensland University of Technology, University of New South Wales                                                          | EMNLP2023(Workshop) |           [Measuring Misogyny in Natural Language Generation: Preliminary Results from a Case Study on two Reddit Communities](https://arxiv.org/abs/2312.03330)            |                      **Misogyny**&**Toxicity Classifier**s&**Benchmark**                      |
| 23.12 |                                   Carnegie Mellon University, Pittsburgh, Universidade NOVA de Lisboa, Allen Institute for Artificial Intelligence                                   |      EMNLP2023      |                          [Understanding the Effect of Model Compression on Social Bias in Large Language Models](https://arxiv.org/abs/2312.05662)                          |               **Model Compression**&**Social Bias**&&**Knowledge Distillation**               |
| 23.12 |                                 Eindhoven University of Technology, University of Liverpool, Griffith University, The Pennsylvania State University                                  |        arXiv        |                             [GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models](https://arxiv.org/abs/2312.06315)                             |           **Bias Evaluation**&**Bias Attack Instructions**&**Intersectional Bias**            |
| 23.12 |                                                                                   IBM Research AI                                                                                    |      AAAI2024       |                        [SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models](https://arxiv.org/abs/2312.07492)                        |                        **Social Bias**&**Question-answering Dataset**                         |
| 23.12 |                                                                       New York University, CUNY Queens College                                                                       |        arxiv        |                         [Red AI? Inconsistent Responses from GPT Models on Political Issues in the US and China](https://arxiv.org/abs/2312.09917)                          |                                     **Bias**&**Politics**                                     |
| 23.12 |                                                                  University of California Los Angeles, Amazon Alexa                                                                  |        arXiv        |     [Are you talking to [‚Äòxem‚Äô] or [‚Äòx‚Äô ‚Äòem‚Äô]? On Tokenization and Addressing Misgendering in LLMs with Pronoun Tokenization Parity](https://arxiv.org/abs/2312.11779)      |                                        **Gender Bias**                                        |
| 23.12 |                                                       University of Oxford, University Canada West, Amazon Web Services (AWS)                                                        |        arxiv        |                                               [Large Language Model (LLM) Bias Index‚ÄîLLMBI](https://arxiv.org/abs/2312.14769)                                               |                          **Bias Quantification**&**Bias Mitigation**                          |
| 23.12 | Institute of Information Engineering Chinese Academy of Sciences, School of Cyber Security University of Chinese Academy of Sciences, JD AI Research Beijing, Independent Researcher |        arXiv        |                                             [A Group Fairness Lens for Large Language Models](https://arxiv.org/abs/2312.15478)                                             |                              **Group Fairness**&**Social Bias**                               |
| 24.01 |                                                                             Hong Kong Baptist University                                                                             |        arxiv        |                         [GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse](https://arxiv.org/abs/2401.01523)                          |      **Large Multimodal Models (LMMs)**&**Meme-Based Social Abuse**&**Safety Insights**       |
| 24.01 |                                                                       Beihang University, Westcliff University                                                                       |        arxiv        | [Unveiling Bias in Fairness Evaluations of Large Language Models: A Critical Literature Review of Music and Movie Recommendation Systems](https://arxiv.org/abs/2401.04057) | **Music and Movie Recommendations**&**Recommender Systems**&**Fairness Evaluation Framework** |
| 24.01 |                                                            Pacific Northwest National Laboratory, University of Michigan                                                             |        arxiv        |                    [Whose wife is it anyway? Assessing bias against same-gender relationships in machine translation](https://arxiv.org/abs/2401.04972)                     |                         **Gender Bias**&**Same-Gender Relationships**                         |
| 24.01 |                             Universit√© Jean Monnet Saint-Etienne, CNRS Institut d'Optique Graduate School, T√©l√©com Paris Institut Polytechnique de Paris                             |        arxiv        |                            [An investigation ofstructures responsible for gender bias in BERT and DistilBERT](https://arxiv.org/abs/2401.06495)                             |                                  **Fairness**&**Imbalance**                                   |
| 24.01 |                                                                                University of Toronto                                                                                 |        arXiv        |  [Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models](https://arxiv.org/abs/2401.10745)  |                                 **Ethical AI**&**Governance**                                 |
| 24.02 |                                                University of Manchester, Idiap Research Institute, National Biomarker Centre CRUK-MI                                                 |        arxiv        |                      [Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement](https://arxiv.org/abs/2402.00745)                      |                     **Ethical Reasoning**&**Neuro-Symbolic Integration**                      |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars
