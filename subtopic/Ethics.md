# Ethics

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating witha the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                                         Institute                                                                         | Publication |                                                                               Paper                                                                                |                                  Keywords                                  |
|:-----:|:---------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------:|
| 23.06 |                                                        University of Illinois at Urbana-Champaign                                                         |    arxiv    |                           [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)                           |             **Robustness**&**Ethics**&**Privacy**&**Toxicity**             |
| 23.11 |                                                                  Allen Institute for AI                                                                   |    arxiv    |                               [Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs](https://arxiv.org/abs/2311.04892)                               |                          **Bias**&**Stereotypes**                          |
| 23.11 |                                                                     Adobe Inc. India                                                                      |    arxiv    |                [All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation](https://arxiv.org/abs/2311.05451)                 |                          **Fairness**&**Biases**                           |
| 23.11 |                                                            National Taiwan University, Meta AI                                                            |    arXiv    |                     [Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems](https://arxiv.org/abs/2311.06513)                      |     **Task-oriented Dialogue Systems**&**Societal Bias**&**Fairness**      |
| 23.11 |                                                             UNC Chapel Hill, IBM Research MIT                                                             |    arxiv    |                         [Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion](https://arxiv.org/abs/2311.07682)                         |       **Model Fusion**&**Bias Reduction**&**Selective Memorization**       |
| 23.11 |                                                                        ETH Z√ºrich                                                                         |    arxiv    |              [Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures](https://arxiv.org/abs/2311.08605)               |                 **Political Debates**&**Bias Attribution**                 |
| 23.11 |                                                       University of Pisa, University of Copenhagen                                                        |    arxiv    |                                 [Social Bias Probing: Fairness Benchmarking for Language Models](https://arxiv.org/abs/2311.09090)                                 |    **Social Biases**&**Fairness Benchmarking**&**Identity Stereotypes**    |
| 23.11 |                                                                    Tsinghua University                                                                    |    arxiv    | [ALIGNMENT IS NOT SUFFICIENT TO PREVENT LARGE LANGUAGE MODELS FROM GENERATING HARMFUL INFORMATION: A PSYCHOANALYTIC PERSPECTIVE](https://arxiv.org/abs/2311.08487) |             **Alignment**&**Psychoanalysis Theory**&**Ethics**             |
| 23.11 |                                                       University of Toronto, University of Michigan                                                       |    arxiv    |                [Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks](https://arxiv.org/abs/2311.09730)                 |                      **Gender Bias**&**Racial Bias**                       |
| 23.11 |                                       University of Michigan, University of Hawaii at Hilo, Northeastern University                                       |    arxiv    |                                 [MOKA: Moral Knowledge Augmentation for Moral Event Extraction](https://arxiv.org/abs/2311.09733)                                  | **Moral Event Extraction**&**MOKA Framewor**k&**External Moral Knowledge** |
| 23.11 | Mila - Quebec AI Institute, Universit√© du Qu√©bec √† Montr√©al, Data & Society Research Institute, Mantium, IBM Research, University of California Riverside |    arxiv    |                             [Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset](https://arxiv.org/abs/2311.09443)                              |       **Subtle Misogyny**&B**iasly Dataset**&**Dataset Development**       |
| 23.11 |                                     Illinois Institute of Technology, University of Illinois Chicago, Cisco Research                                      |    arxiv    |                        [Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models](https://arxiv.org/abs/2311.09428)                         |    **Fairness**&**Abusive Language Detection**&**Adversarial Attacks**     |
| 23.11 |                                     Cornell University, KTH Royal Institute of Technology, University of Pennsylvania                                     |    arxiv    |                                         [Auditing and Mitigating Cultural Bias in LLMs](https://arxiv.org/abs/2311.14096)                                          |                **Cultural Bias**&**Mitigation Strategies**                 |
| 23.11 |                                                          University College London, Holistic AI                                                           |    arxiv    |                       [Towards Auditing Large Language Models: Improving Text-based Stereotype Detection](https://arxiv.org/abs/2311.14126)                        |                          **Stereotype Detection**                          |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars
