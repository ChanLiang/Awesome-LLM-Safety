# Ethics

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating witha the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                                         Institute                                                                         |     Publication     |                                                                               Paper                                                                                |                                  Keywords                                  |
|:-----:|:---------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------:|
| 23.06 |                                                        University of Illinois at Urbana-Champaign                                                         |        arxiv        |                           [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)                           |             **Robustness**&**Ethics**&**Privacy**&**Toxicity**             |
| 23.11 |                                                                  Allen Institute for AI                                                                   |        arxiv        |                               [Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs](https://arxiv.org/abs/2311.04892)                               |                          **Bias**&**Stereotypes**                          |
| 23.11 |                                                                     Adobe Inc. India                                                                      |        arxiv        |                [All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation](https://arxiv.org/abs/2311.05451)                 |                          **Fairness**&**Biases**                           |
| 23.11 |                                                            National Taiwan University, Meta AI                                                            |        arXiv        |                     [Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems](https://arxiv.org/abs/2311.06513)                      |     **Task-oriented Dialogue Systems**&**Societal Bias**&**Fairness**      |
| 23.11 |                                                             UNC Chapel Hill, IBM Research MIT                                                             |        arxiv        |                         [Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion](https://arxiv.org/abs/2311.07682)                         |       **Model Fusion**&**Bias Reduction**&**Selective Memorization**       |
| 23.11 |                                                                        ETH Z√ºrich                                                                         |        arxiv        |              [Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures](https://arxiv.org/abs/2311.08605)               |                 **Political Debates**&**Bias Attribution**                 |
| 23.11 |                                                       University of Pisa, University of Copenhagen                                                        |        arxiv        |                                 [Social Bias Probing: Fairness Benchmarking for Language Models](https://arxiv.org/abs/2311.09090)                                 |    **Social Biases**&**Fairness Benchmarking**&**Identity Stereotypes**    |
| 23.11 |                                                                    Tsinghua University                                                                    |        arxiv        | [ALIGNMENT IS NOT SUFFICIENT TO PREVENT LARGE LANGUAGE MODELS FROM GENERATING HARMFUL INFORMATION: A PSYCHOANALYTIC PERSPECTIVE](https://arxiv.org/abs/2311.08487) |             **Alignment**&**Psychoanalysis Theory**&**Ethics**             |
| 23.11 |                                                       University of Toronto, University of Michigan                                                       |        arxiv        |                [Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks](https://arxiv.org/abs/2311.09730)                 |                      **Gender Bias**&**Racial Bias**                       |
| 23.11 |                                       University of Michigan, University of Hawaii at Hilo, Northeastern University                                       |        arxiv        |                                 [MOKA: Moral Knowledge Augmentation for Moral Event Extraction](https://arxiv.org/abs/2311.09733)                                  | **Moral Event Extraction**&**MOKA Framewor**k&**External Moral Knowledge** |
| 23.11 | Mila - Quebec AI Institute, Universit√© du Qu√©bec √† Montr√©al, Data & Society Research Institute, Mantium, IBM Research, University of California Riverside |        arxiv        |                             [Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset](https://arxiv.org/abs/2311.09443)                              |       **Subtle Misogyny**&B**iasly Dataset**&**Dataset Development**       |
| 23.11 |                                     Illinois Institute of Technology, University of Illinois Chicago, Cisco Research                                      |        arxiv        |                        [Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models](https://arxiv.org/abs/2311.09428)                         |    **Fairness**&**Abusive Language Detection**&**Adversarial Attacks**     |
| 23.11 |                                     Cornell University, KTH Royal Institute of Technology, University of Pennsylvania                                     |        arxiv        |                                         [Auditing and Mitigating Cultural Bias in LLMs](https://arxiv.org/abs/2311.14096)                                          |                **Cultural Bias**&**Mitigation Strategies**                 |
| 23.11 |                                                          University College London, Holistic AI                                                           |        arxiv        |                       [Towards Auditing Large Language Models: Improving Text-based Stereotype Detection](https://arxiv.org/abs/2311.14126)                        |                          **Stereotype Detection**                          |
| 23.11 |                                                              Georgia Institute of Technology                                                              |        arxiv        |                             [Evaluating Large Language Models through Gender and Racial Stereotypes](https://arxiv.org/abs/2311.14788)                             |      **Gender Bias**&**Racial Stereotypes**&**Evaluation Framework**       |
| 23.11 |                                Polytechnique Montr√©al, √âTS Montr√©al, CISPA-Helmholtz Center for Information Security, Mila                                |        arxiv        |                                       [Survey on AI Ethics: A Socio-technical Perspective](https://arxiv.org/abs/2311.17228)                                       |            **AI Ethics&****Trustworthiness**&**Responsibility**            |
| 23.11 |                                                                           Meta                                                                            |      EMNLP2023      |                               [ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://arxiv.org/abs/2311.18140)                               |               **Bias Evaluation**&**Fairness**&**Toxicity**                |
| 23.11 |                                                        Comcast Applied AI, University of Waterloo                                                         |        arxiv        |                   [What Do Llamas Really Think? Revealing Preference Biases in Language Model Representations](https://arxiv.org/abs/2311.18812)                   |      **Implicit Bias**&**Sociodemographic Bias**&B**ias Mitigation**       |
| 23.12 |                                            University of Auckland, University of Waikato, University of Macau                                             |        arXiv        |                  [Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies](https://arxiv.org/abs/2312.01509)                  |                   **Governmental Regulations**&**Bias**                    |
| 23.12 |               Yildiz Technical University, Istanbul Technical University, Maersk Mc-kinney Moeller Institute University of Southern Denmark               |        arXiv        |                 [Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection](https://arxiv.org/abs/2312.01787)                 |                **Offensive Language**&**Data-augmentation**                |
| 23.12 |                                                                    TCS Research India                                                                     | EMNLP2023(Workshop) |                [Towards Mitigating Perceived Unfairness in Contracts from a Non-Legal Stakeholder‚Äôs Perspective](https://arxiv.org/abs/2312.01398)                 |       **Fairness**&**Non-legal Stakeholders**&**Data Augmentation**        |
| 23.12 |                                                                     Korea University                                                                      |      EMNLP2023      |                        [Improving Bias Mitigation through Bias Experts in Natural Language Understanding](https://arxiv.org/abs/2312.03577)                        |             **Bias Mitigation**&**Multi-Class Classification**             |
| 23.12 |                                            Queensland University of Technology, University of New South Wales                                             | EMNLP2023(Workshop) |       [Measuring Misogyny in Natural Language Generation: Preliminary Results from a Case Study on two Reddit Communities](https://arxiv.org/abs/2312.03330)       |            **Misogyny**&**Toxicity Classifier**s&**Benchmark**             |
| 23.12 |                     Carnegie Mellon University, Pittsburgh, Universidade NOVA de Lisboa, Allen Institute for Artificial Intelligence                      |      EMNLP2023      |                     [Understanding the Effect of Model Compression on Social Bias in Large Language Models](https://arxiv.org/abs/2312.05662)                      |     **Model Compression**&**Social Bias**&&**Knowledge Distillation**      |
| 23.12 |                    Eindhoven University of Technology, University of Liverpool, Griffith University, The Pennsylvania State University                    |        arXiv        |                        [GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models](https://arxiv.org/abs/2312.06315)                         |  **Bias Evaluation**&**Bias Attack Instructions**&**Intersectional Bias**  |
| 23.12 |                                                                      IBM Research AI                                                                      |      AAAI2024       |                   [SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models](https://arxiv.org/abs/2312.07492)                    |               **Social Bias**&**Question-answering Dataset**               |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars
