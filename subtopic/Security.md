# Security

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |      Institute       |                                     Publication                                     |                                                                                            Paper                                                                                            |                                      Keywords                                      |
|:-----:|:--------------------:|:-----------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------:|
| 20.10 | Facebook AI Research |                                        arxiv                                        |                                                       [Recipes for Safety in Open-domain Chatbots](https://arxiv.org/abs/2010.07079)                                                        |                         **Toxic Behavior**&**Open-domain**                         |
| 22.02 |       DeepMind       |                                      EMNLP2022                                      |                                              [Red Teaming Language Models with Language Model](https://aclanthology.org/2022.emnlp-main.225/)                                               |                           **Red Teaming**&**Harm Test**                            |
| 22.03 |        OpenAI        |                                      NIPS2022                                       | [Training language models to follow instructions with human feedback](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html) |                       **InstructGPT**&**RLHF**&**Harmless**                        |
| 22.04 |      Anthropic       |                                        arxiv                                        |                                [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862)                                |                              **Helpful**&**Harmless**                              |
| 22.05 |         UCSD         |                                      EMNLP2022                                      |                             [An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models](https://aclanthology.org/2022.emnlp-main.119/)                             |                         **Privacy Risks**&**Memorization**                         |
| 22.09 |      Anthropic       |                                        arxiv                                        |                              [Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/abs/2209.07858)                               |                      **Red Teaming**&**Harmless**&**Helpful**                      |
| 22.12 |      Anthropic       |                                        arxiv                                        |                                                    [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)                                                     |                    **Harmless**&**Self-improvement**&**RLAIF**                     |
| 23.07 |     UC Berkeley      |                                        arxiv                                        |                                                     [Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)                                                      |        **Jailbreak**&**Competing Objectives**&**Mismatched Generalization**        |
| 23.09 |  Peking University   |                                        arxiv                                        |                                           [RAIN: Your Language Models Can Align Themselves without Finetuning](https://arxiv.org/abs/2309.07124)                                            |                      **Self-boosting**&**Rewind Mechanisms**                       |
| 23.11 |       KAIST AI       |                                        arxiv                                        |                                           [HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning](https://arxiv.org/abs/2311.00321)                                           |                           **Hate Speech**&**Detection**                            |
| 23.11 |         CMU          |                          AACL2023(ART or Safety workshop)                           |                                                             [Measuring Adversarial Datasets](https://arxiv.org/abs/2311.03566)                                                              |         **Adversarial Robustness**&**AI Safety**&**Adversarial Datasets**          |



## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars
