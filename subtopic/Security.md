# Security

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                                                                                       Institute                                                                                        |           Publication            |                                                                                            Paper                                                                                            |                                            Keywords                                             |
|:-----:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------:|
| 20.10 |                                                                                  Facebook AI Research                                                                                  |              arxiv               |                                                       [Recipes for Safety in Open-domain Chatbots](https://arxiv.org/abs/2010.07079)                                                        |                               **Toxic Behavior**&**Open-domain**                                |
| 22.02 |                                                                                        DeepMind                                                                                        |            EMNLP2022             |                                              [Red Teaming Language Models with Language Model](https://aclanthology.org/2022.emnlp-main.225/)                                               |                                  **Red Teaming**&**Harm Test**                                  |
| 22.03 |                                                                                         OpenAI                                                                                         |             NIPS2022             | [Training language models to follow instructions with human feedback](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html) |                              **InstructGPT**&**RLHF**&**Harmless**                              |
| 22.04 |                                                                                       Anthropic                                                                                        |              arxiv               |                                [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862)                                |                                    **Helpful**&**Harmless**                                     |
| 22.05 |                                                                                          UCSD                                                                                          |            EMNLP2022             |                             [An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models](https://aclanthology.org/2022.emnlp-main.119/)                             |                               **Privacy Risks**&**Memorization**                                |
| 22.09 |                                                                                       Anthropic                                                                                        |              arxiv               |                              [Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/abs/2209.07858)                               |                            **Red Teaming**&**Harmless**&**Helpful**                             |
| 22.12 |                                                                                       Anthropic                                                                                        |              arxiv               |                                                    [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)                                                     |                           **Harmless**&**Self-improvement**&**RLAIF**                           |
| 23.07 |                                                                                      UC Berkeley                                                                                       |             NIPS2023             |                                                     [Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)                                                      |              **Jailbreak**&**Competing Objectives**&**Mismatched Generalization**               |
| 23.08 |                                        The Chinese University of Hong Kong Shenzhen China, Tencent AI Lab, The Chinese University of Hong Kong                                         |              arxiv               |                                            [GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs Via Cipher](https://arxiv.org/abs/2308.06463)                                            |                           **Safety Alignment**&**Adversarial Attack**                           |
| 23.08 |                                                        University College London, University College London, Tilburg University                                                        |              arxiv               |                                   [Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities](https://arxiv.org/abs/2308.12833)                                   |                                  **Security**&**AI Alignment**                                  |
| 23.09 |                                                                                   Peking University                                                                                    |              arxiv               |                                           [RAIN: Your Language Models Can Align Themselves without Finetuning](https://arxiv.org/abs/2309.07124)                                            |                             **Self-boosting**&**Rewind Mechanisms**                             |
| 23.10 |                                                         Princeton University, Virginia Tech, IBM Research, Stanford University                                                         |              arxiv               |                                [FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY EVEN WHEN USERS DO NOT INTEND TO!](https://arxiv.org/abs/2310.03693)                                 |                    **Fine-tuning****Safety Risks**&**Adversarial Training**                     |
| 23.11 |                                                                                        KAIST AI                                                                                        |              arxiv               |                                           [HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning](https://arxiv.org/abs/2311.00321)                                           |                                  **Hate Speech**&**Detection**                                  |
| 23.11 |                                                                                          CMU                                                                                           | AACL2023(ART or Safety workshop) |                                                             [Measuring Adversarial Datasets](https://arxiv.org/abs/2311.03566)                                                              |                **Adversarial Robustness**&**AI Safety**&**Adversarial Datasets**                |
| 23.11 |                                                                                          UIUC                                                                                          |              arxiv               |                                                   [Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/abs/2311.05553)                                                    |                              **Remove Protection**&**Fine-Tuning**                              |
| 23.11 |                                                                  IT University of CopenhagenÔºåUniversity of Washington                                                                  |              arxiv               |                                      [Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild](https://arxiv.org/abs/2311.06237)                                       |                                         **Red Teaming**                                         |
| 23.11 |                                                                            Fudan University&Shanghai AI lab                                                                            |              arxiv               |                                                      [Fake Alignment: Are LLMs Really Aligned Well?](https://arxiv.org/abs/2311.05915)                                                      |                           **Alignment Failure**&**Safety Evaluation**                           |
| 23.11 |                                                                           University of Southern California                                                                            |              arxiv               |                                         [SAFER-INSTRUCT: Aligning Language Models with Automated Preference Data](https://arxiv.org/abs/2311.08685)                                         |                                       **RLHF**&**Safety**                                       |
| 23.11 |                                                                                    Google Research                                                                                     |              arxiv               |                               [AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications](https://arxiv.org/abs/2311.08592)                               |           **Adversarial Testing**&**AI-Assisted Red Teaming**&**Application Safety**            |
| 23.11 |                                                                                     Tencent AI Lab                                                                                     |              arxiv               |                                                           [ADVERSARIAL PREFERENCE OPTIMIZATION](https://arxiv.org/abs/2311.08045)                                                           | **Human Preference Alignment**&**Adversarial Preference Optimization**&**Annotation Reduction** |
| 23.11 |                                                                                        Docta.ai                                                                                        |              arxiv               |                          [Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models](https://arxiv.org/abs/2311.11202)                          |                            **Data Credibility**&**Safety alignment**                            |
| 23.11 |                                                                                  CIIRC CTU in Prague                                                                                   |              arxiv               |                                                   [A Security Risk Taxonomy for Large Language Models](https://arxiv.org/abs/2311.11415)                                                    |                    **Security risks**&**Taxonomy**&**Prompt-based attacks**                     |
| 23.12 |                                                                                   Drexel University                                                                                    |              arXiv               |                               [A Survey on Large Language Model (LLM) Security and Privacy: The Good the Bad and the Ugly](https://arxiv.org/abs/2312.02003)                                |                              **Security**&**Privacy**&**Attacks**                               |
| 23.12 |                                                                                         Tenyx                                                                                          |              arXiv               |                                  [Characterizing Large Language Model Geometry Solves Toxicity Detection and Generation](https://arxiv.org/abs/2312.01648)                                  |           **Geometric Interpretation**&**Intrinsic Dimension**&**Toxicity Detection**           |
| 23.12 |                                                                          Independent (Now at Google DeepMind)                                                                          |              arXiv               |                                           [Scaling Laws for Adversarial Attacks on Language Model Activations](https://arxiv.org/abs/2312.02780)                                            |             **Adversarial Attacks**&**Language Model Activations**&**Scaling Laws**             |
| 23.12 |                                                                 University of Liechtenstein, University of Duesseldorf                                                                 |              arxiv               |                                         [NEGOTIATING WITH LLMS: PROMPT HACKS, SKILL GAPS, AND REASONING DEFICITS](https://arxiv.org/abs/2312.03720)                                         |                        **Negotiation**&**Reasoning**&**Prompt Hacking**                         |
| 23.12 |                                             University of Wisconsin Madison, University of Michigan Ann Arbor, ASU, Washington University                                              |              arXiv               |                                            [Exploring the Limits of ChatGPT in Software Security Applications](https://arxiv.org/abs/2312.05275)                                            |                                        Software Security                                        |
| 23.12 |                                                                                     GenAI at Meta                                                                                      |              arxiv               |                                        [Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations](https://arxiv.org/abs/2312.06674)                                         |                           Human-AI Conversation&Safety Risk taxonomy                            |
| 23.12 |                                                                     University of California Riverside, Microsoft                                                                      |              arxiv               |                                   [Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack](https://arxiv.org/abs/2312.06924)                                   |                          Safety Alignment&Summarization&Vulnerability                           |
| 23.12 |                                                                                      MIT, Harvard                                                                                      |        NIPS2023(Workshop)        |                                          [Forbidden Facts: An Investigation of Competing Objectives in Llama-2](https://arxiv.org/abs/2312.08793)                                           |            **Competing Objectives**&**Forbidden Fact Task**&**Model Decomposition**             |
| 23.12 |                                                                     University of Science and Technology of China                                                                      |              arxiv               |                                  [Silent Guardian: Protecting Text from Malicious Exploitation by Large Language Models](https://arxiv.org/abs/2312.09669)                                  |                             **Text Protection**&**Silent Guardian**                             |
| 23.12 |                                                                                         OpenAI                                                                                         |             Open AI              |                                 [Practices for Governing Agentic AI Systems](https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf)                                  |                            **Agentic AI Systems**&**LM Based Agent**                            |
| 23.12 |                                       University of Massachusetts Amherst, Columbia University, Google, Stanford University, New York University                                       |              arxiv               |                                            [Learning and Forgetting Unsafe Examples in Large Language Models](https://arxiv.org/abs/2312.12736)                                             |                 **Safety Issues**&**ForgetFilter Algorithm**&**Unsafe Content**                 |
| 23.12 |                                                                  Tencent AI Lab, The Chinese University of Hong Kong                                                                   |              arxiv               |                                                         [Aligning Language Models with Judgments](https://arxiv.org/abs/2312.14591)                                                         |                  **Judgment Alignment**&**Contrastive Unlikelihood Training**                   |
| 24.01 |                                                                             Delft University of Technology                                                                             |              arxiv               |                              [Red Teaming for Large Language Models At Scale: Tackling Hallucinations on Mathematics Tasks](https://arxiv.org/abs/2401.00290)                               |                    **Red Teaming**&**Hallucinations**&**Mathematics Tasks**                     |
| 24.01 |                                                 Apart Research, University of Edinburgh, Imperial College London, University of Oxford                                                 |              arxiv               |                                                     [Large Language Models Relearn Removed Concepts](https://arxiv.org/abs/2401.01814)                                                      |                         **Neuroplasticity**&**Concept Redistribution**                          |
| 24.01 | Tsinghua University, Xiaomi AI Lab, Huawei, Shenzhen Heytap Technology, vivo AI Lab, Viomi Technology, Li Auto, Beijing University of Posts and Telecommunications, Soochow University |              arxiv               |                                  [PERSONAL LLM AGENTS: INSIGHTS AND SURVEY ABOUT THE CAPABILITY EFFICIENCY AND SECURITY](https://arxiv.org/abs/2401.05459)                                  |            **Intelligent Personal Assistant**&**LLM Agent**&**Security and Privacy**            |
| 24.01 |                               Zhongguancun Laboratory, Tsinghua University, Institute of Information Engineering Chinese Academy of Sciences, Ant Group                                |              arxiv               |                                   [Risk Taxonomy Mitigation and Assessment Benchmarks of Large Language Model Systems](https://arxiv.org/abs/2401.05778)                                    |                     **Safety**&**Risk Taxonomy**&**Mitigation Strategies**                      |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

| Date  |  Type  |                                Title                                 |                           URL                            |
|:-----:|:------:|:--------------------------------------------------------------------:|:--------------------------------------------------------:|
| 23.01 | video  | ChatGPT and InstructGPT: Aligning Language Models to Human Intention | [link](https://www.youtube.com/watch?v=RkFS6-GwCxE&t=6s) |
| 23.06 | Report |         ‚ÄúDual-use dilemma‚Äù for GenAI Workshop Summarization          |         [link](https://arxiv.org/abs/2308.14840)         |
| 23.10 |  News  |              Joint Statement on AI Safety and Openness               |         [link](https://open.mozilla.org/letter/)         |

## üßë‚Äçüè´Scholars
