# Jailbreaks&Attackl

## ðŸ“‘PapersðŸ“‘

- 22.11 - NIPS2022(ML Safety Workshop) - [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527)
  - **Prompt Injection**&**Misaligned**
- 23.02 - arxiv - [Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173)
  - **Adversarial Prompting**&**Indirect Prompt Injection**&**LLM-Integrated Applications**
- 23.06 - arxiv - [Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)
  - **Multimodal**&**Jailbreak**
- 23.07 - arxiv - [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)
  - **Jailbreak**&**Transferable Attack**&**Adversarial Attack**

## ðŸ’»Presentations & TalksðŸ’»


## ðŸ“–Tutorials & WorkshopsðŸ“–

- 23.10 - GitHub - [awesome-LLM-safety-paper-list](https://github.com/ydyjya/awesome-LLM-safety-paper-list)
  - **Repo**&**Tutorials**

## ðŸ“°News & ArticlesðŸ“°