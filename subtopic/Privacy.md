# Privacy

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                               Institute                                |       Publication        |                                                                                          Paper                                                                                           |                                       Keywords                                        |
|:-----:|:----------------------------------------------------------------------:|:------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------:|
| 18.02 |                              Google Brain                              |   USENIX Security 2021   |        [The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)        |                               **Memorization**&**LSTM**                               |
| 19.12 |                               Microsoft                                |         CCS2020          |                                [Analyzing Information Leakage of Updates to Natural Language Models](https://dl.acm.org/doi/abs/10.1145/3372297.3417880)                                 |                  **Privacy Leakage**&**Model Update**&**Duplicated**                  |
| 21.07 |                            Google Research                             |         ACL2022          |                                         [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)                                          |               **Privacy Protected**&**Deduplication**&**Memorization**                |
| 21.10 |                                Stanford                                |         ICLR2022         |                                    [Large language models can be strong differentially private learners](https://openreview.net/forum?id=bVuP3ltATMz)                                    |                    **Differential Privacy**&**Gradient Clipping**                     |
| 22.02 |                            Google Research                             |         ICLR2023         |                                           [Quantifying Memorization Across Neural Language Models](https://openreview.net/forum?id=TatRHT_1cK)                                           |                        **Memorization**&**Verbatim Sequence**                         |
| 22.02 |                            UNC Chapel Hill                             |         ICML2022         |                               [Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://proceedings.mlr.press/v162/kandpal22a.html)                               |                    **Memorization**&**Deduplicate Training Data**                     |
| 22.05 |                                  UCSD                                  |        EMNLP2022         |                           [An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models](https://aclanthology.org/2022.emnlp-main.119/)                            |                          **Privacy Risks**&**Memorization**                           |
| 22.05 |                               Princeton                                |         NIPS2022         | [Recovering Private Text in Federated Learning of Language Models](https://proceedings.neurips.cc/paper_files/paper/2022/hash/35b5c175e139bff5f22a5361270fce87-Abstract-Conference.html) |                       **Federated Learning**&**Gradient Based**                       |
| 22.05 |               University of Illinois at Urbana-Champaign               |   EMNLP2022(findings)    |                              [Are Large Pre-Trained Language Models Leaking Your Personal Information?](https://aclanthology.org/2022.findings-emnlp.148/)                               |              **Personal Information**&**Memorization**&**Privacy Risk**               |
| 22.10 |                            Google Research                             |         INLG2023         |                      [Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy](https://aclanthology.org/2023.inlg-main.3/)                      |            **Verbatim Memorization**&**Filter**&**Style Transfer Prompts**            |
| 23.02 |                         University of Waterloo                         | Security and Privacy2023 |             [Analyzing Leakage of Personally Identifiable Information in Language Models](https://www.computer.org/csdl/proceedings-article/sp/2023/933600a346/1NrbXJj80H6)              |            **PII Leakage**&**PII Reconstruction**&**Differential Privacy**            |
| 23.04 |             Hong Kong University of Science and Technology             |   EMNLP2023(findings)    |                                                  [Multi-step Jailbreaking Privacy Attacks on ChatGPT](https://arxiv.org/abs/2304.05197)                                                  |                              **Privacy**&**Jailbreaks**                               |
| 23.05 |               University of Illinois at Urbana-Champaign               |          arxiv           |                        [Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage](https://arxiv.org/abs/2305.12707)                         |                               **Co-occurrence**&**PII**                               |
| 23.05 |                   The University of Texas at Dallas                    |         ACL2023          |                               [Controlling the Extraction of Memorized Datafrom Large Language Models via Prompt-Tuning](https://arxiv.org/abs/2305.11759)                               |                          **Prompt-Tuning**&**Memorization**                           |
| 23.06 |               University of Illinois at Urbana-Champaign               |          arxiv           |                                      [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)                                      |                  **Robustness**&**Ethics**&**Privacy**&**Toxicity**                   |
| 23.09 |                            UNC Chapel Hill                             |          arxiv           |                         [Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://arxiv.org/abs/2309.17410)                          | **Hidden States Attack**&**Hidden States Defense**&**Deleting Sensitive Information** |
| 23.09 |                     Princeton University&Microsoft                     |          arxiv           |                                [Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation](https://arxiv.org/abs/2309.11765)                                |                   **In-Context Learning**&**Differential Privacy**                    |
| 23.10 |                                  ETH                                   |          arxiv           |                                   [Beyond Memorization: Violating Privacy Via Inference with Large Language Models](https://arxiv.org/abs/2310.07298)                                    |              **Context Inference**&**Privacy-Invasive**&**Extract PII**               |
| 23.10 | University of Washington & Allen Institute for Artificial Intelligence |          arxiv           |                       [Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory](https://arxiv.org/abs/2310.17884)                        |               **Benchmark**&**Contextual Privacy**&**Chain-of-thought**               |
| 23.10 |                    Georgia Institute of Technology                     |          arxiv           |                                            [Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://arxiv.org/abs/2310.20150)                                            |           **Unlearning**&**Teacher-student Framework**&**Data Protection**            |
| 23.10 |                           Tianjin University                           |        EMNLP2023         |                                      [DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models](https://arxiv.org/abs/2310.20138)                                       |         **Privacy Neuron Detection**&**Model Editing**&**Data Memorization**          |

## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles
| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.11 | News  |Wild: GPT-3.5 leaked a random dude's photo in the output. | [link](https://twitter.com/thealexker/status/1719896871009694057) |

## üßë‚Äçüè´Scholars
