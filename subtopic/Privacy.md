# Privacy

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                                                       Institute                                                                                        |        Publication        |                                                                                          Paper                                                                                           |                                               Keywords                                                |
|:-----:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------:|
| 18.02 |                                                                                      Google Brain                                                                                      |   USENIX Security 2021    |        [The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)        |                                       **Memorization**&**LSTM**                                       |
| 19.12 |                                                                                       Microsoft                                                                                        |          CCS2020          |                                [Analyzing Information Leakage of Updates to Natural Language Models](https://dl.acm.org/doi/abs/10.1145/3372297.3417880)                                 |                          **Privacy Leakage**&**Model Update**&**Duplicated**                          |
| 21.07 |                                                                                    Google Research                                                                                     |          ACL2022          |                                         [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)                                          |                       **Privacy Protected**&**Deduplication**&**Memorization**                        |
| 21.10 |                                                                                        Stanford                                                                                        |         ICLR2022          |                                    [Large language models can be strong differentially private learners](https://openreview.net/forum?id=bVuP3ltATMz)                                    |                            **Differential Privacy**&**Gradient Clipping**                             |
| 22.02 |                                                                                    Google Research                                                                                     |         ICLR2023          |                                           [Quantifying Memorization Across Neural Language Models](https://openreview.net/forum?id=TatRHT_1cK)                                           |                                **Memorization**&**Verbatim Sequence**                                 |
| 22.02 |                                                                                    UNC Chapel Hill                                                                                     |         ICML2022          |                               [Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://proceedings.mlr.press/v162/kandpal22a.html)                               |                            **Memorization**&**Deduplicate Training Data**                             |
| 22.05 |                                                                                          UCSD                                                                                          |         EMNLP2022         |                           [An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models](https://aclanthology.org/2022.emnlp-main.119/)                            |                                  **Privacy Risks**&**Memorization**                                   |
| 22.05 |                                                                                       Princeton                                                                                        |         NIPS2022          | [Recovering Private Text in Federated Learning of Language Models](https://proceedings.neurips.cc/paper_files/paper/2022/hash/35b5c175e139bff5f22a5361270fce87-Abstract-Conference.html) |                               **Federated Learning**&**Gradient Based**                               |
| 22.05 |                                                                       University of Illinois at Urbana-Champaign                                                                       |    EMNLP2022(findings)    |                              [Are Large Pre-Trained Language Models Leaking Your Personal Information?](https://aclanthology.org/2022.findings-emnlp.148/)                               |                      **Personal Information**&**Memorization**&**Privacy Risk**                       |
| 22.10 |                                                                                    Google Research                                                                                     |         INLG2023          |                      [Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy](https://aclanthology.org/2023.inlg-main.3/)                      |                    **Verbatim Memorization**&**Filter**&**Style Transfer Prompts**                    |
| 23.02 |                                                                                 University of Waterloo                                                                                 | Security and Privacy2023  |             [Analyzing Leakage of Personally Identifiable Information in Language Models](https://www.computer.org/csdl/proceedings-article/sp/2023/933600a346/1NrbXJj80H6)              |                    **PII Leakage**&**PII Reconstruction**&**Differential Privacy**                    |
| 23.04 |                                                                     Hong Kong University of Science and Technology                                                                     |    EMNLP2023(findings)    |                                                  [Multi-step Jailbreaking Privacy Attacks on ChatGPT](https://arxiv.org/abs/2304.05197)                                                  |                                      **Privacy**&**Jailbreaks**                                       |
| 23.05 |                                                                       University of Illinois at Urbana-Champaign                                                                       |           arxiv           |                        [Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage](https://arxiv.org/abs/2305.12707)                         |                                       **Co-occurrence**&**PII**                                       |
| 23.05 |                                                                           The University of Texas at Dallas                                                                            |          ACL2023          |                               [Controlling the Extraction of Memorized Datafrom Large Language Models via Prompt-Tuning](https://arxiv.org/abs/2305.11759)                               |                                  **Prompt-Tuning**&**Memorization**                                   |
| 23.05 |                                                                                    Google Research                                                                                     |    NAACL2024(findings)    |                                    [Can Public Large Language Models Help Private Cross-device Federated Learning?](https://arxiv.org/abs/2305.12132)                                    |               **Federated Learning**&**Large Language Models**&**Differential Privacy**               |
| 23.06 |                                                                       University of Illinois at Urbana-Champaign                                                                       |           arxiv           |                                      [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)                                      |                          **Robustness**&**Ethics**&**Privacy**&**Toxicity**                           |
| 23.08 |                                                                          Bern University of Applied Sciences                                                                           |    NAACL2024(findings)    |                       [Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models in Court Decisions](https://arxiv.org/abs/2308.11103)                        |                   **Anonymization**&**Re-Identification**&**Large Language Models**                   |
| 23.09 |                                                                                    UNC Chapel Hill                                                                                     |           arxiv           |                         [Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://arxiv.org/abs/2309.17410)                          |         **Hidden States Attack**&**Hidden States Defense**&**Deleting Sensitive Information**         |
| 23.09 |                                                                             Princeton University&Microsoft                                                                             |           arxiv           |                                [Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation](https://arxiv.org/abs/2309.11765)                                |                           **In-Context Learning**&**Differential Privacy**                            |
| 23.10 |                                                                                          ETH                                                                                           |           arxiv           |                                   [Beyond Memorization: Violating Privacy Via Inference with Large Language Models](https://arxiv.org/abs/2310.07298)                                    |                      **Context Inference**&**Privacy-Invasive**&**Extract PII**                       |
| 23.10 |                                                                             Indiana University Bloomington                                                                             |         CCS 2024          |                              [The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks](https://arxiv.org/abs/2310.15469)                               |                                  **Privacy risks**&**PII Recovery**                                   |
| 23.10 |                                                         University of Washington & Allen Institute for Artificial Intelligence                                                         |           arxiv           |                       [Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory](https://arxiv.org/abs/2310.17884)                        |                       **Benchmark**&**Contextual Privacy**&**Chain-of-thought**                       |
| 23.10 |                                                                            Georgia Institute of Technology                                                                             |           arxiv           |                                            [Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://arxiv.org/abs/2310.20150)                                            |                   **Unlearning**&**Teacher-student Framework**&**Data Protection**                    |
| 23.10 |                                                                                   Tianjin University                                                                                   |         EMNLP2023         |                                      [DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models](https://arxiv.org/abs/2310.20138)                                       |                 **Privacy Neuron Detection**&**Model Editing**&**Data Memorization**                  |
| 23.11 |                                                                                  Zhejiang University                                                                                   |           arxiv           |                                     [Input Reconstruction Attack against Vertical Federated Large Language Models](https://arxiv.org/abs/2311.07585)                                     |             **Vertical Federated Learning**&**Input Reconstruction**&**Privacy Concerns**             |
| 23.11 |                                                              Georgia Institute of Technology, Carnegie Mellon University                                                               |           arxiv           |                                        [Reducing Privacy Risks in Online Self-Disclosures with Language Models](https://arxiv.org/abs/2311.09538)                                        |             **Online Self-Disclosure**&**Privacy Risks**&**Self-Disclosure Abstraction**              |
| 23.11 |                                                                                   Cornell University                                                                                   |           arxiv           |                                                               [Language Model Inversion](https://arxiv.org/abs/2311.13647)                                                               |                       **Model Inversion**&**Prompt Reconstruction**&**Privacy**                       |
| 23.11 |                                                                                       Ant Group                                                                                        |           arxiv           |                                                   [PrivateLoRA for Efficient Privacy Preserving LLM](https://arxiv.org/abs/2311.14030)                                                   |                                    **Privacy Preserving**&**LoRA**                                    |
| 23.12 |                                                                                   Drexel University                                                                                    |           arXiv           |                              [A Survey on Large Language Model (LLM) Security and Privacy: The Good the Bad and the Ugly](https://arxiv.org/abs/2312.02003)                              |                                 **Security**&**Privacy**&**Attacks**                                  |
| 23.12 |                                                    University of Texas at Austin, Princeton University, MIT, University of Chicago                                                     |           arxiv           |                                      [DP-OPT: MAKE LARGE LANGUAGE MODEL YOUR PRIVACY-PRESERVING PROMPT ENGINEER](https://arxiv.org/abs/2312.03724)                                       |                              **Prompt Tuning**&**Differential Privacy**                               |
| 23.12 |                                                                             Delft University of Technology                                                                             |         ICSE 2024         |                                               [Traces of Memorisation in Large Language Models for Code](https://arxiv.org/abs/2312.11658)                                               |                           **Code Memorisation**&**Data Extraction Attacks**                           |
| 23.12 |                                                                             University of Texas at Austin                                                                              |           arXiv           |                     [SentinelLMs: Encrypted Input Adaptation and Fine-tuning of Language Models for Private and Secure Inference](https://arxiv.org/abs/2312.17342)                      |                        **Privacy**&**Security**&**Encrypted Input Adaptation**                        |
| 23.12 |                                                                 Rensselaer Polytechnic Institute, Columbia University                                                                  |           arXiv           |                             [Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning](https://arxiv.org/abs/2312.17493)                              |               **Federated Learning**&**Differential Privacy**&**Efficient Fine-Tuning**               |
| 24.01 |                                                         Harbin Institute of Technology Shenzhen&Peng Cheng Laboratory Shenzhen                                                         |           arxiv           |                             [SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models](https://arxiv.org/abs/2401.00793)                              | **Privacy-Preserving Inference (PPI)**&**Secure Multi-Party Computing (SMPC)**&**Transformer Models** |
| 24.01 |                                              NUS (Chongqing) Research Institute, Huawei Noah‚Äôs Ark Lab, National University of Singapore                                               |           arxiv           |                                                    [Teach Large Language Models to Forget Privacy](https://arxiv.org/abs/2401.00870)                                                     |                    **Data Privacy**&**Prompt Learning**&**Problem Decomposition**                     |
| 24.01 |                                                                     Princeton University, Google DeepMind, Meta AI                                                                     |           arxiv           |                                     [Private Fine-tuning of Large Language Models with Zeroth-order Optimization](https://arxiv.org/abs/2401.04343)                                      |                        **Differential Privacy**&**Zeroth-order Optimization**                         |
| 24.01 |                                                                    Harvard&USC&UCLA&UW Seattle&UW-Madison&UC Davis                                                                     |         NAACL2024         |                                                [Instructional Fingerprinting of Large Language Models](https://arxiv.org/abs/2401.12255)                                                 |                **Model Fingerprinting**&**Instructional Backdoor**&**Model Ownership**                |
| 24.02 |                                                                            Florida International University                                                                            |           arxiv           |                                          [Security and Privacy Challenges of Large Language Models: A Survey](https://arxiv.org/abs/2402.00888)                                          |                            **Security**&**Privacy Challenges**&**Suevey**                             |
| 24.02 |                                                 Northeastern University, Carnegie Mellon University, Rensselaer Polytechnic Institute                                                  |           arxiv           |                                         [Human-Centered Privacy Research in the Age of Large Language Models](https://arxiv.org/abs/2402.01994)                                          |                     **Generative AI**&**Privacy**&**Human-Computer Interaction**                      |
| 24.02 |                                                                    CISPA Helmholtz Center for Information Security                                                                     |           arxiv           |                                                [Conversation Reconstruction Attack Against GPT Models](https://arxiv.org/abs/2402.02987)                                                 |                 **Conversation Reconstruction Attack**&**Privacy risks**&**Security**                 |
| 24.02 |                                                                 Columbia University, M365 Research, Microsoft Research                                                                 |           arxiv           |                                             [Differentially Private Training of Mixture of Experts Models](https://arxiv.org/abs/2402.07334)                                             |                            **Differential Privacy**&**Mixture of Experts**                            |
| 24.02 |                                                                   Stanford University, Truera ,Princeton University                                                                    |           arxiv           |                                      [De-amplifying Bias from Differential Privacy in Language Model Fine-tuning](https://arxiv.org/abs/2402.04489)                                      |                      **Fairness**&**Differential Privacy**&**Data Augmentation**                      |
| 24.02 |                                                                        Sun Yat-sen University, Google Research                                                                         |           arxiv           |                                          [Privacy-Preserving Instructions for Aligning Large Language Models](https://arxiv.org/abs/2402.13659)                                          |                             **Privacy Risks**&**Synthetic Instructions**                              |
| 24.02 |                                                                       National University of Defense Technology                                                                        |           arxiv           |            [LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification](https://arxiv.org/abs/2402.16515)            |       **Privacy Data Augmentation**&**Knowledge Distillation**&**Medical Text Classification**        |
| 24.02 |                                                                         Michigan State University, Baidu Inc.                                                                          |           arxiv           |                                [The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2402.16893)                                |                         **Privacy**&**Retrieval-Augmented Generation (RAG)**                          |
| 24.02 |                                                          University of Washington&Allen Institute for Artificial Intelligence                                                          |         NAACL2024         |                          [JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models](https://arxiv.org/abs/2402.08761)                           |             **Authorship Obfuscation**&**Constrained Decoding**&**Small Language Models**             |
| 24.03 |                                                                                     Virginia Tech                                                                                      |           arxiv           |                                                [Privacy-Aware Semantic Cache for Large Language Models](https://arxiv.org/abs/2403.02694)                                                |                           **Federated Learning**&**Cache Hit**&**Privacy**                            |
| 24.03 |                                                                                  Tsinghua University                                                                                   |           arxiv           |                 [CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following](https://arxiv.org/abs/2403.03129)                  |             **Small Language Models**&**Privacy**&**Context-Aware Instruction Following**             |
| 24.03 |                                                               Shandong University, Leiden University, Drexel University                                                                |           arxiv           |                                       [On Protecting the Data Privacy of Large Language Models (LLMs): A Survey](https://arxiv.org/abs/2403.05156)                                       |                          **Data Privacy**&**Privacy Protection**&**Survey**                           |
| 24.03 |     Arizona State University, University of Minnesota, University of Science and Technology of China, North Carolina State University, University of North Carolina at Chapel Hill     |           arxiv           |                                       [Privacy-preserving Fine-tuning of Large Language Models through Flatness](https://arxiv.org/abs/2403.04124)                                       |                           **Differential Privacy**&**Model Generalization**                           |
| 24.03 |                                                                           University of Southern California                                                                            |           arxiv           |                                        [Differentially Private Next-Token Prediction of Large Language Models](https://arxiv.org/abs/2403.15638)                                         |                                       **Differential Privacy**                                        |
| 24.04 |                        University of Maryland, Oregon State University, ELLIS Institute T√ºbingen & MPI Intelligent Systems, T√ºbingen AI Center, Google DeepMind                        |           arxiv           |                                [Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models](https://arxiv.org/abs/2404.01231)                                |                  **Privacy Backdoors**&**Membership Inference**&**Model Poisoning**                   |
| 24.04 |                                                    City University of Hong Kong, The Hong Kong University of Science and Technology                                                    |           arxiv           |                                           [LMEraser: Large Model Unlearning through Adaptive Prompt Tuning](https://arxiv.org/abs/2404.11056)                                            |               **Machine Unlearning**&**Adaptive Prompt Tuning**&**Privacy Protection**                |
| 24.04 |                                               University of Electronic Science and Technology of China, Chengdu University of Technology                                               |           arxiv           |                                      [Understanding Privacy Risks of Embeddings Induced by Large Language Models](https://arxiv.org/abs/2404.16587)                                      |                                   **Privacy Risks**&**Embeddings**                                    |
| 24.04 |                                                                                 Salesforce AI Research                                                                                 |           arxiv           |                            [Investigating the prompt leakage effect and black-box defenses for multi-turn LLM interactions](https://arxiv.org/abs/2404.16251)                            |                               **Prompt Leakage**&**Black-box Defenses**                               |
| 24.04 |                                      University of Texas at El Paso, Texas A&M University Central Texas, University of Maryland Baltimore County                                       |           arxiv           |                     [PrivComp-KG: Leveraging Knowledge Graph and Large Language Models for Privacy Policy Compliance Verification](https://arxiv.org/abs/2404.19744)                     |                     **Privacy Policy**&**Policy Compliance**&**Knowledge Graph**                      |
| 24.05 |                                                                               Renmin University of China                                                                               |        COLING 2024        |                                                  [Locally Differentially Private In-Context Learning](https://arxiv.org/abs/2405.04032)                                                  |                        **In-context Learning**&**Local Differential Privacy**                         |
| 24.05 |                                                                                 University of Maryland                                                                                 |         NAACL2024         |                                              [Keep It Private: Unsupervised Privatization of Online Text](https://arxiv.org/abs/2405.10260)                                              |               **Unsupervised Privatization**&**Online Text**&**Large Language Models**                |
| 24.05 |                                                                                  Zhejiang University                                                                                   |           arxiv           |                                    [PermLLM: Private Inference of Large Language Models within 3 Seconds under WAN](https://arxiv.org/abs/2405.18744)                                    |                             **Private Inference**&**Secure Computation**                              |
| 24.05 |                                                                               University of Connecticut                                                                                |           arxiv           |                    [LMO-DP: Optimizing the Randomization Mechanism for Differentially Private Fine-Tuning of Large Language Models](https://arxiv.org/abs/2405.18776)                    |                               **Differential Privacy**&**Fine-Tuning**                                |
| 24.05 |                                                                            University of Technology, Sydney                                                                            |           arxiv           |                                        [Large Language Model Watermark Stealing with Mixed Integer Programming](https://arxiv.org/abs/2405.19677)                                        |                 **Watermark Stealing**&**Mixed Integer Programming**&**LLM Security**                 |
| 24.05 |                                                                     Huazhong University of Science and Technology                                                                      | Procedia Computer Science |                                              [No Free Lunch Theorem for Privacy-Preserving LLM Inference](https://arxiv.org/abs/2405.20681)                                              |                        **Privacy**&**LLM Inference**&**No Free Lunch Theory**                         |
| 24.05 |                                                                                       ETH Zurich                                                                                       |           arxiv           |                                                   [Black-Box Detection of Language Model Watermarks](https://arxiv.org/abs/2405.20777)                                                   |                             **Watermark Detection**&**Black-Box Testing**                             |
| 24.06 |                                                                          South China University of Technology                                                                          |           arxiv           |                      [PrivacyRestore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration](https://arxiv.org/abs/2406.01394)                       |                                 **Privacy-Preserving**&**Inference**                                  |
| 24.06 |                                                                               Carnegie Mellon University                                                                               |         ICML 2024         |                                   [PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs](https://arxiv.org/abs/2406.02958)                                    |                  **Federated Learning**&**Differential Privacy**&**Synthetic Data**                   |
| 24.06 |                                                                          University of California, Santa Cruz                                                                          |           arxiv           |                                           [Large Language Model Unlearning via Embedding-Corrupted Prompts](https://arxiv.org/abs/2406.07933)                                            |                            **Unlearning**&**Embedding-Corrupted Prompts**                             |
| 24.06 |                                                                            University of Technology Sydney                                                                             |           arxiv           |                                 [Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey](https://arxiv.org/abs/2406.07973)                                  |                               **Security Threats**&**Privacy Threats**                                |
| 24.06 |                                                                                    UC Santa Barbara                                                                                    |           arxiv           |                         [Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference](https://arxiv.org/abs/2406.08607)                          |                          **LLM Unlearning**&**Logit Difference**&**Privacy**                          |
| 24.06 |                                                                       Technion ‚Äì Israel Institute of Technology                                                                        |           arxiv           |                          [REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space](https://arxiv.org/abs/2406.09325)                          |                               **Unlearning**&**Sensitive Information**                                |
| 24.06 |                                             University of Maryland, ELLIS Institute T√ºbingen, Max Planck Institute for Intelligent Systems                                             |           arxiv           |                                    [Be like a Goldfish, Don‚Äôt Memorize! Mitigating Memorization in Generative LLMs](https://arxiv.org/abs/2406.10209)                                    |                                  **Memorization**&**Goldfish Loss**                                   |
| 24.06 |                                                               McCombs School of Business, University of Texas at Austin                                                                |           arxiv           |                                          [PRISM: A Design Framework for Open-Source Foundation Model Safety](https://arxiv.org/abs/2406.10415)                                           |                         **PRISM**&**Open-Source**&**Foundation Model Safety**                         |
| 24.06 |                                                                             Zhejiang University, MIT, UCLA                                                                             |           arxiv           |                                          [MemDPT: Differential Privacy for Memory Efficient Language Models](https://arxiv.org/abs/2406.11087)                                           |               **MemDPT**&**Differential Privacy**&**Memory Efficient Language Models**                |
| 24.06 |                                                                             Zhejiang University, MIT, UCLA                                                                             |           arxiv           |                              [GOLDCOIN: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory](https://arxiv.org/abs/2406.11149)                               |                     **GOLDCOIN**&**Contextual Integrity Theory**&**Privacy Laws**                     |
| 24.06 |                                                                                      IBM Research                                                                                      |           arxiv           |                               [Split, Unlearn, Merge: Leveraging Data Attributes for More Effective Unlearning in LLMs](https://arxiv.org/abs/2406.11780)                                |                                  **SPUNGE**&**Unlearning**&**LLMs**                                   |
| 24.06 |                                                                        Ping An Technology (Shenzhen) Co., Ltd.                                                                         |           arxiv           |                                             [PFID: Privacy First Inference Delegation Framework for LLMs](https://arxiv.org/abs/2406.12238)                                              |                             **PFID**&**Privacy**&**Inference Delegation**                             |
| 24.06 |                                                                     Hong Kong University of Science and Technology                                                                     |           arxiv           |                             [PDSS: A Privacy-Preserving Framework for Step-by-Step Distillation of Large Language Models](https://arxiv.org/abs/2406.12403)                              |                                    **PDSS**&**Privacy-Preserving**                                    |
| 24.06 |                                                                            KAIST AI, Hyundai Motor Company                                                                             |           arxiv           |                        [Protecting Privacy Through Approximating Optimal Parameters for Sequence Unlearning in Language Models](https://arxiv.org/abs/2406.14091)                        |                 **Privacy Protection**&**Optimal Parameters**&**Sequence Unlearning**                 |
| 24.06 |                                                                                   Nanjing University                                                                                   |           arxiv           |                                    [The Fire Thief Is Also the Keeper: Balancing Usability and Privacy in Prompts](https://arxiv.org/abs/2406.14318)                                     |                      **Prompt Privacy**&**Anonymization**&**Privacy Protection**                      |
| 24.06 |                                                                      University of Massachusetts Amherst, Google                                                                       |           arxiv           |                                           [POSTMARK: A Robust Blackbox Watermark for Large Language Models](https://arxiv.org/abs/2406.14517)                                            |                     **Blackbox Watermark**&**Paraphrasing Attacks**&**Detection**                     |
| 24.06 |                                                                               Michigan State University                                                                                |           arxiv           |                            [Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG) via Pure Synthetic Data](https://arxiv.org/abs/2406.14773)                             |                   **Retrieval-Augmented Generation**&**Privacy**&**Synthetic Data**                   |
| 24.06 |                                                                                   Beihang University                                                                                   |           arxiv           |                              [Safely Learning with Private Data: A Federated Learning Framework for Large Language Model](https://arxiv.org/abs/2406.14898)                              |                                  **Federated Learning**&**Privacy**                                   |
| 24.06 |                                                                             University of Rome Tor Vergata                                                                             |           arxiv           |                                 [Enhancing Data Privacy in Large Language Models through Private Association Editing](https://arxiv.org/abs/2406.18221)                                  |                           **Data Privacy**&**Private Association Editing**                            |
| 24.07 |                                                                             Huawei Munich Research Center                                                                              |           arxiv           |                     [IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization](https://arxiv.org/abs/2407.02956)                      |                                  **Text Anonymization**&**Privacy**                                   |
| 24.07 |                                                                             Huawei Munich Research Center                                                                              |           arxiv           |                          [ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets](https://arxiv.org/abs/2407.02960)                           |                          **Inference**&**Proprietary LLMs**&**Private Data**                          |
| 24.07 |                                                                                  Texas A&M University                                                                                  |           arxiv           |                               [Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment](https://arxiv.org/abs/2407.06443)                                |                 **Membership Inference Attack**&**Preference Data**&**LLM Alignment**                 |
| 24.07 |                                                                                    Google Research                                                                                     |           arxiv           |                                        [Fine-Tuning Large Language Models with User-Level Differential Privacy](https://arxiv.org/abs/2407.07737)                                        |                          **User-Level Differential Privacy**&**Fine-Tuning**                          |
| 24.07 |                                                                                  Newcastle University                                                                                  |           arxiv           |                              [Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models](https://arxiv.org/abs/2407.08152)                               |       **Privacy-Preserving Deduplication**&**Federated Learning**&**Private Set Intersection**        |
| 24.07 |                                                                     Huazhong University of Science and Technology                                                                      |           arxiv           |                                                        [On the (In)Security of LLM App Stores](https://arxiv.org/abs/2407.08422)                                                         |                              **LLM App Stores**&**Security**&**Privacy**                              |
| 24.07 |                                                                                   Soochow University                                                                                   |           arxiv           |                                             [Learning to Refuse: Towards Mitigating Privacy Risks in LLMs](https://arxiv.org/abs/2407.10058)                                             |                               **Privacy Risks**&**Machine Unlearning**                                |
| 24.07 |                                                                The University of Texas Health Science Center at Houston                                                                |           arxiv           |                        [Robust Privacy Amidst Innovation with Large Language Models Through a Critical Assessment of the Risks](https://arxiv.org/abs/2407.16166)                        |                   **Text Generation**&**Privacy**&**Protected Health Information**                    |
| 24.07 |                                                                             Huawei Munich Research Center                                                                              |     ACL 2024 Workshop     |                            [PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding](https://arxiv.org/abs/2407.02943)                            |                         **PII Extraction**&**Data Privacy**&**LLM Security**                          |
| 24.07 |                                                                            University of Technology Sydney                                                                             |           arxiv           |                                      [The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies](https://arxiv.org/abs/2407.19354)                                       |                          **LLM Agent**&**Privacy Preservation**&**Defense**                           |
| 24.07 |                                                                                University of Notre Dame                                                                                |           arxiv           |                                                    [Machine Unlearning in Generative AI: A Survey](https://arxiv.org/abs/2407.20516)                                                     |                       **Generative Models**&**Trustworthy ML**&**Data Privacy**                       |
| 24.07 |                                                                                                                                                                                        |           arxiv           |                                 [Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens](https://arxiv.org/abs/2407.21248)                                 |                **Pre-training Data Detection**&**Surprising Tokens**&**Data Privacy**                 |
| 24.08 |                                                                                   Sichuan University                                                                                   |           arxiv           |                                     [HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection](https://arxiv.org/abs/2408.02927)                                      |                           **Tabular Data Synthesis**&**Privacy Protection**                           |
| 24.08 |                                                                                      UC Berkeley                                                                                       |           arxiv           |                                                          [MPC-Minimized Secure LLM Inference](https://arxiv.org/abs/2408.03561)                                                          |                  **Secure Multi-party Computation**&**Privacy-Preserving Inference**                  |
| 24.08 |                                                                              Sapienza University of Rome                                                                               |           arxiv           |                                [Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions](https://arxiv.org/abs/2408.05212)                                |                             **Privacy Attacks**&**Differential Privacy**                              |
| 24.08 |                                                                           New Jersey Institute of Technology                                                                           |           arxiv           |                              [Casper: Prompt Sanitization for Protecting User Privacy in Web-Based Large Language Models](https://arxiv.org/abs/2408.07004)                              |                               **Prompt Sanitization**&**User Privacy**                                |
| 24.08 |                                                                                   Xidian University                                                                                    |           arxiv           |                    [DePrompt: Desensitization and Evaluation of Personal Identifiable Information in Large Language Model Prompts](https://arxiv.org/abs/2408.08930)                     |        **Personal Identifiable Information**&**Prompt Desensitization**&**Privacy Protection**        |
| 24.08 |                                                                           Huawei Technologies Canada Co. Ltd                                                                           |           arxiv           |                             [Tracing Privacy Leakage of Language Models to Training Data via Adjusted Influence Functions](https://arxiv.org/abs/2408.10468)                             |                              **Privacy Leakage**&**Influence Functions**                              |
| 24.08 |                                                                              Chinese Academy of Sciences                                                                               |           arxiv           |       [Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models](https://arxiv.org/abs/2408.10682)       |              **Knowledge Unlearning**&**Adversarial Attacks**&**Unlearning Robustness**               |
| 24.08 |                                                                             Universit√§tsklinikum Erlangen                                                                              |           arxiv           |         [Fine-Tuning a Local LLaMA-3 Large Language Model for Automated Privacy-Preserving Physician Letter Generation in Radiation Oncology](https://arxiv.org/abs/2408.10715)          |                        **Radiation Oncology**&**Data Privacy**&**Fine-Tuning**                        |
| 24.08 |                                                                           University of California, Berkeley                                                                           |           arxiv           |                                               [LLM-PBE: Assessing Data Privacy in Large Language Models](https://arxiv.org/abs/2408.12787)                                               |                                     **Data Privacy**&**Toolkit**                                      |
| 24.08 |                                                                       Mitsubishi Electric Research Laboratories                                                                        |           arxiv           |                         [Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage](https://arxiv.org/abs/2408.17354)                          |                **Privacy Leakage**&**Model-Unlearning**&**Pretrained Language Models**                |
| 24.09 |                                                                                  Stanford University                                                                                   |           arxiv           |                                     [PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action](https://arxiv.org/abs/2409.00138)                                      |                        **Privacy Norm Awareness**&**Privacy Risk Evaluation**                         |
| 24.09 |                                                                                       ByteDance                                                                                        |           arxiv           |                         [How Privacy-Savvy Are Large Language Models? A Case Study on Compliance and Privacy Technical Review](https://arxiv.org/abs/2409.02375)                         |        **Privacy Compliance**&**Privacy Information Extraction**&**Technical Privacy Review**         |
| 24.09 |                                                                                          MIT                                                                                           |         COLM 2024         |                                                   [Unforgettable Generalization in Language Models](https://arxiv.org/abs/2409.02228)                                                    |                          **Unlearning**&**Generalization**&**Random Labels**                          |
| 24.09 |                                                                Anhui University of Technology, University of Cambridge                                                                 |           arxiv           |                              [On the Weaknesses of Backdoor-based Model Watermarks: An Information-theoretic Perspective](https://arxiv.org/abs/2409.06130)                              |                  **Model Watermarking**&**Backdoor Attacks**&**Information Theory**                   |
| 24.09 |                                                                                   Bilkent University                                                                                   |           arxiv           |                       [Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on Generated Data](https://arxiv.org/abs/2409.11423)                        |                       **Privacy Risks**&**PII**&**Membership Inference Attack**                       |
| 24.09 |                                                                            National University of Singapore                                                                            |           arxiv           |                                 [Context-Aware Membership Inference Attacks against Pre-trained Large Language Models](https://arxiv.org/abs/2409.13745)                                 |                         **Membership Inference Attack**&**Context-Awareness**                         |
| 24.09 |                                                                                George Mason University                                                                                 |           arxiv           |                                     [Unlocking Memorization in Large Language Models with Dynamic Soft Prompting](https://arxiv.org/abs/2409.13853)                                      |                              **Memorization**&**Dynamic Soft Prompting**                              |
| 24.09 |                                                                   CAS Key Lab of Network Data Science and Technology                                                                   |        EMNLP 2024         |                             [Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method](https://arxiv.org/abs/2409.14781)                              |          **Pretraining Data Detection**&**Divergence Calibration**&**Membership Inference**           |
| 24.09 |                                                                                  √âcole Polytechnique                                                                                   |           arxiv           |                                    [Predicting and Analyzing Memorization Within Fine-Tuned Large Language Models](https://arxiv.org/abs/2409.18858)                                     |                             **Memorization**&**Fine-tuning**&**Privacy**                              |
| 24.10 |                                                                           University of Southern California                                                                            |           arxiv           |                                          [ADAPTIVELY PRIVATE NEXT-TOKEN PREDICTION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2410.02016)                                           |                  **Differential Privacy**&**Next-Token Prediction**&**Adaptive DP**                   |
| 24.10 |                                                                                 University of Chicago                                                                                  |           arxiv           |                                                      [MITIGATING MEMORIZATION IN LANGUAGE MODELS](https://arxiv.org/abs/2410.02159)                                                      |                              **Memorization Mitigation**&**Unlearning**                               |
| 24.10 |                                                                                University of Groningen                                                                                 |           arxiv           |                                             [Undesirable Memorization in Large Language Models: A Survey](https://arxiv.org/abs/2410.02650)                                              |                                     **Memorization**&**Privacy**                                      |
| 24.10 |                                                                  University of California, Santa Barbara, AWS AI Lab                                                                   |           arxiv           |                                    [Detecting Training Data of Large Language Models via Expectation Maximization](https://arxiv.org/abs/2410.07582)                                     |                     **Membership Inference Attack**&**Expectation Maximization**                      |
| 24.10 |                                                                      Queen‚Äôs University, J.P. Morgan AI Research                                                                       |    EMNLP 2024 Findings    |                               [Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation](https://arxiv.org/abs/2410.02912)                                |                        **Differential Privacy**&**Adaptive Noise Allocation**                         |
| 24.10 |                                                       King Abdullah University of Science and Technology, Ruhr University Bochum                                                       |        EMNLP 2024         |                                              [Private Language Models via Truncated Laplacian Mechanism](https://arxiv.org/abs/2410.08027)                                               |             **Differential Privacy**&**Word Embedding**&**Truncated Laplacian Mechanism**             |
| 24.10 |                                                                   Purdue University, Georgia Institute of Technology                                                                   |           arxiv           |                                [Privately Learning from Graphs with Applications in Fine-tuning Large Language Models](https://arxiv.org/abs/2410.08299)                                 |                **Privacy-preserving learning**&**Graph learning**&**Fine-tuning LLMs**                |
| 24.10 |                                                                                Northeastern University                                                                                 |           arxiv           |            [Rescriber: Smaller-LLM-Powered User-Led Data Minimization for Navigating Privacy Trade-offs in LLM-Based Conversational Agents](https://arxiv.org/abs/2410.11876)            |                                          **Privacy**&**PII**                                          |
| 24.10 |                                            The Pennsylvania State University, University of California Los Angeles, University of Virginia                                             |           arxiv           |                                    [Data-adaptive Differentially Private Prompt Synthesis for In-Context Learning](https://arxiv.org/abs/2410.12085)                                     |            **Differential privacy**&**In-context learning**&**Synthetic data generation**             |
| 24.10 | Nanjing University of Science and Technology, Western Sydney University, Institute of Information Engineering (Chinese Academy of Sciences), CSIRO‚Äôs Data61, The University of Chicago |           arxiv           |                                 [Reconstruction of Differentially Private Text Sanitization via Large Language Models](https://arxiv.org/abs/2410.12443)                                 |                 **Differential Privacy**&**Reconstruction attacks**&**Privacy risks**                 |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles
| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.11 | News  |Wild: GPT-3.5 leaked a random dude's photo in the output. | [link](https://twitter.com/thealexker/status/1719896871009694057) |

## üßë‚Äçüè´Scholars
