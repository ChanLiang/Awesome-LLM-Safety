# Datasets & Benchmark


## ðŸ“‘Papers

| Date  |                          Institute                           |       Publication       |                            Paper                             |                           Keywords                         |
| :---: | :----------------------------------------------------------: | :---------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 20.09 |                   University of Washington                   |   EMNLP2020(findings)   | [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462) |                         **Toxicity**                       |
| 21.09 |                     University of Oxford                     |         ACL2022         | [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958) |                       **Truthfulness**                     |
| 22.03 |                             MIT                              |         ACL2022         | [ToxiGen: A Large-Scale Machine-Generated datasets for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509) |                         **Toxicity**                       |
| 23.07 | Zhejiang University; School of Engineering Westlake University |          arxiv          | [Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models](https://arxiv.org/abs/2307.08487) |        **Text Safety**&**Benchmark**&**Jailbreaking**      |
| 23.07 |               Stevens Institute of Technology                |   NAACL2024(findings)   | [HateModerate: Testing Hate Speech Detectors against Content Moderation Policies](https://arxiv.org/abs/2307.12418) | **Hate Speech Detection**&**Content Moderation**&**Machine Learning** |
| 23.08 |                      Meta Reality Labs                       |        NAACL2024        | [Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)?](https://arxiv.org/abs/2308.10168) | **Large Language Models**&**Knowledge Graphs**&**Question Answering** |
| 23.08 |                      Bocconi University                      |        NAACL2024        | [XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models](https://arxiv.org/abs/2308.01263) | **Large Language Models**&**Safety Behaviours**&**Test Suite** |
| 23.09 |         LibrAI, MBZUAI, The University of Melbourne          |          arxiv          | [Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs](https://arxiv.org/abs/2308.13387) |             **Safety Evaluation**&**Safeguards**           |
| 23.10 |    University of Edinburgh, Huawei Technologies Co., Ltd.    |        NAACL2024        | [Assessing the Reliability of Large Language Model Knowledge](https://arxiv.org/abs/2310.09820) | **Large Language Models**&**Factual Knowledge**&**Knowledge Probing** |
| 23.10 |                  University of Pennsylvania                  |   NAACL2024(findings)   | [Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks](https://arxiv.org/abs/2310.12516) | **Hallucination Assessment**&**Adversarial Attacks**&**Large Language Models** |
| 23.11 |                       Fudan University                       |          arxiv          | [JADE: A Linguistic-based Safety Evaluation Platform for LLM](https://arxiv.org/abs/2311.00286) |                    **Safety Benchmarks**                   |
| 23.11 |                       UNC-Chapel Hill                        |          arxiv          | [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287) |        **Hallucination**&**Benchmark**&**Multimodal**      |
| 23.11 |                       IBM Research AI                        | EMNLP2023(GEM workshop) | [Unveiling Safety Vulnerabilities of Large Language Models](https://arxiv.org/abs/2311.04124) | **Adversarial Examples**&**Clustering**&**Automatically Identifying** |
| 23.11 |      The Hong Kong University of Science and Technology      |          arxiv          | [P-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models](https://arxiv.org/abs/2311.04044) |       **Differential Privacy**&**Privacy Evaluation**      |
| 23.11 |                         UC Berkeley                          |          arxiv          | [CAN LLMS FOLLOW SIMPLE RULES](https://arxiv.org/abs/2311.04235) |             **Evaluation**&**Attack Strategies**           |
| 23.11 |                University of Central Florida                 |          arxiv          | [THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech](https://arxiv.org/abs/2311.06446) |       **Hate Speech**&**Offensive Speech**&**Dataset**     |
| 23.11 | Beijing Jiaotong University; DAMO Academy, Alibaba Group, Peng Cheng Lab |          arXiv          | [AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation](https://arxiv.org/abs/2311.07397) |  Multi-modal Large Language Models&Hallucination&Benchmark |
| 23.11 |    Patronus AI, University of Oxford, Bocconi University     |          arxiv          | [SIMPLESAFETYTESTS: a Test Suite for Identifying Critical Safety Risks in Large Language Models](https://arxiv.org/abs/2311.08370) |        **Safety Risks**&**Test Suite**&**Evaluation**      |
| 23.11 | University of Southern California, University of Pennsylvania, University of California Davis |          arxiv          | [Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702) |  **Hallucinations**&**Semantic Associations**&**Benchmark** |
| 23.11 | Seoul National University, Chung-Ang University, NAVER AI Lab, NAVER Cloud, University of Richmond |          arxiv          | [LifeTox: Unveiling Implicit Toxicity in Life Advice](https://arxiv.org/abs/2311.09585) | **LifeTox Dataset**&**Toxicity Detection**&**Social Media Analysis** |
| 23.11 |       School of Information Renmin University of China       |          arxiv          | [UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://arxiv.org/abs/2311.15296) |          **Hallucination**&**Evaluation Benchmark**        |
| 23.11 |                UC Santa Cruz, UNC-Chapel Hill                |          arxiv          | [How Many Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.16101) | **Vision Large Language Models**&**Safety Evaluation**&A**dversarial Robustness** |
| 23.11 | Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences; Baidu Inc. |          arxiv          | [FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality Fairness Toxicity](https://arxiv.org/abs/2311.18580) |                 **Harmlessness Evaluation**                |
| 23.11 | Fudan University&Shanghai Artificial Intelligence Laboratory |        NAACL2024        | [Fake Alignment: Are LLMs Really Aligned Well?](https://arxiv.org/abs/2311.05915) | **Large Language Models**&**Safety Evaluation**&**Fake Alignment** |
| 23.11 |                 Kahlert School of Computing                  |        NAACL2024        | [Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness](https://arxiv.org/abs/2311.09694) | **NLP Robustness**&**Out-of-Domain Evaluation**&**Adversarial Evaluation** |
| 23.11 |                Shanghai Jiao Tong University                 |   NAACL2024(findings)   | [CLEANâ€“EVAL: Clean Evaluation on Contaminated Large Language Models](https://arxiv.org/abs/2311.09154) | **Clean Evaluation**&**Data Contamination**&**Large Language Models** |
| 23.12 |                             Meta                             |          arxiv          | [Purple Llama CYBERSECEVAL: A Secure Coding Benchmark for Language Models](https://arxiv.org/abs/2312.04724) |   **Safety**&**Cybersecurity**&**Code Security Benchmark** |
| 23.12 | University of Illinois Chicago, Bosch Research North America & Bosch Center for Artificial Intelligence (BCAI), UNC Chapel-Hill |          arxiv          | [DELUCIONQA: Detecting Hallucinations in Domain-specific Question Answering](https://arxiv.org/abs/2312.05200) | **Hallucination Detection**&**Domain-specific QA**&**Retrieval-augmented LLMs** |
| 23.12 | University of Science and Technology of China, Hong Kong University of Science and Technology, Microsoft |          arxiv          | [Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models](https://arxiv.org/abs/2312.14197) | **Indirect Prompt Injection Attacks**&**BIPIA Benchmark**&**Defense** |
| 24.01 |      NewsBreak, University of Illinois Urbana-Champaign      |          arxiv          | [RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models](https://arxiv.org/abs/2401.00396) | **Retrieval-Augmented Generation**&**Hallucination Detection**&**Dataset** |
| 24.01 | Lehigh University, Illinois Institute of Technology, Institut Polytechnique de Paris, William & Mary, Texas A&M University, University of Georgia, Samsung Research America, Stanford University(Major contribution) |          arxiv          | [TRUSTLLM: TRUSTWORTHINESS IN LARGE LANGUAGE MODELS](https://arxiv.org/abs/2401.05561) |         **Trustworthiness**&**Benchmark Evaluation**       |
| 24.01 |                  University College London                   |          arxiv          | [Hallucination Benchmark in Medical Visual Question Answering](https://arxiv.org/abs/2401.05827) | **Medical Visual Question Answering**&**Hallucination Benchmark** |
| 24.01 |                  Carnegie Mellon University                  |          arxiv          | [TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121) |     **Data Privacy**&**Ethical Concerns**&**Unlearning**   |
| 24.01 |     IRLab CITIC Research Centre, Universidade da CoruÃ±a      |          arxiv          | [MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection](https://arxiv.org/abs/2401.06526) |          **Hate Speech Detection**&**Social Media**        |
| 24.01 | Northwestern University, New York University, University of Liverpool, Rutgers University |          arxiv          | [AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models](https://arxiv.org/abs/2401.09002) | **Jailbreak Attack**&**Evaluation Frameworks**&**Ground Truth Dataset** |
| 24.01 |                Shanghai Jiao Tong University                 |          arxiv          | [R-Judge: Benchmarking Safety Risk Awareness for LLM Agents](https://arxiv.org/abs/2401.10019) |    **LLM Agents**&**Safety Risk Awareness**&**Benchmark**  |
| 24.02 | University of Illinois Urbana-Champaign, Center for AI Safety, Carnegie Mellon University, UC Berkeley, Microsoft |          arxiv          | [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249) |         **Automated Red Teaming**&**Robust Refusal**       |
| 24.02 | Shanghai Artificial Intelligence Laboratory, Harbin Institute of Technology, Beijing Institute of Technology, Chinese University of Hong Kong |          arxiv          | [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044) | **Safety Benchmark**&Safety Evaluation**&**Hierarchical Taxonomy** |
| 24.02 |               Middle East Technical University               |          arxiv          | [HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs](https://arxiv.org/abs/2402.16211) |          **Hallucination**&**Benchmarking Dataset**        |
| 24.02 |           Indian Institute of Technology Kharagpur           |          arxiv          | [How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries](https://arxiv.org/abs/2402.15302) | **Instruction-centric Responses**&**Ethical Vulnerabilities** |
| 24.03 |                 East China Normal University                 |          arxiv          | [DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2403.00896) | **Dialogue-level Hallucination**&**Benchmarking**&**Human-machine Interaction** |
| 24.03 | Tianjin University, Tianjin University, Zhengzhou University, China Academy of Information and Communications Technology |          arxiv          | [OpenEval: Benchmarking Chinese LLMs across Capability, Alignment, and Safety](https://arxiv.org/abs/2403.12316) |         **Chinese LLMs**&**Benchmarking**&**Safety**       |
| 24.04 |    University of Pennsylvania, ETH Zurich, EPFL, Sony AI     |          arxiv          | [JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models](https://arxiv.org/abs/2404.01318) |      **Jailbreaking Attacks**&**Robustness Benchmark**     |
| 24.04 | Vector Institute for Artificial Intelligence, University of Limerick |          arxiv          | [Developing Safe and Responsible Large Language Models - A Comprehensive Framework](https://arxiv.org/abs/2404.01399) |      **Responsible AI**&**AI Safety**&**Generative AI**    |
| 24.04 | LMU Munich, University of Oxford, Siemens AG, Munich Center for Machine Learning (MCML), Wuhan University |          arxiv          | [RED TEAMING GPT-4V: ARE GPT-4V SAFE AGAINST UNI/MULTI-MODAL JAILBREAK ATTACKS?](https://arxiv.org/abs/2404.03411) | **Jailbreak Attacks**&**GPT-4V**&**Evaluation Benchmark**&**Robustness** |
| 24.04 |           Bocconi University, University of Oxford           |          arxiv          | [SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety](https://arxiv.org/abs/2404.05399) |    **LLM Safety**&**Open Datasets**&**Systematic Review**  |
| 24.04 |        University of Alberta&The University of Tokyo         |          arxiv          | [Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward](https://arxiv.org/abs/2404.08517) |   **LLM Safety**&**Online Safety Analysis**&**Benchmark**  |
| 24.04 |  Technion â€“ Israel Institute of Technology, Google Research  |          arxiv          | [Constructing Benchmarks and Interventions for Combating Hallucinations in LLMs](https://arxiv.org/abs/2404.09971) |              **Hallucinations**&**Benchmarks**             |
| 24.05 |                  Carnegie Mellon University                  |          arxiv          | [PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models](https://arxiv.org/abs/2405.09373) |           **Multilingual Evaluation**&**Datasets*          |
| 24.05 | Paul G. Allen School of Computer Science & Engineering | arxiv | [MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection](https://arxiv.org/abs/2405.19285) | **Hallucination Detection**&**Multilingual AMR**&**Dataset** |
| 24.05 | University of California, Riverside | arxiv | [Cross-Task Defense: Instruction-Tuning LLMs for Content Safety](https://arxiv.org/abs/2405.15202) | **Instruction-Tuning**&**LLM Safety**&**Content Safety** |
| 24.06 | University of Waterloo | arxiv | [TruthEval: A Dataset to Evaluate LLM Truthfulness and Reliability](https://arxiv.org/abs/2406.01855) | **Truthfulness**&**Reliability** |
| 24.06 | Rutgers University | arxiv | [MoralBench: Moral Evaluation of LLMs](https://arxiv.org/abs/2406.04428) | **Moral Evaluation**&**MoralBench** |
| 24.06 | Tsinghua University | arxiv | [Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study](https://arxiv.org/abs/2406.07057) | **Trustworthiness**&**MLLMs**&**Benchmark** |
| 24.06 | Beijing Academy of Artificial Intelligence | arxiv | [HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level Hallucination Evaluation](https://arxiv.org/abs/2406.07070) | **Hallucination Evaluation**&**Dialogue-Level**&**HalluDial** |
| 24.06 | Sichuan University | arxiv | [LEGEND: Leveraging Representation Engineering to Annotate Safety Margin for Preference Datasets](https://arxiv.org/abs/2406.08124) | **Safety Margin**&**Preference Datasets**&**Representation Engineering** |
| 24.06 | The Hong Kong University of Science and Technology (Guangzhou) | arxiv | [Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2406.09324) | **Jailbreak Attacks**&**Benchmarking** |
| 24.06 | AI Innovation Center, China Unicom | arxiv | [CHiSafetyBench: A Chinese Hierarchical Safety Benchmark for Large Language Models](https://arxiv.org/abs/2406.10311) | **Chinese Hierarchical Safety Benchmark**&**Large Language Models**&**Automatic Evaluation** |
| 24.06 | Google | arxiv | [Supporting Human Raters with the Detection of Harmful Content using Large Language Models](https://arxiv.org/abs/2406.12800) | **Harmful Content Detection**&**Hate Speech** |
| 24.06 | South China University of Technology, Pazhou Laboratory, University of Maryland, Baltimore County | arxiv | [GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2406.13925) | **Gender Bias Mitigation**&**Alignment Dataset**&**Bias Categories** |
| 24.06 | Center for AI Safety and Governance, Institute for AI, Peking University | arxiv | [SAFESORA: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset](https://arxiv.org/abs/2406.14477) | **Safety Alignment**&**Text2Video Generation** |

## ðŸ“šResource

- Toxicity - [RealToxicityPrompts datasets](https://toxicdegeneration.allenai.org/)
- Truthfulness - [TruthfulQA datasets](https://github.com/sylinrl/TruthfulQA)
- TRUSTLLM - [TRUSTLLM](https://trustllmbenchmark.github.io/TrustLLM-Website/)