# Datasets & Benchmark


## ðŸ“‘Papers

| Date  |                                             Institute                                              |       Publication       |                                                                  Paper                                                                   |                               Keywords                                |
|:-----:|:--------------------------------------------------------------------------------------------------:|:-----------------------:|:----------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------:|
| 20.09 |                                      University of Washington                                      |   EMNLP2020(findings)   |             [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)             |                             **Toxicity**                              |
| 21.09 |                                        University of Oxford                                        |         ACL2022         |                       [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                        |                           **Truthfulness**                            |
| 22.03 |                                                MIT                                                 |         ACL2022         | [ToxiGen: A Large-Scale Machine-Generated datasets for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509) |                             **Toxicity**                              |
| 23.11 |                                          Fudan University                                          |          arxiv          |                     [JADE: A Linguistic-based Safety Evaluation Platform for LLM](https://arxiv.org/abs/2311.00286)                      |                         **Safety Benchmarks**                         |
| 23.11 |                                          UNC-Chapel Hill                                           |          arxiv          |        [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)         |            **Hallucination**&**Benchmark**&**Multimodal**             |
| 23.11 |                                          IBM Research AI                                           | EMNLP2023(GEM workshop) |                      [Unveiling Safety Vulnerabilities of Large Language Models](https://arxiv.org/abs/2311.04124)                       | **Adversarial Examples**&**Clustering**&**Automatically Identifying** |
| 23.11 |                         The Hong Kong University of Science and Technology                         |          arxiv          |               [P-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models](https://arxiv.org/abs/2311.04044)                |            **Differential Privacy**&**Privacy Evaluation**            |
| 23.11 |                                            UC Berkeley                                             |          arxiv          |                                     [CAN LLMS FOLLOW SIMPLE RULES](https://arxiv.org/abs/2311.04235)                                     |                 **Evaluation**&**Attack Strategies**                  |
| 23.11 |                                   University of Central Florida                                    |          arxiv          |                   [THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech](https://arxiv.org/abs/2311.06446)                   |           **Hate Speech**&**Offensive Speech**&**Dataset**            |
| 23.11 |              Beijing Jiaotong University; DAMO Academy, Alibaba Group, Peng Cheng Lab              |          arXiv          |          [AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation](https://arxiv.org/abs/2311.07397)           |       Multi-modal Large Language Models&Hallucination&Benchmark       |
| 23.11 |                       Patronus AI, University of Oxford, Bocconi University                        |          arxiv          |    [SIMPLESAFETYTESTS: a Test Suite for Identifying Critical Safety Risks in Large Language Models](https://arxiv.org/abs/2311.08370)    |            **Safety Risks**&**Test Suite**&**Evaluation**             |
| 23.11 |   University of Southern California, University of Pennsylvania, University of California Davis    |          arxiv          |    [Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702)    |      **Hallucinations**&**Semantic Associations**&**Benchmark**       |
| 23.11 | Seoul National University, Chung-Ang University, NAVER AI Lab, NAVER Cloud, University of Richmond |          arxiv          |                         [LifeTox: Unveiling Implicit Toxicity in Life Advice](https://arxiv.org/abs/2311.09585)                          | **LifeTox Dataset**&**Toxicity Detection**&**Social Media Analysis**  |



## ðŸ“šResource

- Toxicity - [RealToxicityPrompts datasets](https://toxicdegeneration.allenai.org/)
- Truthfulness - [TruthfulQA datasets](https://github.com/sylinrl/TruthfulQA)