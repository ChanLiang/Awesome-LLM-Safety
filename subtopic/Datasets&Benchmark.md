# Datasets & Benchmark


## ðŸ“‘Papers

| Date  |        Institute         |     Publication     |                                                                  Paper                                                                   |                    Keywords                    |
|:-----:|:------------------------:|:-------------------:|:----------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------:|
| 20.09 | University of Washington | EMNLP2020(findings) |             [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)             |                  **Toxicity**                  |
| 21.09 |   University of Oxford   |       ACL2022       |                       [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                        |                **Truthfulness**                |
| 22.03 |           MIT            |       ACL2022       | [ToxiGen: A Large-Scale Machine-Generated datasets for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509) |                  **Toxicity**                  |
| 23.11 |     Fudan University     |        arxiv        |                     [JADE: A Linguistic-based Safety Evaluation Platform for LLM](https://arxiv.org/abs/2311.00286)                      |             **Safety Benchmarks**              |
| 23.11 |     UNC-Chapel Hill      |        arxiv        |        [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)         | **Hallucination**&**Benchmark**&**Multimodal** |




## ðŸ“šResource

- Toxicity - [RealToxicityPrompts datasets](https://toxicdegeneration.allenai.org/)
- Truthfulness - [TruthfulQA datasets](https://github.com/sylinrl/TruthfulQA)