# Datasets & Benchmark


## ðŸ“‘Papers

| Date  |        Institute         |     Publication     |                                                                  Paper                                                                   |       Keywords        |
|:-----:|:------------------------:|:-------------------:|:----------------------------------------------------------------------------------------------------------------------------------------:|:---------------------:|
| 20.09 | University of Washington | EMNLP2020(findings) |             [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)             |     **Toxicity**      |
| 21.09 |   University of Oxford   |       ACL2022       |                       [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                        |   **Truthfulness**    |
| 22.03 |           MIT            |       ACL2022       | [ToxiGen: A Large-Scale Machine-Generated datasets for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509) |     **Toxicity**      |
| 23.11 |                          |        arxiv        |                     [JADE: A Linguistic-based Safety Evaluation Platform for LLM](https://arxiv.org/abs/2311.00286)                      | **Safety Benchmarks** |

## ðŸ“šResource

- Toxicity - [RealToxicityPrompts datasets](https://toxicdegeneration.allenai.org/)
- Truthfulness - [TruthfulQA datasets](https://github.com/sylinrl/TruthfulQA)