# Robustness

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                          Institute                           |                         Publication                          |                            Paper                             |                           Keywords                           |
| :---: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 23.02 |                      Microsoft Research                      | ICLR 2023(workshop on Trustworthy and Reliable Large-Scale Machine Learning Models) | [On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective](https://arxiv.org/abs/2302.12095) | **Robustness Evaluation**&**Adversarial Robustness**&**Out-of-Distribution (OOD)** |
| 23.05 |                Harbin Institute of Technology                |                          NAACL2024                           | [Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting](https://arxiv.org/abs/2305.13733) | **Large Language Models**&**Inductive Instructions**&**Dual-critique Prompting** |
| 23.05 | National Key Laboratory for Multimedia Information Processing |                          NAACL2024                           | [DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade](https://arxiv.org/abs/2305.14751) | **Natural Language Understanding**&**Dialogue System**&**Multi-label Classification** |
| 23.06 |          University of Illinois at Urbana-Champaign          |                            arxiv                             | [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698) |      **Robustness**&**Ethics**&**Privacy**&**Toxicity**      |
| 23.08 | CISPA Helmholtz Center for Information Security & Tsinghua University |                            arxiv                             | [Robustness Over Time: Understanding Adversarial Examples‚Äô Effectiveness on Longitudinal Versions of Large Language Models](https://arxiv.org/abs/2308.07847) |       **Longitudinal Study**&**Robustness Assessment**       |
| 23.11 |                             CMU                              |               AACL2023(ART or Safety workshop)               | [Measuring Adversarial Datasets](https://arxiv.org/abs/2311.03566) | **Adversarial Robustness**&**AI Safety**&**Adversarial Datasets** |
| 23.11 |                      Amazon Alexa AI-NU                      |                            arXiv                             | [JAB: Joint Adversarial Prompting and Belief Augmentation](https://arxiv.org/abs/2311.09473) | **Adversarial Prompting**&T**oxicity Reduction**&**Robustness** |
| 23.11 |                      Amazon Alexa AI-NU                      |                          NAACL2024                           | [JAB: Joint Adversarial Prompting and Belief Augmentation](https://arxiv.org/abs/2311.09473) | **Adversarial Prompting**&T**oxicity Reduction**&**Robustness** |
| 23.11 |                  Michigan State University                   |                     NAACL2024(findings)                      | [A Robust Semantics-based Watermark for Large Language Models against Paraphrasing](https://arxiv.org/abs/2311.08721) |   **Watermark**&**Large Language Models**&**Paraphrasing**   |
| 24.01 | University of Trento, Concordia University, Mila-Quebec AI Institute |                            arxiv                             | [Are LLMs Robust for Spoken Dialogues?](https://arxiv.org/abs/2401.02297) | **Task-Oriented Dialogues**&**Automatic Speech Recognition**&**Error Analysis** |
| 24.01 |       School of Computer Science University of Windsor       |                            arxiv                             | [Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing Approach For Uncovering Edge Cases with Minimal Distribution Distortion](https://arxiv.org/abs/2401.11373) | **Adversarial Attacks**&**Targeted Paraphrasing**&**Reinforcement Learning** |
| 24.02 |                   University of Cambridge                    |                            arxiv                             | [Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment](https://arxiv.org/abs/2402.14016) |     **LLM as a Judge**&**Universal Adversarial Attacks**     |
| 24.02 |                Technical University of Munich                |                            arxiv                             | [Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images](https://arxiv.org/abs/2402.14899) | **Multimodal Large Language Models**&**Chain-of-Thought Reasoning**&**Adversarial Images** |
| 24.03 | Beijing Institute of Technology, The University of Sydney, Hong Kong Baptist University |                            arxiv                             | [Few-Shot Adversarial Prompt Learning on Vision-Language Models](https://arxiv.org/abs/2403.14774) | **Few-Shot Learning**&**Adversarial Prompt**&**Vision-Language Models** |
| 24.03 |                             UIUC                             |                          NAACL2024                           | [Fact Checking Beyond Training Set](https://arxiv.org/abs/2403.18671) | **Fact Checking**&**Domain Adaptation**&**Adversarial Training** |
| 24.03 | Institute of Data Science, National University of Singapore  |                          NAACL2024                           | [SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks](https://arxiv.org/abs/2403.18423) | **Adversarial Training**&**Word-Level Attacks**&**Robust Representations** |
| 24.03 |                   Georgia State University                   |                     NAACL2024(findings)                      | [RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning](https://arxiv.org/abs/2403.11082) | **Sentence Embeddings**&**Adversarial Learning**&**Contrastive Learning** |
| 24.04 | University of Chinese Academy of Sciences, Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences |                         COLING 2024                          | [Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack](https://arxiv.org/abs/2404.01907) | **Adversarial Attack**&**AI-Text Detection**&**Dynamic Adversarial Learning** |
| 24.04 | Institute for Intelligent Computing, Alibaba Group; School of Information Management, Wuhan University |                            arxiv                             | [Enhance Robustness of Language Models Against Variation Attack through Graph Integration](https://arxiv.org/abs/2404.12014) |     **Chinese Adversarial Attacks**&**Variation Graph**      |



## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars