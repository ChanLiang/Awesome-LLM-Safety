# Jailbreaks&Attack

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                                                   Institute                                                   |          Publication          |                                                                         Paper                                                                         |                                                Keywords                                                |
|:-----:|:-------------------------------------------------------------------------------------------------------------:|:-----------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------:|
| 20.12 |                                                    Google                                                     |     USENIX Security 2021      |       [Extracting Training Data from Large Language Models](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)       |                            **Verbatim Text Sequences**&**Rank Likelihood**                             |
| 22.11 |                                                   AE Studio                                                   | NIPS2022(ML Safety Workshop)  |                           [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527)                           |                                  **Prompt Injection**&**Misaligned**                                   |
| 23.02 |                                              Saarland University                                              |             arxiv             | [Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173) |        **Adversarial Prompting**&**Indirect Prompt Injection**&**LLM-Integrated Applications**         |
| 23.04 |                                Hong Kong University of Science and Technology                                 |      EMNLP2023(findings)      |                                [Multi-step Jailbreaking Privacy Attacks on ChatGPT](https://arxiv.org/abs/2304.05197)                                 |                                       **Privacy**&**Jailbreaks**                                       |
| 23.06 |                                                    Google                                                     |             arxiv             |                                [Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)                                 |                                      **Multimodal**&**Jailbreak**                                      |
| 23.07 |                                                      CMU                                                      |             arxiv             |                     [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)                     |                      **Jailbreak**&**Transferable Attack**&**Adversarial Attack**                      |
| 23.07 |                                       Nanyang Technological University                                        |           NDSS2023            |                   [MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots](https://arxiv.org/abs/2307.08715)                    |                     **Jailbreak**&**Reverse-Engineering**&**Automatic Generation**                     |
| 23.07 |                                 UNC Chapel Hill, Google DeepMind, ETH Zurich                                  | AdvML Frontiers Workshop 2023 |                           [Backdoor Attacks for In-Context Learning with Language Models](https://arxiv.org/abs/2307.14692)                           |                              **Backdoor Attacks**&**In-Context Learning**                              |
| 23.10 |                    Princeton University, Virginia Tech, IBM Research, Stanford University                     |             arxiv             |             [FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY EVEN WHEN USERS DO NOT INTEND TO!](https://arxiv.org/abs/2310.03693)              |                        **Fine-tuning****Safety Risks**&**Adversarial Training**                        |
| 23.10 |               University of California Santa Barbara, Fudan University, Shanghai AI Laboratory                |             arxiv             |                      [SHADOW ALIGNMENT: THE EASE OF SUBVERTING SAFELY-ALIGNED LANGUAGE MODELS](https://arxiv.org/abs/2310.02949)                      |                            **AI Safety**&**Malicious Use**&**Fine-tuning**                             |
| 23.11 |                                                    MBZUAI                                                     |             arxiv             |                [Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks](https://arxiv.org/abs/2311.00508)                |               **Adversarially-synthesized Texts**&**Word-level Attacks**&**Evaluation**                |
| 23.11 |                                               Palisade Research                                               |             arxiv             |                        [BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B](https://arxiv.org/abs/2311.00117)                        |                                     **Remove Safety Fine-tuning**                                      |
| 23.11 |                                             University of Twente                                              |          ICNLSP 2023          |                         [Efficient Black-Box Adversarial Attacks on Neural Text Detectors](https://arxiv.org/abs/2311.01873)                          |                             **Misclassification**&**Adversarial attacks**                              |
| 23.11 |                                PRISM AI&Harmony Intelligenc&Leap Laboratories                                 |             arxiv             |            [Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation](https://arxiv.org/abs/2311.03348 )             |                   **Persona-modulation Attacks**&**Jailbreaks**&**Automated Prompt**                   |
| 23.11 |                                              Tsinghua University                                              |             arxiv             |                     [Jailbreaking Large Vision-Language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)                      |                      **Typographic Attack**&**Multi-modal**&**Safety Evaluation**                      |
| 23.11 |                      Huazhong University of Science and Technology, Tsinghua University                       |             arxiv             |    [Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration](https://arxiv.org/abs/2311.06062)    |                       **Membership Inference Attacks**&**Privacy and Security**                        |
| 23.11 |                                        Nanjing University, Meituan Inc                                        |             arxiv             |      [A Wolf in Sheep‚Äôs Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://arxiv.org/abs/2311.08268)       |                 **Jailbreak Prompts**&**Safety Alignment**&**Safeguard Effectiveness**                 |
| 23.11 |                                                Google DeepMind                                                |             arxiv             | [Frontier Language Models Are Not Robust to Adversarial Arithmetic or "What Do I Need To Say So You Agree 2+2=5?"](https://arxiv.org/abs/2311.07587)  |                **Adversarial Arithmetic**&**Model Robustness**&**Adversarial Attacks**                 |
| 23.11 |                             University of Illinois Chicago, Texas A&M University                              |             arxiv             |           [DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Pre-trained Language Models](https://arxiv.org/abs/2311.08598)            |                  **Adversarial Attack**&**Distribution-Aware**&**LoRA-Based Attack**                   |
| 23.11 |                                       Illinois Institute of Technology                                        |             arxiv             |      [Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment](https://arxiv.org/abs/2311.09433)      | Backdoor Activation Attack&Large Language Models&AI Safety&Activation Steering&Trojan Steering Vectors |
| 23.11 |                                            Wayne State University                                             |             arXiv             |                        [Hijacking Large Language Models via Adversarial In-Context Learning](https://arxiv.org/abs/2311.09948)                        |           **Adversarial Attacks**&**Gradient-Based Prompt Search**&**Adversarial Suffixes**            |
| 23.11 | Hong Kong Baptist University, Shanghai Jiao Tong University, Shanghai AI Laboratory, The University of Sydney |             arXiv             |                          [DeepInception: Hypnotize Large Language Model to Be Jailbreaker](https://arxiv.org/abs/2311.03191)                          |                                    **Jailbreak**&**DeepInception**                                     |
| 23.11 |                                      Xi‚Äôan Jiaotong-Liverpool University                                      |             arxiv             |                   [Generating Valid and Natural Adversarial Examples with Large Language Models](https://arxiv.org/abs/2311.11861)                    |                            **Adversarial examples**&**Text classification**                            |
| 23.11 |                                           Michigan State University                                           |             arxiv             |                  [Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems](https://arxiv.org/abs/2311.11796)                  |                    **Transferable Attacks**&**AI Systems**&**Adversarial Attacks**                     |
| 23.11 |                                   Tsinghua University & Kuaishou Technology                                   |             arxiv             |                           [Evil Geniuses: Delving into the Safety of LLM-based Agents](https://arxiv.org/abs/2311.11855v1)                            |                         **LLM-based Agents**&**Safety**&**Malicious Attacks**                          |
| 23.11 |                                                  ETH Zurich                                                   |             arxiv             |                            [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455)                             |                                     **RLHF**&**Backdoor Attacks**                                      |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |        Type        |                                       Title                                       |                         URL                          |
|:-----:|:------------------:|:---------------------------------------------------------------------------------:|:----------------------------------------------------:|
| 23.01 |     Community      |                              Reddit/ChatGPTJailbrek                               |  [link](https://www.reddit.com/r/ChatGPTJailbreak)   |
| 23.02 | Resource&Tutorials |                                  Jailbreak Chat                                   |        [link](https://www.jailbreakchat.com/)        |
| 23.10 |     Tutorials      |                                Awesome-LLM-Safety                                 | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |
| 23.11 |       Video        | [1hr Talk] Intro to Large Language Models<br/>From 45:45(Author: Andrej Karpathy) | [link](https://www.youtube.com/watch?v=zjkBMFhNj_g)  |

## üì∞News & Articles

| Date  |  Type   |            Title            |   Author    |                                  URL                                  |
|:-----:|:-------:|:---------------------------:|:-----------:|:---------------------------------------------------------------------:|
| 23.10 | Article | Adversarial Attacks on LLMs | Lilian Weng | [link](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/) |


## üßë‚Äçüè´Scholars