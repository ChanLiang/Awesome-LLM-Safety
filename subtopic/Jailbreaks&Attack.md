# Jailbreaks&Attack

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                             Institute                              |         Publication          |                                                                         Paper                                                                         |                                        Keywords                                         |
|:-----:|:------------------------------------------------------------------:|:----------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------:|
| 20.12 |                               Google                               |     USENIX Security 2021     |       [Extracting Training Data from Large Language Models](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)       |                     **Verbatim Text Sequences**&**Rank Likelihood**                     |
| 22.11 |                             AE Studio                              | NIPS2022(ML Safety Workshop) |                           [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527)                           |                           **Prompt Injection**&**Misaligned**                           |
| 23.02 |                        Saarland University                         |            arxiv             | [Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173) | **Adversarial Prompting**&**Indirect Prompt Injection**&**LLM-Integrated Applications** |
| 23.04 |           Hong Kong University of Science and Technology           |     EMNLP2023(findings)      |                                [Multi-step Jailbreaking Privacy Attacks on ChatGPT](https://arxiv.org/abs/2304.05197)                                 |                               **Privacy**&**Jailbreaks**                                |
| 23.06 |                               Google                               |            arxiv             |                                [Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)                                 |                              **Multimodal**&**Jailbreak**                               |
| 23.07 |                                CMU                                 |            arxiv             |                     [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)                     |              **Jailbreak**&**Transferable Attack**&**Adversarial Attack**               |
| 23.07 |                  Nanyang Technological University                  |           NDSS2023           |                   [MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots](https://arxiv.org/abs/2307.08715)                    |             **Jailbreak**&**Reverse-Engineering**&**Automatic Generation**              |
| 23.11 |                               MBZUAI                               |            arxiv             |                [Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks](https://arxiv.org/abs/2311.00508)                |        **Adversarially-synthesized Texts**&**Word-level Attacks**&**Evaluation**        |
| 23.11 |                         Palisade Research                          |            arxiv             |                        [BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B](https://arxiv.org/abs/2311.00117)                        |                              **Remove Safety Fine-tuning**                              |
| 23.11 |                        University of Twente                        |         ICNLSP 2023          |                         [Efficient Black-Box Adversarial Attacks on Neural Text Detectors](https://arxiv.org/abs/2311.01873)                          |                      **Misclassification**&**Adversarial attacks**                      |
| 23.11 |           PRISM AI&Harmony Intelligenc&Leap Laboratories           |            arxiv             |            [Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation](https://arxiv.org/abs/2311.03348 )             |           **Persona-modulation Attacks**&**Jailbreaks**&**Automated Prompt**            |
| 23.11 |                        Tsinghua University                         |            arxiv             |                     [Jailbreaking Large Vision-Language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)                      |              **Typographic Attack**&**Multi-modal**&**Safety Evaluation**               |
| 23.11 | Huazhong University of Science and Technology, Tsinghua University |            arxiv             |    [Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration](https://arxiv.org/abs/2311.06062)    |                **Membership Inference Attacks**&**Privacy and Security**                |
| 23.11 |                  Nanjing University, Meituan Inc                   |            arxiv             |      [A Wolf in Sheep‚Äôs Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://arxiv.org/abs/2311.08268)       |         **Jailbreak Prompts**&**Safety Alignment**&**Safeguard Effectiveness**          |
| 23.11 |                          Google DeepMind                           |            arxiv             | [Frontier Language Models Are Not Robust to Adversarial Arithmetic or "What Do I Need To Say So You Agree 2+2=5?"](https://arxiv.org/abs/2311.07587)  |         **Adversarial Arithmetic**&**Model Robustness**&**Adversarial Attacks**         |
| 23.11 | University of Illinois Chicago, Texas A&M University | arxiv | [DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Pre-trained Language Models](https://arxiv.org/abs/2311.08598) | **Adversarial Attack**&**Distribution-Aware**&**LoRA-Based Attack** |
| 23.11 | Tsinghua University | arxiv | [Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization](https://arxiv.org/abs/2311.09096) | **Jailbreaking Attacks**&**Goal Prioritization**&**Safety** |



## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars