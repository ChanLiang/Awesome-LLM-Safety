# NAACL2024

## ðŸ“‘Papers

| Date  |                          Institute                           |     Publication     |                            Paper                             |                           Keywords                           |
| :---: | :----------------------------------------------------------: | :-----------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 23.05 |                  UC Davis, USC, UW-Madison                   |      NAACL2024      | [From Shortcuts to Triggers: Backdoor Defense with Denoised PoE](https://arxiv.org/abs/2305.14910) | **Backdoor Attacks**&**Defense Methods**&**Natural Language Processing** |
| 23.08 |                      Meta Reality Labs                       |      NAACL2024      | [Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)?](https://arxiv.org/abs/2308.10168) | **Large Language Models**&**Knowledge Graphs**&**Question Answering** |
| 23.08 |                      Bocconi University                      |      NAACL2024      | [XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models](https://arxiv.org/abs/2308.01263) | **Large Language Models**&**Safety Behaviours**&**Test Suite** |
| 23.10 |    University of Edinburgh, Huawei Technologies Co., Ltd.    |      NAACL2024      | [Assessing the Reliability of Large Language Model Knowledge](https://arxiv.org/abs/2310.09820) | **Large Language Models**&**Factual Knowledge**&**Knowledge Probing** |
| 23.10 |        The Chinese University of Hong Kong&Microsoft         |      NAACL2024      | [SELF-GUARD: Empower the LLM to Safeguard Itself](https://arxiv.org/abs/2310.15851) | **Large Language Models**&**Jailbreak Attacks**&**Safety Mechanisms** |
| 23.11 | Fudan University&Shanghai Artificial Intelligence Laboratory |      NAACL2024      | [Fake Alignment: Are LLMs Really Aligned Well?](https://arxiv.org/abs/2311.05915) | **Large Language Models**&**Safety Evaluation**&**Fake Alignment** |
| 23.11 |                 Kahlert School of Computing                  |      NAACL2024      | [Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness](https://arxiv.org/abs/2311.09694) | **NLP Robustness**&**Out-of-Domain Evaluation**&**Adversarial Evaluation** |
| 24.03 | XiaMen University, Yanshan University, IDEA Research, Inner Mongolia University, Microsoft, Microsoft Research Asia |      NAACL2024      | [Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models](https://arxiv.org/abs/2403.11838) | **Language Models**&**Safety Guidelines**&**Model Alignment** |
| 24.03 |                          IIT Delhi                           |      NAACL2024      | [Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF](https://arxiv.org/abs/2403.10088) | **Counterspeech Generation**&**Multi-Task Instruction Tuning**&**Reinforcement Learning from AI Feedback (RLAIF)** |
| 24.04 |              Princeton University&UC Davis&USC               |      NAACL2024      | [Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors](https://arxiv.org/abs/2404.02356) | **Nested Product of Experts**&**Data Poisoning**&**Backdoor Defense** |
| 23.11 | Shanghai Artificial Intelligence Laboratory&Fudan University |      NAACL2024      | [FLAMES: Benchmarking Value Alignment of LLMs in Chinese](https://arxiv.org/abs/2311.06899) |           **Value Alignment**&**LLMs**&**Safety**            |
| 23.11 | University of Michigan&University of Hawaii at Hilo&Northeastern University |      NAACL2024      | [MOKA: Moral Knowledge Augmentation for Moral Event Extraction](https://arxiv.org/abs/2311.09733) | **Moral Knowledge Augmentation**&**Moral Event Extraction**&**NLP** |
| 23.04 |    University of Michigan&Arizona State University&NVIDIA    |      NAACL2024      | [ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger](https://arxiv.org/abs/2304.14475) | **Textual Backdoor Attack**&**Blackbox Generative Model**&**Trigger Detection** |
| 23.11 |               Nanjing University&Meituan Inc.                |      NAACL2024      | [A Wolf in Sheepâ€™s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://arxiv.org/abs/2311.08268) | **Jailbreak Prompts**&**LLM Security**&**Automated Framework** |
| 24.03 | School of Information Science and Technology, ShanghaiTech University |      NAACL2024      | [LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models](https://arxiv.org/abs/2403.16432) | **Prompt-based Language Models**&**Universal Adversarial Triggers**&**Natural Language Attacks** |
| 24.04 |                Purdue University, Fort Wayne                 |      NAACL2024      | [Vert Attack: Taking advantage of Text Classifiersâ€™ horizontal vision](https://arxiv.org/abs/2404.08538) | **Text Classifiers**&**Adversarial Attacks**&**VertAttack**  |
| 24.04 | The University of Melbourne&Macquarie University&University College London |      NAACL2024      | [Backdoor Attacks on Multilingual Machine Translation](https://arxiv.org/abs/2404.02393) | **Multilingual Machine Translation**&**Security**&**Backdoor Attacks** |
| 24.01 |       Harvard&USC&UCLA&UW Seattle&UW-Madison&UC Davis        |      NAACL2024      | [Instructional Fingerprinting of Large Language Models](https://arxiv.org/abs/2401.12255) | **Model Fingerprinting**&**Instructional Backdoor**&**Model Ownership** |
| 24.02 | University of Washington&Allen Institute for Artificial Intelligence |      NAACL2024      | [JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models](https://arxiv.org/abs/2402.08761) | **Authorship Obfuscation**&**Constrained Decoding**&**Small Language Models** |
| 24.05 |                    University of Maryland                    |      NAACL2024      | [Keep It Private: Unsupervised Privatization of Online Text](https://arxiv.org/abs/2405.10260) | **Unsupervised Privatization**&**Online Text**&**Large Language Models** |
| 23.05 |                Harbin Institute of Technology                |      NAACL2024      | [Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting](https://arxiv.org/abs/2305.13733) | **Large Language Models**&**Inductive Instructions**&**Dual-critique Prompting** |
| 23.05 | National Key Laboratory for Multimedia Information Processing |      NAACL2024      | [DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade](https://arxiv.org/abs/2305.14751) | **Natural Language Understanding**&**Dialogue System**&**Multi-label Classification** |
| 23.11 |                      Amazon Alexa AI-NU                      |      NAACL2024      | [JAB: Joint Adversarial Prompting and Belief Augmentation](https://arxiv.org/abs/2311.09473) | **Adversarial Prompting**&T**oxicity Reduction**&**Robustness** |
| 24.03 |                             UIUC                             |      NAACL2024      | [Fact Checking Beyond Training Set](https://arxiv.org/abs/2403.18671) | **Fact Checking**&**Domain Adaptation**&**Adversarial Training** |
| 24.03 | Institute of Data Science, National University of Singapore  |      NAACL2024      | [SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks](https://arxiv.org/abs/2403.18423) | **Adversarial Training**&**Word-Level Attacks**&**Robust Representations** |
| 23.11 |         Meta&University of Illinois Urbana-Champaign         |      NAACL2024      | [MART: Improving LLM Safety with Multi-round Automatic Red-Teaming](https://arxiv.org/abs/2311.07689) | **Automatic Red-Teaming**&**LLM Safety**&**Adversarial Prompt Writing** |
| 23.11 |  The Ohio State University&University of California, Davis   |      NAACL2024      | [How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities](https://arxiv.org/abs/2311.09447) | **Open-Source LLMs**&**Malicious Demonstrations**&**Trustworthiness** |
| 24.02 |         Shanghai Artificial Intelligence Laboratory          |      NAACL2024      | [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283) | **Large Language Models**&**Conversation Safety**&**Survey** |
| 22.11 |             University of California, Santa Cruz             |      NAACL2024      | [Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning](https://arxiv.org/abs/2211.14769) | **Federated Learning**&**Vision-and-Language Navigation**&**Security** |
| 23.05 | Harvard&UCLA&USC&University of Wisconsin, Madison&UC, Davis  |      NAACL2024      | [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.14710) | **Instruction Tuning**&**Large Language Models**&**Backdoor Attacks** |
| 23.12 | University of Central Florida, Samsung Research America, University of Pittsburgh, Indiana University Bloomington |      NAACL2024      | [TrojFSP: Trojan Insertion in Few-shot Prompt Tuning](https://arxiv.org/abs/2312.10467) | **Prompt Tuning**&**Backdoor Attacks**&**Few-shot Learning** |
| 23.12 |              The Pennsylvania State University               |      NAACL2024      | [Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://arxiv.org/abs/2312.00027) | **Backdoor Injections**&**LLM Security**&**Persistent Unalignment** |
| 23.10 | Tsinghua University, Allen Institute for AI, University of Illinois Urbana-Champaign |      NAACL2024      | [Language Models Hallucinate, but May Excel at Fact Verification](https://arxiv.org/abs/2310.14564) | **Large Language Models**&**Hallucination**&**Fact Verification** |
| 23.10 | Stanford University&University of Maryland&Carnegie Mellon University&NYU Shanghai&New York University&Microsoft Research |      NAACL2024      | [Large Language Models Help Humans Verify Truthfulnessâ€”Except When They Are Convincingly Wrong](https://arxiv.org/abs/2310.12558) | **Large Language Models**&**Fact-Checking**&**Truthfulness** |
| 23.10 |                     Shandong University                      |      NAACL2024      | [Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method](https://arxiv.org/abs/2310.17918) | **Large Language Models**&**Self-Detection**&**Non-Factuality Detection** |
| 23.11 |                   Arizona State University                   |      NAACL2024      | [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914) | **Knowledge Graphs**&**Large Language Models**&**Hallucination Reduction** |
| 23.11 |   Mohamed bin Zayed University of Artificial Intelligence    |      NAACL2024      | [A Survey of Confidence Estimation and Calibration in Large Language Models](https://arxiv.org/abs/2311.08298) | **Confidence Estimation**&**Calibration**&**Large Language Models** |
| 23.11 |               University of California, Davis                |      NAACL2024      | [Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702) | **Semantic Shortcuts**&**Reasoning Chains**&**Hallucination** |
| 23.11 |                      University of Utah                      |      NAACL2024      | [To Tell The Truth: Language of Deception and Language Models](https://arxiv.org/abs/2311.07092) | **Deception Detection**&**Language Models**&**Conversational Analysis** |
| 24.02 | AWS AI Labs&Korea Advanced Institute of Science & Technology&The University of Texas at Austin |      NAACL2024      | [TOFUEVAL: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization](https://arxiv.org/abs/2402.13249) | **Hallucination Evaluation**&**LLMs**&**Dialogue Summarization** |
| 24.03 |           University of Illinois Urbana-Champaign            |      NAACL2024      | [Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation](https://arxiv.org/abs/2403.14952) | **Online Misinformation**&**Retrieval Augmented Response**&**Evidence-Based Countering** |
| 24.03 | Department of Electronic Engineering, Tsinghua University, Pattern Recognition Center, WeChat AI, Tencent Inc, China |      NAACL2024      | [On Large Language Modelsâ€™ Hallucination with Regard to Known Facts](https://arxiv.org/abs/2403.20009) |           **Hallucination**&**Inference Dynamics**           |
| 24.03 | Department of Electronic Engineering, Tsinghua University, Pattern Recognition Center, WeChat AI, Tencent Inc, China |      NAACL2024      | [On Large Language Modelsâ€™ Hallucination with Regard to Known Facts](https://arxiv.org/abs/2403.20009) |           **Hallucination**&**Inference Dynamics**           |
| 24.03 |         Tsinghua University, WeChat AI, Tencent Inc.         |      NAACL2024      | [On Large Language Modelsâ€™ Hallucination with Regard to Known Facts](https://arxiv.org/abs/2403.20009) | **Large Language Models**&**Hallucination**&**Inference Dynamics** |
| 24.04 |              University of California, Berkeley              |      NAACL2024      | [ALOHa: A New Measure for Hallucination in Captioning Models](https://davidmchan.github.io/aloha) |         **Adversarial Attack**&**AI-Text Detection**         |
| 24.04 |                          ServiceNow                          |      NAACL2024      | [Reducing hallucination in structured outputs via Retrieval-Augmented Generation](https://arxiv.org/abs/2404.08189) | **Retrieval-Augmented Generation**&**Structured Outputs**&**Generative AI** |
| 24.04 |                     Stanford University                      |      NAACL2024      | [NLP Systems That Canâ€™t Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps](https://arxiv.org/abs/2404.01651) | **Counterspeech**&**Censorship**&**Use-Mention Distinction** |
| 24.04 |   Department of Computing Science, University of Aberdeen    |      NAACL2024      | [Improving Factual Accuracy of Neural Table-to-Text Output by Addressing Input Problems in ToTTo](https://arxiv.org/abs/2404.04103) | **Neural Table-to-Text**&**Factual Accuracy**&**Input Problems** |
| 23.08 |             Bern University of Applied Sciences              | NAACL2024(findings) | [Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models in Court Decisions](https://arxiv.org/abs/2308.11103) | **Anonymization**&**Re-Identification**&**Large Language Models** |
| 23.05 |                            KAIST                             | NAACL2024(findings) | [Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against Counterfactual Noise](https://arxiv.org/abs/2305.01579) | **Retrieval-Augmented Models**&**Counterfactual Noise**&**Open-Domain Question Answering** |
| 23.07 |               Stevens Institute of Technology                | NAACL2024(findings) | [HateModerate: Testing Hate Speech Detectors against Content Moderation Policies](https://arxiv.org/abs/2307.12418) | **Hate Speech Detection**&**Content Moderation**&**Machine Learning** |
| 24.03 |                    Stony Brook University                    | NAACL2024(findings) | [Task-Agnostic Detector for Insertion-Based Backdoor Attacks](https://arxiv.org/abs/2403.17155) |  **Backdoor Detection**&**Logit Features**&**NLP Security**  |
| 24.02 |               Nanyang Technological University               | NAACL2024(findings) | [Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.12168) |    **Backdoor Attacks**&**Fine-Tuning**&**NLP Security**     |
| 24.03 |                     Chung-Ang University                     | NAACL2024(findings) | [Donâ€™t be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks](https://arxiv.org/abs/2403.15467) | **Offensive Language Detection**&**Adversarial Attacks**&**Pooling Strategies** |
| 24.03 |                   Fondazione Bruno Kessler                   | NAACL2024(findings) | [NLP for Counterspeech against Hate: A Survey and How-To Guide](https://arxiv.org/abs/2403.20103) |          **Counterspeech**&**Online Hate**&**NLP**           |
| 23.11 |              University of Southern California               | NAACL2024(findings) | [Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking](https://arxiv.org/abs/2311.09827) | **Jailbreaking**&**Large Language Models**&**Cognitive Overload** |
| 24.03 |                   Georgia State University                   | NAACL2024(findings) | [RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning](https://arxiv.org/abs/2403.11082) | **Sentence Embeddings**&**Adversarial Learning**&**Contrastive Learning** |
| 23.10 |                       Rice University                        | NAACL2024(findings) | [Secure Your Model: An Effective Key Prompt Protection Mechanism for Large Language Models](https://www.researchgate.net/publication/374555007_Secure_Your_Model_An_Effective_Key_Prompt_Protection_Mechanism_for_Large_Language_Models/link/65f8a03b286738732d5ce0d3/download?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1YmxpY2F0aW9uIn19) | **Key Prompt Protection**&**Large Language Models**&**Unauthorized Access Prevention** |
| 24.05 |                         GSAI POSTECH                         | NAACL2024(findings) | [Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents](https://arxiv.org/abs/2405.12900) | **Adversarial Training**&**Toxicity Reduction**&**Dialogue Systems** |
| 23.10 |                  University of Pennsylvania                  | NAACL2024(findings) | [Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks](https://arxiv.org/abs/2310.12516) | **Hallucination Assessment**&**Adversarial Attacks**&**Large Language Models** |
| 23.11 |                Shanghai Jiao Tong University                 | NAACL2024(findings) | [CLEANâ€“EVAL: Clean Evaluation on Contaminated Large Language Models](https://arxiv.org/abs/2311.09154) | **Clean Evaluation**&**Data Contamination**&**Large Language Models** |
| 24.04 |                       Google Research                        | NAACL2024(findings) | [Modeling the Sacred: Considerations when Using Religious Texts in Natural Language Processing](https://arxiv.org/abs/2404.14740) | **Religious Texts**&**Natural Language Processing**&**Ethics** |
| 23.05 |                       Google Research                        | NAACL2024(findings) | [Can Public Large Language Models Help Private Cross-device Federated Learning?](https://arxiv.org/abs/2305.12132) | **Federated Learning**&**Large Language Models**&**Differential Privacy** |
| 23.11 |                  Michigan State University                   | NAACL2024(findings) | [A Robust Semantics-based Watermark for Large Language Models against Paraphrasing](https://arxiv.org/abs/2311.08721) |   **Watermark**&**Large Language Models**&**Paraphrasing**   |
| 23.10 |       CISPA Helmholtz Center for Information Security        | NAACL2024(findings) | [Composite Backdoor Attacks Against Large Language Models](https://arxiv.org/abs/2310.07676) | **Backdoor Attacks**&**Large Language Models**&**Security**  |
| 24.05 |                   Johns Hopkins University                   | NAACL2024(findings) | [MICo: Preventative Detoxification of Large Language Models through Inhibition Control](https://assets.amazon.science/e7/94/facc166449eba31223cbc88614a9/mico-preventative-detoxification-of-large-language-models-through-inhibition-control.pdf) | **Detoxification**&**Inhibition Control**&**Toxicity Reduction** |
| 23.11 |                      Cornell University                      | NAACL2024(findings) | [Adapting Fake News Detection to the Era of Large Language Models](https://arxiv.org/abs/2311.04917) | **Fake News Detection**&**Large Language Models**&**Machine-Generated Content** |
| 24.04 |                  Seoul National University                   | NAACL2024(findings) | [Mitigating Hallucination in Abstractive Summarization with Domain-Conditional Mutual Information](https://arxiv.org/abs/2404.09480) | **Hallucination**&**Abstractive Summarization**&**Domain-Conditional Mutual Information** |
