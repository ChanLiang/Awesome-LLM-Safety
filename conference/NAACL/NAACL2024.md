# NAACL2024

## ðŸ“‘Papers

| 23.08 |                      Meta Reality Labs                       | NAACL2024  | [Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)?](https://arxiv.org/abs/2308.10168) | **Large Language Models**&**Knowledge Graphs**&**Question Answering** |
| :---: | :----------------------------------------------------------: | :--------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 23.08 |                      Bocconi University                      | NAACL2024  | [XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models](https://arxiv.org/abs/2308.01263) | **Large Language Models**&**Safety Behaviours**&**Test Suite** |
| 23.10 |    University of Edinburgh, Huawei Technologies Co., Ltd.    | NAACL2024  | [Assessing the Reliability of Large Language Model Knowledge](https://arxiv.org/abs/2310.09820) | **Large Language Models**&**Factual Knowledge**&**Knowledge Probing** |
| 23.11 | Fudan University&Shanghai Artificial Intelligence Laboratory | NAACL2024  | [Fake Alignment: Are LLMs Really Aligned Well?](https://arxiv.org/abs/2311.05915) | **Large Language Models**&**Safety Evaluation**&**Fake Alignment** |
| 23.11 |                 Kahlert School of Computing                  | NAACL2024  | [Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness](https://arxiv.org/abs/2311.09694) | **NLP Robustness**&**Out-of-Domain Evaluation**&**Adversarial Evaluation** |
| 23.05 |                  UC Davis, USC, UW-Madison                   | NAACL2024  | [From Shortcuts to Triggers: Backdoor Defense with Denoised PoE](https://arxiv.org/abs/2305.14910) | **Backdoor Attacks**&**Defense Methods**&**Natural Language Processing** |
| 23.10 |        The Chinese University of Hong Kong&Microsoft         | NAACL2024  | [SELF-GUARD: Empower the LLM to Safeguard Itself](https://arxiv.org/abs/2310.15851) | **Large Language Models**&**Jailbreak Attacks**&**Safety Mechanisms** |
| 24.03 | XiaMen University, Yanshan University, IDEA Research, Inner Mongolia University, Microsoft, Microsoft Research Asia | NAACL2024  | [Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models](https://arxiv.org/abs/2403.11838) | **Language Models**&**Safety Guidelines**&**Model Alignment** |
| 24.03 |                          IIT Delhi                           | NAACL2024  | [Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF](https://arxiv.org/abs/2403.10088) | **Counterspeech Generation**&**Multi-Task Instruction Tuning**&**Reinforcement Learning from AI Feedback (RLAIF)** |
| 24.04 |              Princeton University&UC Davis&USC               | NAACL2024  | [Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors](https://arxiv.org/abs/2404.02356) | **Nested Product of Experts**&**Data Poisoning**&**Backdoor Defense** |
| 23.11 | Shanghai Artificial Intelligence Laboratory&Fudan University | NAACL2024  | [FLAMES: Benchmarking Value Alignment of LLMs in Chinese](https://arxiv.org/abs/2311.06899) |           **Value Alignment**&**LLMs**&**Safety**            |
| 23.11 | University of Michigan&University of Hawaii at Hilo&Northeastern University | NAACL2024  | [MOKA: Moral Knowledge Augmentation for Moral Event Extraction](https://arxiv.org/abs/2311.09733) | **Moral Knowledge Augmentation**&**Moral Event Extraction**&**NLP** |
| 23.04 |    University of Michigan&Arizona State University&NVIDIA    | NAACL2024  | [ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger](https://arxiv.org/abs/2304.14475) | **Textual Backdoor Attack**&**Blackbox Generative Model**&**Trigger Detection** |
| 23.11 |               Nanjing University&Meituan Inc.                | NAACL2024  | [A Wolf in Sheepâ€™s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://arxiv.org/abs/2311.08268) | **Jailbreak Prompts**&**LLM Security**&**Automated Framework** |
| 24.03 | School of Information Science and Technology, ShanghaiTech University | NAACL2024  | [LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models](https://arxiv.org/abs/2403.16432) | **Prompt-based Language Models**&**Universal Adversarial Triggers**&**Natural Language Attacks** |
| 24.04 |                Purdue University, Fort Wayne                 | NAACL2024  | [Vert Attack: Taking advantage of Text Classifiersâ€™ horizontal vision](https://arxiv.org/abs/2404.08538) | **Text Classifiers**&**Adversarial Attacks**&**VertAttack**  |
| 24.04 | The University of Melbourne&Macquarie University&University College London | NAACL2024  | [Backdoor Attacks on Multilingual Machine Translation](https://arxiv.org/abs/2404.02393) | **Multilingual Machine Translation**&**Security**&**Backdoor Attacks** |
| 24.01 |       Harvard&USC&UCLA&UW Seattle&UW-Madison&UC Davis        | NAACL2024  | [Instructional Fingerprinting of Large Language Models](https://arxiv.org/abs/2401.12255) | **Model Fingerprinting**&**Instructional Backdoor**&**Model Ownership** |
| 24.02 | University of Washington&Allen Institute for Artificial Intelligence | NAACL2024  | [JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models](https://arxiv.org/abs/2402.08761) | **Authorship Obfuscation**&**Constrained Decoding**&**Small Language Models** |
| 24.05 |                    University of Maryland                    | NAACL2024  | [Keep It Private: Unsupervised Privatization of Online Text](https://arxiv.org/abs/2405.10260) | **Unsupervised Privatization**&**Online Text**&**Large Language Models** |
| 23.05 |                Harbin Institute of Technology                | NAACL2024  | [Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting](https://arxiv.org/abs/2305.13733) | **Large Language Models**&**Inductive Instructions**&**Dual-critique Prompting** |
| 23.05 | National Key Laboratory for Multimedia Information Processing | NAACL2024  | [DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade](https://arxiv.org/abs/2305.14751) | **Natural Language Understanding**&**Dialogue System**&**Multi-label Classification** |
| 23.11 |                      Amazon Alexa AI-NU                      | NAACL2024  | [JAB: Joint Adversarial Prompting and Belief Augmentation](https://arxiv.org/abs/2311.09473) | **Adversarial Prompting**&T**oxicity Reduction**&**Robustness** |
| 24.03 |                             UIUC                             | NAACL2024  | [Fact Checking Beyond Training Set](https://arxiv.org/abs/2403.18671) | **Fact Checking**&**Domain Adaptation**&**Adversarial Training** |
| 24.03 | Institute of Data Science, National University of Singapore  | NAACL2024  | [SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks](https://arxiv.org/abs/2403.18423) | **Adversarial Training**&**Word-Level Attacks**&**Robust Representations** |
| 23.11 |         Meta&University of Illinois Urbana-Champaign         | NAACL2024  | [MART: Improving LLM Safety with Multi-round Automatic Red-Teaming](https://arxiv.org/abs/2311.07689) | **Automatic Red-Teaming**&**LLM Safety**&**Adversarial Prompt Writing** |
| 23.11 |  The Ohio State University&University of California, Davis   | NAACL2024  | [How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities](https://arxiv.org/abs/2311.09447) | **Open-Source LLMs**&**Malicious Demonstrations**&**Trustworthiness** |
| 24.02 |         Shanghai Artificial Intelligence Laboratory          | NAACL2024  | [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283) | **Large Language Models**&**Conversation Safety**&**Survey** |
| 22.11 |             University of California, Santa Cruz             | NAACL2024  | [Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning](https://arxiv.org/abs/2211.14769) | **Federated Learning**&**Vision-and-Language Navigation**&**Security** |
| 23.05 | Harvard&UCLA&USC&University of Wisconsin, Madison&UC, Davis  | NAACL2024  | [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.14710) | **Instruction Tuning**&**Large Language Models**&**Backdoor Attacks** |
| 23.12 | University of Central Florida, Samsung Research America, University of Pittsburgh, Indiana University Bloomington | NAACL2024  | [TrojFSP: Trojan Insertion in Few-shot Prompt Tuning](https://arxiv.org/abs/2312.10467) | **Prompt Tuning**&**Backdoor Attacks**&**Few-shot Learning** |
| 23.12 |              The Pennsylvania State University               | NAACL2024  | [Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://arxiv.org/abs/2312.00027) | **Backdoor Injections**&**LLM Security**&**Persistent Unalignment** |
| 23.10 | Tsinghua University, Allen Institute for AI, University of Illinois Urbana-Champaign | NAACL2024  | [Language Models Hallucinate, but May Excel at Fact Verification](https://arxiv.org/abs/2310.14564) | **Large Language Models**&**Hallucination**&**Fact Verification** |
| 23.10 | Stanford University&University of Maryland&Carnegie Mellon University&NYU Shanghai&New York University&Microsoft Research | NAACL2024  | [Large Language Models Help Humans Verify Truthfulnessâ€”Except When They Are Convincingly Wrong](https://arxiv.org/abs/2310.12558) | **Large Language Models**&**Fact-Checking**&**Truthfulness** |
| 23.10 |                     Shandong University                      | NAACL2024  | [Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method](https://arxiv.org/abs/2310.17918) | **Large Language Models**&**Self-Detection**&**Non-Factuality Detection** |
| 23.11 |                   Arizona State University                   | NAACL2024  | [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914) | **Knowledge Graphs**&**Large Language Models**&**Hallucination Reduction** |
| 23.11 |   Mohamed bin Zayed University of Artificial Intelligence    | NAACL2024  | [A Survey of Confidence Estimation and Calibration in Large Language Models](https://arxiv.org/abs/2311.08298) | **Confidence Estimation**&**Calibration**&**Large Language Models** |
| 23.11 |               University of California, Davis                | NAACL2024  | [Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702) | **Semantic Shortcuts**&**Reasoning Chains**&**Hallucination** |
| 23.11 |                      University of Utah                      | NAACL2024  | [To Tell The Truth: Language of Deception and Language Models](https://arxiv.org/abs/2311.07092) | **Deception Detection**&**Language Models**&**Conversational Analysis** |
| 24.02 | AWS AI Labs&Korea Advanced Institute of Science & Technology&The University of Texas at Austin | NAACL2024  | [TOFUEVAL: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization](https://arxiv.org/abs/2402.13249) | **Hallucination Evaluation**&**LLMs**&**Dialogue Summarization** |
| 24.03 |           University of Illinois Urbana-Champaign            | NAACL2024  | [Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation](https://arxiv.org/abs/2403.14952) | **Online Misinformation**&**Retrieval Augmented Response**&**Evidence-Based Countering** |
| 24.03 | Department of Electronic Engineering, Tsinghua University, Pattern Recognition Center, WeChat AI, Tencent Inc, China | NAACL 2024 | [On Large Language Modelsâ€™ Hallucination with Regard to Known Facts](https://arxiv.org/abs/2403.20009) |           **Hallucination**&**Inference Dynamics**           |
| 24.03 | Department of Electronic Engineering, Tsinghua University, Pattern Recognition Center, WeChat AI, Tencent Inc, China | NAACL 2024 | [On Large Language Modelsâ€™ Hallucination with Regard to Known Facts](https://arxiv.org/abs/2403.20009) |           **Hallucination**&**Inference Dynamics**           |
| 24.03 |         Tsinghua University, WeChat AI, Tencent Inc.         | NAACL2024  | [On Large Language Modelsâ€™ Hallucination with Regard to Known Facts](https://arxiv.org/abs/2403.20009) | **Large Language Models**&**Hallucination**&**Inference Dynamics** |
| 24.04 |              University of California, Berkeley              | NAACL 2024 | [ALOHa: A New Measure for Hallucination in Captioning Models](https://davidmchan.github.io/aloha) |         **Adversarial Attack**&**AI-Text Detection**         |
| 24.04 |                          ServiceNow                          | NAACL 2024 | [Reducing hallucination in structured outputs via Retrieval-Augmented Generation](https://arxiv.org/abs/2404.08189) | **Retrieval-Augmented Generation**&**Structured Outputs**&**Generative AI** |
| 24.04 |                     Stanford University                      | NAACL2024  | [NLP Systems That Canâ€™t Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps](https://arxiv.org/abs/2404.01651) | **Counterspeech**&**Censorship**&**Use-Mention Distinction** |
| 24.04 |   Department of Computing Science, University of Aberdeen    | NAACL2024  | [Improving Factual Accuracy of Neural Table-to-Text Output by Addressing Input Problems in ToTTo](https://arxiv.org/abs/2404.04103) | **Neural Table-to-Text**&**Factual Accuracy**&**Input Problems** |



