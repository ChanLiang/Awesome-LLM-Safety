# ICML 2024

<!-- In ICML 2024, there are total xxx papers, and as for safety, there are xxx papars.  -->

## ðŸ“‘Papers-Oral 

| Date  |                          Institute                           |     Publication     |                            Paper                             |                           Keywords                           |
| :---: | :----------------------------------------------------------: | :-----------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 23.11 |                  Google DeepMind                   |      ICML2024      | [Scalable AI Safety via Doubly-Efficient Debate](https://openreview.net/forum?id=6jmdOTRMIO) | **Evaluation**&**Protocols** |
| 23.12 |                  Redwood Research, Anthropic                   |      ICML2024      | [AI Control: Improving Safety Despite Intentional Subversion](https://openreview.net/forum?id=KviM5k8pcP) | **Red Team**&**Protocols** |
| 24.01 |                  University of Michigan                   |      ICML2024      | [A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity](https://openreview.net/forum?id=dBqHGZPGZI) | **Alignment**&**Explaination** |
| 24.02 |                  UC Berkeley, Meta AI, TU Dresden                   |      ICML2024      | [A Touch, Vision, and Language Dataset for Multimodal Alignment](https://openreview.net/forum?id=tFEOOH9eH0) | **Alignment**&**Multimodal** |
| 24.02 |                  Huazhong University of Science and Technology                   |      ICML2024      | [MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark](https://openreview.net/forum?id=dbFEFHAD79) | **LLM-as-a-Judge**&**Multimodel** |
| 24.02 |                  Princeton University, Anthropic, Google DeepMind                   |      ICML2024      | [How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?](https://openreview.net/forum?id=685vj0lC9z) | **Honesty and Helpfulness**&**Alignment**&**Prompts** |
| 24.04 |                  Tsinghua University, OpenPsi Inc, Shanghai Qi Zhi Institute                   |      ICML2024      | [Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://openreview.net/forum?id=6XH8R7YrSk) | **Alignment**&**DPO** |


## ðŸ“‘Papers-Spotlight 

| Date | Institute | Conference | Title | Keywords |
| --- | --- | --- | --- | --- |
| 23.07 | Columbia University, UC Berkeley, NYU Shanghai, NYU | ICML2024 | [Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations](https://openreview.net/forum?id=99jx5U81jx) | **Counterfactual Simulatability**&**Natural Language Explanations** |
| 23.10 | Mila, Polytechnique Montreal, McGill, Facebook CIFAR AI Chair, Canada CIFAR AI Chair | ICML2024 | [Faithfulness Measurable Masked Language Models](https://openreview.net/forum?id=tw1PwpuAuN) | **Explanation Faithfulness**&**Masked Language Models** |
| 24.02 | DeepMind, Google Research, Uni Basel, Uni Edinburgh, Univ Paris-Saclay | ICML2024 | [Decoding-time Realignment of Language Models](https://openreview.net/forum?id=n8g6WMxt09) | **Language Model Alignment**&**Reinforcement Learning** |
| 24.02 | SJTU, Shanghai AI Lab, MGI Crew | ICML2024 | [Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation](https://openreview.net/forum?id=l7shXGuGBT) | **Self-Alignment**&**Social Scene Simulation** |
| 24.03 | UIUC, MSR, SYSU, UChicago | ICML2024 | [Differentially Private Synthetic Data via Foundation Model APIs 2: Text](https://openreview.net/attachment?id=LWD7upg1ob&name=pdf) | **Privacy-Preserving ML**&**Synthetic Data** |
| 24.05 | KAUST, Lehigh, UGA, ISU | ICML2024 | [Improving Interpretation Faithfulness for Vision Transformers](https://openreview.net/forum?id=YdwwWRX20q) | **Vision Transformers**&**Interpretation Faithfulness** |
| 24.06 | Google Research, Bocconi University | ICML2024 | [Perturb-and-Project: Differentially Private Similarities and Marginals](https://openreview.net/forum?id=45HNimd4YI) | **Differential Privacy**&**Projection Techniques** |
| 24.06 | Purdue University, CUHK | ICML2024 | [LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models](https://openreview.net/forum?id=NsHxeSCtgr) | **Fairness**&**Bias Mitigation** |
| 24.07 | UChicago, HKUST, USC, UIUC | ICML2024 | [Pessimism Meets Risk: Risk-Sensitive Offline Reinforcement Learning](https://openreview.net/forum?id=InUUQkExsw) | **Risk-Sensitive Learning**&**Offline Reinforcement Learning** |
| 24.07 | University of Chicago, University of Washington | ICML2024 | [Pessimism Meets Risk: Risk-Sensitive Offline Reinforcement Learning](https://openreview.net/forum?id=SfcB4cVvPz) | **Risk-Sensitive Learning**&**Offline Reinforcement Learning** |


## ðŸ“‘Papers-Poster 

| Date  |                          Institute                           |     Publication     |                            Paper                             |                           Keywords                           |
| :---: | :----------------------------------------------------------: | :-----------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 23.05 | UW, Princeton, NYU, AI2 | ICML2024 | [How Language Model Hallucinations Can Snowball](https://openreview.net/forum?id=FPlaQyAGHu) | **Hallucination Analysis**&**Model Behavior** |
| 23.05 | MIT, Google Deepmind | ICML2024 | [Improving Factuality and Reasoning in Language Models through Multiagent Debate](https://openreview.net/forum?id=zj7YuTE4t8) | **Factuality Improvement**&**Multiagent Debate** |
| 23.06 | OSU, Google Research, Courant Institute | ICML2024 | [Differentially Private Domain Adaptation with Theoretical Guarantees](https://openreview.net/forum?id=kkqIEp2bRa) | **Privacy Constraints**&**Domain Adaptation** |
| 23.09 | NYCU, IBM | ICML2024 | [Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts](https://openreview.net/forum?id=VyGo1S5A6d) | **Red Teaming**&**Text-to-Image Diffusion Models** |
| 23.09 | Harvard, Cambridge, UC Berkeley, Berkeley | ICML2024 | [Image Hijacks: Adversarial Images can Control Generative Models at Runtime](https://openreview.net/forum?id=8ho1l6RZNB) | **Adversarial Images**&**Generative Models Control** |
| 23.10 | University of Edinburgh, EPFL | ICML2024 | [Fool Your (Vision and) Language Model with Embarrassingly Simple Permutations](https://openreview.net/forum?id=IUijgjJgWO) | **Permutation Sensitivity**&**MCQA Vulnerability** |
| 23.10 | UCLA | ICML2024 | [Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks](https://openreview.net/forum?id=ycLHJuLYuD) | **Data Poisoning**&**Backdoor Attacks** |
| 23.12 | UMass Amherst, Columbia, Google, Stanford, NYU | ICML2024 | [Learning and Forgetting Unsafe Examples in Large Language Models](https://openreview.net/forum?id=RYmmgedVjR) | **Unsafe Examples**&**Large Language Models** |
| 24.01 | ETH Zurich, Max Planck, KU Leuven, NYU | ICML2024 | [Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?](https://openreview.net/forum?id=k1JXxbpIY6) | **Cognitive Biases**&**Problem Solving** |
| 24.02 | Imperial, Paris-Saclay | ICML2024 | [Copyright Traps for Large Language Models](https://openreview.net/forum?id=LDq1JPdc55) | **Copyright Enforcement**&**Model Training** |
| 24.02 | UIUC, CAIS, CMU, UC Berkeley, MSR, UChicago | ICML2024 | [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://openreview.net/forum?id=f3TUipYU3U) | **Automated Red Teaming**&**Robust Refusal** |
| 24.02 | HKUST, UCLA, UIUC | ICML2024 | [Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption](https://openreview.net/forum?id=Z0S6fUdW68) | **Adversarial Corruption**&**Model-Based Reinforcement Learning** |
| 24.02 | UVA, NYU | ICML2024 | [Disparate Impact on Group Accuracy of Linearization for Private Inference](https://openreview.net/forum?id=6VZOONPn8S) | **Private Inference**&**Linearization Techniques** |
| 24.02 | Sun Yat-sen University, Google Research | ICML2024 | [Privacy-Preserving Instructions for Aligning Large Language Models](https://openreview.net/forum?id=mUT1biz09t) | **Privacy-Preserving**&**Large Language Models Alignment** |
| 24.02 | UCLA, Cisco Research | ICML2024 | [Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension](https://openreview.net/forum?id=7DbIyQlfaO) | **Local Intrinsic Dimension**&**Hallucination Detection** |
| 24.02 | University of Maryland, Meta, Nvidia | ICML2024 | [ODIN: Disentangled Reward Mitigates Hacking in RLHF](https://openreview.net/forum?id=zcIV8OQFVF) | **Reward Disentanglement**&**Reinforcement Learning** |
| 24.02 | UIUC, UCSD, AI2 | ICML2024 | [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://openreview.net/forum?id=7DbIyQlfaO) | **Jailbreaking LLMs**&**Adversarial Attacks** |
| 24.02 | ETH Zurich | ICML2024 | [Watermark Stealing in Large Language Models](https://openreview.net/forum?id=yUxdk32TU6) | **Watermark Stealing**&**Spoofing and Scrubbing Attacks** |
| 24.02 | University of Maryland | ICML2024 | [Fast Adversarial Attacks on Language Models in One GPU Minute](https://openreview.net/forum?id=wCMNbdshcY) | **Adversarial Attacks**&**Language Models** |
| 24.02 | University of Edinburgh, EPFL | ICML2024 | [Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models](https://openreview.net/forum?id=bWZKvF0g7G) | **Vision-Language Models**&**Safety Fine-Tuning** |
| 24.02 | UIUC, Delft, Netflix, UC Berkeley, Berkeley, UChicago | ICML2024 | [C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models](https://openreview.net/forum?id=FMa4c5NhOe) | **Certified Generation Risks**&**Retrieval-Augmented Models** |
| 24.02 | UChicago, Google Research, Stanford, DeepMind | ICML2024 | [Transforming and Combining Rewards for Aligning Large Language Models](https://openreview.net/forum?id=cAWbm9KRZO) | **Reward Transformation**&**Alignment Strategies** |
| 24.03 | ETH ZÃ¼rich | ICML2024 | [Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation](https://openreview.net/pdf?id=pXaEYzrFae) | **Constrained Decoding**&**Language Model Efficiency** |
| 24.03 | UIUC, VT, Salesforce, UC Berkeley, UChicago | ICML2024 | [RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content](https://openreview.net/forum?id=QAGRPiC3FS) | **Resilient Guardrails**&**Content Moderation** |
| 24.03 | CityU HK, NUS, SJTU, Stanford, PSU, HKUST | ICML2024 | [In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation](https://openreview.net/forum?id=s3e8poX3kb) | **Hallucination Mitigation**&**Language Models** |
| 24.03 | ETH Zurich, NUS | ICML2024 | [In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation](https://openreview.net/forum?id=s3e8poX3kb) | **Hallucination Mitigation**&**Language Models** |
| 24.03 | Google DeepMind | ICML2024 | [Human Alignment of Large Language Models through Online Preference Optimisation](https://openreview.net/forum?id=2RQqg2Y7Y6) | **Online Preference Optimisation**&**Language Model Alignment** |
| 24.04 | Yale University, DeepMind, UIUC | ICML2024 | [NExT: Teaching Large Language Models to Reason about Code Execution](https://openreview.net/forum?id=B1W712hMBi) | **Code Reasoning**&**Self-Training** |
| 24.04 | Lawrence Livermore National Laboratory | ICML2024 | [Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies](https://openreview.net/forum?id=HQtTg1try7) | **Adversarial Robustness**&**Scaling Laws** |
| 24.04 | ETH Zurich | ICML2024 | [Privacy Backdoors: Stealing Data with Corrupted Pretrained Models](https://openreview.net/forum?id=7yixJXmzb8) | **Privacy Backdoors**&**Pretrained Models** |
| 24.05 | University of Oxford, FLAIR | ICML2024 | [PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition](https://openreview.net/forum?id=tQPkzTdaaN) | **Jailbreak Defense**&**Repetition Strategy** |
| 24.05 | Tsinghua, HUST, HIT, CUP | ICML2024 | [Causality Based Front-door Defense Against Backdoor Attack on Language Models](https://openreview.net/forum?id=dmHHVcHFdM) | **Causality**&**Backdoor Defense** |
| 24.05 | USTC, Hefei CNSC, Ant Group | ICML2024 | [Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning](https://openreview.net/forum?id=XwnABAdH5y) | **Trustworthy Alignment**&**Retrieval-Augmentation** |
| 24.05 | UC Berkeley | ICML2024 | [AI Alignment with Changing and Influenceable Reward Functions](https://openreview.net/forum?id=itYGbe0Cs1) | **Dynamic Reward Functions**&**AI Alignment** |
| 24.05 | Brown University, Tenyx | ICML2024 | [Characterizing Large Language Model Geometry Helps Solve Toxicity Detection and Generation](https://openreview.net/forum?id=glfcwSsks8) | **Toxicity Detection**&**Geometric Analysis** |
| 24.05 | University of Illinois Urbana-Champaign, VMWare | ICML2024 | [Robust Universal Adversarial Perturbations](https://openreview.net/forum?id=Paw0BkPaTN) | **Universal Adversarial Perturbations**&**Real-world Transformations** |
| 24.05 | Technion, Haifa, Israel | ICML2024 | [Private and Federated Stochastic Convex Optimization: Efficient Strategies for Centralized Systems](https://openreview.net/forum?id=sTVSyqD6XX) | **Differential Privacy**&**Federated Learning** |
| 24.05 | ISU, Google Research | ICML2024 | [Data Poisoning Attacks against Conformal Prediction](https://openreview.net/forum?id=f49AkFT5jf) | **Data Poisoning**&**Conformal Prediction** |
| 24.05 | Utah State University, UNC Charlotte | ICML2024 | [Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization](https://openreview.net/forum?id=1SiEfsCecd) | **Backdoor Attack**&**Pre-trained Models** |
| 24.05 | ZJU, HDU | ICML2024 | [One for All: A Universal Generator for Concept Unlearnability via Multi-Modal Alignment](https://openreview.net/forum?id=vSerUPYFtB) | **Concept Unlearnability**&**Multi-Modal Alignment** |
| 24.05 | HIT, Alibaba Group, USTC, Deakin, Griffith, NTU | ICML2024 | [IBD-PSC: Input-level Backdoor Detection via Parameter-oriented Scaling Consistency](https://openreview.net/forum?id=YCzbfs2few) | **Backdoor Detection**&**Scaling Consistency** |
| 24.05 | UIUC, UC Berkeley, Santa Barbara, PSU, UChicago, UC Berkeley | ICML2024 | [SHINE: Shielding Backdoors in Deep Reinforcement Learning](https://openreview.net/forum?id=nMWxLnSBGW) | **Backdoor Attacks**&**Reinforcement Learning** |
